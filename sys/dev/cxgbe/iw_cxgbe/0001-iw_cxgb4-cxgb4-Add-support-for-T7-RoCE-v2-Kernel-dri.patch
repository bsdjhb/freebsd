From 38da2151b4a5ce9e2f73711b7800760bbfb7678f Mon Sep 17 00:00:00 2001
From: Potnuri Bharat Teja <bharat@chelsio.com>
Date: Mon, 15 Apr 2024 02:53:56 -0400
Subject: [PATCH] iw_cxgb4/cxgb4: Add support for T7 RoCE v2 Kernel driver
 (transplanted from 36da866cb1a1f66df563cc773248ce6378bb2575)

Pulled From: http://willow/hg/r_and_d/t7sw/
(transplanted from 03e2b3dcd9f4e8bf3d08248a29edc65f02aeaddb)
---
 dev/T4/common/t4_msg.h           |   80 +-
 dev/T4/firmware/t4fw_interface.h |   22 +
 dev/T4/linux/drv/cxgb4_filter.c  |    2 +
 dev/T4/linux/drv/cxgb4_main.c    |   14 +
 dev/T4/linux/drv/cxgb4_ofld.h    |    1 +
 dev/T4/linux/iw_cxgb4/cm.c       |  960 +++++-----
 dev/T4/linux/iw_cxgb4/cq.c       |  685 +++++--
 dev/T4/linux/iw_cxgb4/device.c   |  327 ++--
 dev/T4/linux/iw_cxgb4/ev.c       |   48 +-
 dev/T4/linux/iw_cxgb4/id_table.c |   14 +-
 dev/T4/linux/iw_cxgb4/iw_cxgb4.h |  967 ++++++----
 dev/T4/linux/iw_cxgb4/mem.c      |  248 +--
 dev/T4/linux/iw_cxgb4/provider.c |  618 ++++--
 dev/T4/linux/iw_cxgb4/qp.c       | 3082 ++++++++++++++++++++++++------
 dev/T4/linux/iw_cxgb4/resource.c |  206 +-
 dev/T4/linux/iw_cxgb4/restrack.c |   32 +-
 dev/T4/linux/iw_cxgb4/t4.h       |   73 +-
 dev/T4/linux/iw_cxgb4/user.h     |   32 +-
 dev/linux/cxgbtool/cxgbtool.h    |    3 +-
 19 files changed, 5275 insertions(+), 2139 deletions(-)

diff --git a/dev/T4/linux/iw_cxgb4/cm.c b/dev/T4/linux/iw_cxgb4/cm.c
index 516e6de80..ef5147a30 100644
--- a/dev/T4/linux/iw_cxgb4/cm.c
+++ b/dev/T4/linux/iw_cxgb4/cm.c
@@ -155,76 +155,75 @@ static struct workqueue_struct *workq;
 
 static struct sk_buff_head rxq;
 
-static struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp);
 static void ep_timeout(struct timer_list *t);
 static void connect_reply_upcall(struct chrd_ep *ep, int status);
 static int sched(struct chrd_dev *dev, struct sk_buff *skb);
 
 static LIST_HEAD(timeout_list);
 static spinlock_t timeout_lock;
 
 static void deref_cm_id(struct chrd_ep_common *epc)
 {
 	epc->cm_id->rem_ref(epc->cm_id);
 	epc->cm_id = NULL;
 	set_bit(CM_ID_DEREFED, &epc->history);
 }
 
 static void ref_cm_id(struct chrd_ep_common *epc)
 {
 	set_bit(CM_ID_REFED, &epc->history);
 	epc->cm_id->add_ref(epc->cm_id);
 }
 
 static void deref_qp(struct chrd_ep *ep)
 {
 	chrd_iw_qp_rem_ref(&ep->com.qp->ibqp);
 	clear_bit(QP_REFERENCED, &ep->com.flags);
 	set_bit(QP_DEREFED, &ep->com.history);
 }
 
 static void ref_qp(struct chrd_ep *ep)
 {
 	set_bit(QP_REFERENCED, &ep->com.flags);
 	set_bit(QP_REFED, &ep->com.history);
 	chrd_iw_qp_add_ref(&ep->com.qp->ibqp);
 }
 
 static void start_ep_timer(struct chrd_ep *ep)
 {
 	pr_debug("ep %p\n", ep);
 	if (timer_pending(&ep->timer)) {
 		pr_err("%s timer already started! " "ep %p \n", __func__, ep);
 		return;
 	}
 	clear_bit(TIMEOUT, &ep->com.flags);
 	chrd_get_ep(&ep->com);
 	ep->timer.expires = jiffies + ep_timeout_secs * HZ;
 	add_timer(&ep->timer);
 }
 
 static int stop_ep_timer(struct chrd_ep *ep)
 {
 	pr_debug("ep %p stopping\n", ep);
 	del_timer_sync(&ep->timer);
 	if (!test_and_set_bit(TIMEOUT, &ep->com.flags)) {
 		chrd_put_ep(&ep->com);
 		return 0;
 	}
 	return 1;
 }
 
-static int chrd_l2t_send(struct chrd_rdev *rdev, struct sk_buff *skb,
-		struct chrd_ep *ep)
+int chrd_l2t_send(struct chrd_rdev *rdev, struct sk_buff *skb,
+		  struct l2t_entry *l2t)
 {
 	int	error = 0;
 
 	if (chrd_fatal_error(rdev)) {
 		kfree_skb(skb);
 		pr_err("%s - device in error state - dropping\n", __func__);
 		return -EIO;
 	}
-	error = cxgb4_l2t_send(rdev->lldi.ports[0], skb, ep->l2t);
+	error = cxgb4_l2t_send(rdev->lldi.ports[0], skb, l2t);
 	if (error < 0)
 		kfree_skb(skb);
 	else if (error == NET_XMIT_DROP)
@@ -488,7 +487,7 @@ static int status2errno(int status)
 /*
  * Try and reuse skbs already allocated...
  */
-static struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp)
+struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp)
 {
 	if (skb && !skb_is_nonlinear(skb) && !skb_cloned(skb)) {
 		skb_trim(skb, 0);
@@ -520,10 +519,10 @@ static int our_interface(struct c4iw_dev *dev, struct net_device *egress_dev)
 	return 0;
 }
 
-static struct dst_entry *find_route6(struct chrd_dev *dev, __u8 *local_ip,
-				     __u8 *peer_ip, __be16 local_port,
-				     __be16 peer_port, u8 tos,
-				     __u32 sin6_scope_id)
+struct dst_entry *find_route6(struct chrd_dev *dev, __u8 *local_ip,
+			      __u8 *peer_ip, __be16 local_port,
+			      __be16 peer_port, u8 tos,
+			      __u32 sin6_scope_id)
 {
 	struct dst_entry *dst = NULL;
 
@@ -549,9 +548,9 @@ out:
 	return dst;
 }
 
-static struct dst_entry *find_route(struct chrd_dev *dev, __be32 local_ip,
-				    __be32 peer_ip, __be16 local_port,
-				    __be16 peer_port, u8 tos)
+struct dst_entry *find_route(struct chrd_dev *dev, __be32 local_ip,
+			     __be32 peer_ip, __be16 local_port,
+			     __be16 peer_port, u8 tos)
 {
 	struct rtable *rt;
 	struct flowi4 fl4;
@@ -763,10 +762,10 @@ static int send_halfclose(struct c4iw_ep *ep)
 	INIT_TP_WR(req, ep->hwtid);
 	OPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_CLOSE_CON_REQ,
 						    ep->hwtid));
-	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep);
+	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
 }
 
 static void read_tcb(struct chrd_ep *ep)
 {
 	struct sk_buff *skb;
 	struct cpl_get_tcb *req;
@@ -814,10 +813,10 @@ static int send_abort_req(struct c4iw_ep *ep)
 	INIT_TP_WR(req, ep->hwtid);
 	OPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_ABORT_REQ, ep->hwtid));
 	req->cmd = CPL_ABORT_SEND_RST;
-	return chrd_l2t_send(&ep->com.dev->rdev, req_skb, ep);
+	return chrd_l2t_send(&ep->com.dev->rdev, req_skb, ep->l2t);
 }
 
 static int send_abort(struct chrd_ep *ep)
 {
 	if (!ep->com.qp || !ep->com.qp->srq) {
 		send_abort_req(ep);
@@ -1094,14 +1093,14 @@ static int send_connect(struct c4iw_ep *ep)
 		}
 	}
 	set_bit(ACT_OPEN_REQ, &ep->com.history);
-	ret = chrd_l2t_send(&ep->com.dev->rdev, skb, ep);
+	ret = chrd_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
 	if (ret && ep->com.remote_addr.ss_family == AF_INET6)
 		cxgb4_clip_release(netdev, (const u32 *)&la6->sin6_addr.s6_addr,
 				   1);
 	return ret;
 }
 
 static int send_mpa_req(struct chrd_ep *ep, struct sk_buff *skb,
 			 u8 mpa_rev_to_use)
 {
 	int mpalen, wrlen;
@@ -1193,7 +1192,7 @@ static int send_mpa_req(struct c4iw_ep *ep, struct sk_buff *skb,
 	skb_get(skb);
 	t4_set_arp_err_handler(skb, NULL, arp_failure_discard);
 	ep->mpa_skb = skb;
-	ret = chrd_l2t_send(&ep->com.dev->rdev, skb, ep);
+	ret = chrd_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
 	if (ret) {
 		return ret;
 	}
@@ -1278,10 +1277,10 @@ static int send_mpa_reject(struct c4iw_ep *ep, const void *pdata, u8 plen)
 	t4_set_arp_err_handler(skb, NULL, mpa_start_arp_failure);
 	ep->mpa_skb = skb;
 	ep->snd_seq += mpalen;
-	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep);
+	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
 }
 
 static int send_mpa_reply(struct chrd_ep *ep, const void *pdata, u8 plen)
 {
 	int mpalen, wrlen;
 	struct fw_ofld_tx_data_wr *req;
@@ -1366,12 +1365,12 @@ static int send_mpa_reply(struct c4iw_ep *ep, const void *pdata, u8 plen)
 	ep->mpa_skb = skb;
 	__state_set(&ep->com, MPA_REP_SENT);
 	ep->snd_seq += mpalen;
-	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep);
+	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
 }
 
 static int act_establish(struct chrd_dev *dev, struct sk_buff *skb)
 {
 	struct chrd_ep *ep;
 	struct cpl_act_establish *req = cplhdr(skb);
 	unsigned short tcp_opt = ntohs(req->tcp_opt);
 	unsigned int tid = GET_TID(req);
@@ -1890,7 +1889,6 @@ static int process_mpa_request(struct c4iw_ep *ep, struct sk_buff *skb)
 	if (ep->mpa_pkt_len + skb->len > sizeof(ep->mpa_pkt))
 		goto err_stop_timer;
 
-	pr_debug("enter (%s line %u)\n", __FILE__, __LINE__);
 
 	/*
 	 * Copy the new data into our accumulation buffer.
@@ -1906,7 +1904,6 @@ static int process_mpa_request(struct c4iw_ep *ep, struct sk_buff *skb)
 	if (ep->mpa_pkt_len < sizeof(*mpa))
 		return 0;
 
-	pr_debug("enter (%s line %u)\n", __FILE__, __LINE__);
 	mpa = (struct mpa_message *) ep->mpa_pkt;
 
 	/*
@@ -2203,7 +2200,7 @@ static int send_fw_act_open_req(struct c4iw_ep *ep, unsigned int atid)
 	req->tcb.opt2 = cpu_to_be32(req->tcb.opt2);
 	set_wr_txq(skb, CPL_PRIORITY_CONTROL, ep->ctrlq_idx);
 	set_bit(ACT_OFLD_CONN, &ep->com.history);
-	ret = chrd_l2t_send(&ep->com.dev->rdev, skb, ep);
+	ret = chrd_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
 	return ret;
 }
 
@@ -2437,35 +2434,54 @@ fail2:
 	 */
 	connect_reply_upcall(ep, -ECONNRESET);
 fail1:
 	chrd_put_ep(&ep->com);
 out:
 	return err;
 }
 
 static int act_open_rpl(struct chrd_dev *dev, struct sk_buff *skb)
 {
-	struct chrd_ep *ep;
 	struct cpl_act_open_rpl *rpl = cplhdr(skb);
-	unsigned int atid = G_TID_TID(G_AOPEN_ATID(ntohl(rpl->atid_status)));
 	struct tid_info *t = dev->rdev.lldi.tids;
+	struct sockaddr_in6 *la6;
+	struct sockaddr_in6 *ra6;
+	struct sockaddr_in *la;
+	struct sockaddr_in *ra;
+	struct chrd_qp *qhp;
+	struct chrd_ep *ep;
+	unsigned int atid = G_TID_TID(G_AOPEN_ATID(ntohl(rpl->atid_status)));
 	int status = G_AOPEN_STATUS(ntohl(rpl->atid_status));
 	int ret = 0;
 
-	struct sockaddr_in *la;
-	struct sockaddr_in *ra;
-	struct sockaddr_in6 *la6;
-	struct sockaddr_in6 *ra6;
-	ep = lookup_atid(t, atid);
+	if (rdma_protocol_roce(&dev->ibdev, 1)) {
+		qhp = lookup_atid(t, atid);
+		qhp->roce_attr.hwtid = GET_TID(rpl);
+
+		pbr_debug("qhp 0x%llx atid %u status %u errno %d hwtid %u\n",
+			  (unsigned long long)qhp, atid, status, status2errno(status),
+			  qhp->roce_attr.hwtid);
+
+		//if (status)							/*Todo: Check the Commented Code */
+		//Bhar: we dont want any error so handle accordingly
+
+		//xa_erase_irq(&ep->com.dev->atids, atid);			/*Todo: Check the Commented Code */
+		cxgb4_free_atid(t, atid);
+		chrd_wake_up_deref(qhp->wr_waitp, status2errno(status));
+
+		return 0;
+	} else {
+		ep = lookup_atid(t, atid);
+		pbr_debug("ep %p atid %u status %u errno %d\n", ep, atid,
+			  status, status2errno(status));
+	}
+
 	la = (struct sockaddr_in *)&ep->com.local_addr;
 	ra = (struct sockaddr_in *)&ep->com.remote_addr;
 	la6 = (struct sockaddr_in6 *)&ep->com.local_addr;
 	ra6 = (struct sockaddr_in6 *)&ep->com.remote_addr;
 
-	pr_debug("ep %p atid %u status %u errno %d\n", ep, atid,
-	     status, status2errno(status));
-
 	if (is_neg_adv(status)) {
 		pr_debug("Connection problems for atid %u status %u (%s)\n",
 			 atid, status, neg_adv_str(status));
 		ep->stats.connect_neg_adv++;
 		mutex_lock(&dev->rdev.stats.lock);
@@ -2695,12 +2711,12 @@ static int accept_cr(struct c4iw_ep *ep, struct sk_buff *skb,
 	rpl->opt2 = cpu_to_be32(opt2);
 	set_wr_txq(skb, CPL_PRIORITY_SETUP, ep->ctrlq_idx);
 	t4_set_arp_err_handler(skb, ep, pass_accept_rpl_arp_failure);
-	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep);
+	return chrd_l2t_send(&ep->com.dev->rdev, skb, ep->l2t);
 }
 
 static void reject_cr(struct chrd_dev *dev, u32 hwtid, struct sk_buff *skb)
 {
 	pr_debug("chrd_dev %p tid %u\n", dev, hwtid);
 	skb_trim(skb, sizeof(struct cpl_tid_release));
 	release_tid(&dev->rdev, hwtid, skb);
 	return;
@@ -4315,10 +4331,6 @@ static int rx_pkt(struct c4iw_dev *dev, struct sk_buff *skb)
 	struct neighbour *neigh = NULL;
 	unsigned int chip_ver = CHELSIO_CHIP_VERSION(dev->rdev.lldi.adapter_type);
 
-	/* Drop all non-SYN packets */
-	if (!(cpl->l2info & cpu_to_be32(F_RXF_SYN)))
-		goto reject;
-
 	/* 
 	 * Drop all packets which did not hit the filter.
 	 * Unlikely to happen.
@@ -4326,17 +4338,6 @@ static int rx_pkt(struct c4iw_dev *dev, struct sk_buff *skb)
 	if (!(rss->filter_hit && rss->filter_tid))
 		goto reject;
 
-	/* 
-	 * Calculate the server tid from filter hit index from cpl_rx_pkt.
-	 */
-	stid = cpu_to_be32(rss->hash_val);
-
-	lep = (struct chrd_ep *)get_ep_from_stid(dev, stid);
-	if (!lep) {
-		pr_debug("connect request on invalid stid %d\n", stid);
-		goto reject;
-	}
-
 	switch (chip_ver) {
 	case CHELSIO_T4:
 		eth_hdr_len = G_RX_ETHHDR_LEN(htonl(cpl->l2info));
@@ -4360,6 +4361,73 @@ static int rx_pkt(struct c4iw_dev *dev, struct sk_buff *skb)
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(cpl->vlan));
 	}
 
+	pr_debug("pkt protocol %d\n", iph->protocol);
+	/* Could be a ROCEv2 CM packet, process it accordingly */
+	if (iph->protocol == IPPROTO_UDP) {
+		struct chrd_rdev *rdev = &dev->rdev;
+		struct chrd_cq *gsi_rcq = rdev->gsi_rcq;
+
+		/* Add the received packet to local queue and invoke cq notifier.
+		  When polled, poll cq will eventually frame a CQE and return it
+		  to the RDMA stack */
+
+
+		if (!skb_linearize(skb)) {
+			unsigned int i, j, k;
+			unsigned char *skb_data = (void *)(rss+1);
+			pr_err("Hex dump skb->protocol %x skb_data %llx, skb->data %llx, rss %llx, skb->len %u, rss hdr size %zu\n",
+				skb->protocol, (unsigned long long)skb_data, (unsigned long long)skb->data, (unsigned long long)rss, skb->len, sizeof(struct rss_header));
+			j = ((skb->len - sizeof(struct rss_header)) / 16);
+			for (i = 0; i < j; i++) {
+				pr_err("%04x: %02x %02x %02x %02x %02x %02x %02x %02x  %02x %02x %02x %02x %02x %02x %02x %02x\n", i*16,
+			        	skb_data[0], skb_data[1], skb_data[2], skb_data[3], skb_data[4], skb_data[5],
+					skb_data[6], skb_data[7], skb_data[8], skb_data[9], skb_data[10], skb_data[11],
+					skb_data[12], skb_data[13], skb_data[14], skb_data[15]);
+				skb_data += 16;
+				if (i == (j - 1)) {
+					pr_err("%04x: ", i*16);
+					for (k = 0; k < ((skb->len - sizeof(struct rss_header)) % 16); k++) {
+						printk(KERN_CONT "%02x ", skb_data[k]);
+						if (k == 7)
+							printk(KERN_CONT " ");
+					}
+					pr_err("Bhar: skb_data %llx mod %zu\n", (unsigned long long)skb_data, (skb->len - sizeof(struct rss_header)) % 16);
+				}
+        		}
+
+		} else {
+			pr_err("skb_linearize failed\n");
+		}
+
+		/* Todo: Copy payload starting from bth to the GSI address pointed by
+		   RQE from the top of local RQ. May need spin lock around this code. */
+
+		/* Todo: Maintain GSI conns count based on each packet received */
+
+		/* Add the skb to the gsi sk_buff_head, refer rxe_resp_queue_pkt()*/
+
+		/* complete recieved CM packet */
+		if (gsi_rcq->ibcq.comp_handler)
+			(*gsi_rcq->ibcq.comp_handler)(&gsi_rcq->ibcq, gsi_rcq->ibcq.cq_context);
+
+		return 0;
+	}
+
+	/* Drop all non-SYN packets */
+	if (!(cpl->l2info & cpu_to_be32(F_RXF_SYN)))
+		goto reject;
+
+	/*
+	 * Calculate the server tid from filter hit index from cpl_rx_pkt.
+	 */
+	stid = cpu_to_be32(rss->hash_val);
+
+	lep = (struct chrd_ep *)get_ep_from_stid(dev, stid);
+	if (!lep) {
+		pr_debug("connect request on invalid stid %d\n", stid);
+		goto reject;
+	}
+
 	if (iph->version != 0x4)
 		goto reject;
 
diff --git a/dev/T4/linux/iw_cxgb4/cq.c b/dev/T4/linux/iw_cxgb4/cq.c
index 6102c4bc4..edc8b3d08 100644
--- a/dev/T4/linux/iw_cxgb4/cq.c
+++ b/dev/T4/linux/iw_cxgb4/cq.c
@@ -170,15 +170,16 @@ static void insert_recv_cqe(struct t4_wq *wq, struct t4_cq *cq, u32 srqidx)
 {
 	struct t4_cqe cqe;
 
 	pr_debug("wq %p cq %p sw_cidx %u sw_pidx %u\n",
 	     wq, cq, cq->sw_cidx, cq->sw_pidx);
 	memset(&cqe, 0, sizeof(cqe));
-	cqe.rss.opcode = CPL_RDMA_CQE;
+	cqe.rss.opcode = CPL_ROCE_CQE;
 	cqe.header = cpu_to_be32(V_CQE_STATUS(T4_ERR_SWFLUSH) |
-				 V_CQE_OPCODE(FW_RI_SEND) |
+				// V_CQE_V2_OPCODE(FW_RI_SEND) |		/*Todo: Check the Commented Code */
 				 V_CQE_TYPE(0) |
 				 V_CQE_SWCQE(1) |
 				 V_CQE_QPID(wq->sq.qid));
+	cqe.u.v2_com.v2_header = cpu_to_be32(V_CQE_V2_OPCODE(FW_RI_SEND));
 	cqe.bits_type_ts = cpu_to_be64(V_CQE_GENBIT((u64)cq->gen));
 	if (srqidx)
 		cqe.u.srcqe.abs_rqe_idx = cpu_to_be32(srqidx);
@@ -205,15 +206,16 @@ static void insert_sq_cqe(struct t4_wq *wq, struct t4_cq *cq,
 {
 	struct t4_cqe cqe;
 
 	pr_debug("wq %p cq %p sw_cidx %u sw_pidx %u\n",
 	     wq, cq, cq->sw_cidx, cq->sw_pidx);
 	memset(&cqe, 0, sizeof(cqe));
-	cqe.rss.opcode = CPL_RDMA_CQE;
+	cqe.rss.opcode = CPL_ROCE_CQE;
 	cqe.header = cpu_to_be32(V_CQE_STATUS(T4_ERR_SWFLUSH) |
-				 V_CQE_OPCODE(swcqe->opcode) |
+				// V_CQE_V2_OPCODE(swcqe->opcode) |			/*Todo: Check the Commented Code */
 				 V_CQE_TYPE(1) |
 				 V_CQE_SWCQE(1) |
 				 V_CQE_QPID(wq->sq.qid));
+	cqe.u.v2_com.v2_header = cpu_to_be32(V_CQE_V2_OPCODE(swcqe->opcode));
 	CQE_WRID_SQ_IDX(&cqe) = swcqe->idx;
 	cqe.bits_type_ts = cpu_to_be64(V_CQE_GENBIT((u64)cq->gen));
 	cq->sw_queue[cq->sw_pidx] = cqe;
@@ -263,14 +265,16 @@ static void flush_completed_wrs(struct t4_wq *wq, struct t4_cq *cq)
 	while (cidx != wq->sq.pidx) {
 		swsqe = &wq->sq.sw_sq[cidx];
 		if (!swsqe->signaled) {
+			pr_debug("sq pidx %u swcq sq idx %u cq idx %u\n",
+				 wq->sq.pidx, cidx, cq->sw_pidx);
 			if (++cidx == wq->sq.size)
 				cidx = 0;
 		} else if (swsqe->complete) {
 			/*
 			 * Insert this completed cqe into the swcq.
 			 */
-			pr_debug("moving cqe into swcq sq idx %u cq idx %u\n",
-				 cidx, cq->sw_pidx);
+			pr_debug("moving cqe into sq pidx %u swcq sq idx %u cq idx %u Signaled %d\n",
+				 wq->sq.pidx, cidx, cq->sw_pidx, swsqe->signaled);
 			swsqe->cqe.header |= htonl(V_CQE_SWCQE(1));
 			cq->sw_queue[cq->sw_pidx] = swsqe->cqe;
 			t4_swcq_produce(cq);
@@ -278,8 +282,11 @@ static void flush_completed_wrs(struct t4_wq *wq, struct t4_cq *cq)
 			if (++cidx == wq->sq.size)
 				cidx = 0;
 			wq->sq.flush_cidx = cidx;
-		} else
+		} else {
+			pr_debug("WARNING: SIGNALLED! sq pidx %u swcq sq idx %u cq idx %u Signaled %d\n",
+				 wq->sq.pidx, cidx, cq->sw_pidx, swsqe->signaled);
 			break;
+		}
 	}
 }
 
@@ -296,6 +303,20 @@ static void create_read_req_cqe(struct t4_wq *wq, struct t4_cqe *hw_cqe,
 	read_cqe->bits_type_ts = hw_cqe->bits_type_ts;
 }
 
+static void create_v2_read_req_cqe(struct t4_wq *wq, struct t4_cqe *hw_cqe,
+		struct t4_cqe *read_cqe)
+{
+	read_cqe->rss = hw_cqe->rss;
+	read_cqe->u.scqe.cidx = wq->sq.oldest_read->idx;
+	read_cqe->len = ntohl(wq->sq.oldest_read->read_len);
+	read_cqe->header = htonl(V_CQE_QPID(CQE_QPID(hw_cqe)) |
+			V_CQE_SWCQE(SW_CQE(hw_cqe)) |
+			//V_CQE_V2_OPCODE(FW_RI_READ_REQ) |			/*Todo: Check the Commented Code */
+			V_CQE_TYPE(1));
+	read_cqe->u.v2_com.v2_header = htonl(V_CQE_V2_OPCODE(FW_RI_READ_REQ));
+	read_cqe->bits_type_ts = hw_cqe->bits_type_ts;
+}
+
 static void advance_oldest_read(struct t4_wq *wq)
 {
 
@@ -319,14 +340,15 @@ static void advance_oldest_read(struct t4_wq *wq)
  * Deal with out-of-order and/or completions that complete
  * prior unsignalled WRs.
  */
 void chrd_flush_hw_cq(struct chrd_cq *chp, struct chrd_qp *flush_qhp)
 {
 	struct t4_cqe *hw_cqe, *swcqe, read_cqe;
 	struct chrd_qp *qhp;
 	struct t4_swsqe *swsqe;
+	u32 cqe2qpid;
 	int ret;
 
 	pr_debug(" cqid 0x%x\n", chp->cq.cqid);
 	ret = t4_next_hw_cqe(&chp->cq, &hw_cqe);
 
 	/*
@@ -335,7 +357,10 @@ void c4iw_flush_hw_cq(struct c4iw_cq *chp, struct c4iw_qp *flush_qhp)
 	 * also do any translation magic that poll_cq() normally does.
 	 */
 	while (!ret) {
-		qhp = get_qhp(chp->rhp, CQE_QPID(hw_cqe));
+		cqe2qpid = CQE_QPID(hw_cqe);
+		if (cqe2qpid == 1)
+			cqe2qpid = 1024;		/*Todo: hardcoded value */
+		qhp = get_qhp(chp->rhp, cqe2qpid);
 
 		/*
 		 * drop CQEs with no associated QP
@@ -530,26 +555,41 @@ static u64 reap_srq_cqe(struct t4_cqe *hw_cqe, struct t4_srq *srq)
  *    -EAGAIN       CQE skipped, try again.
  *    -EOVERFLOW    CQ overflow detected.
  */
-static int poll_cq(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,
+static int poll_iw_cq(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,
 		   u8 *cqe_flushed, u64 *cookie, u32 *credit,
 		   struct t4_srq *srq)
 {
 	int ret = 0;
-	struct t4_cqe *hw_cqe, read_cqe;
+	u8 hcqe = 0;
+	struct t4_cqe *hw_cqe, read_cqe = {0};
 
 	*cqe_flushed = 0;
 	*credit = 0;
-	ret = t4_next_cqe(cq, &hw_cqe);
+	ret = t4_next_cqe(cq, &hw_cqe, &hcqe);
 	if (ret)
 		return ret;
+/*Todo : dump code start*/
+#if 0
+/* Dump CQE start */
+        unsigned char *w = (void *)hw_cqe;
+        unsigned int i;
+        pbr_debug("cq 0x%llx cqe addr 0x%llx\n", (unsigned long long)cq, (unsigned long long)hw_cqe);
+        for (i = 0; i < 4; i++) {
+                pbr_debug("0x%02x: %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
+                          i, w[0], w[1], w[2], w[3], w[4], w[5], w[6], w[7],
+                          w[8], w[9], w[10], w[11], w[12], w[13], w[14], w[15]);
+                w = w + 16;
+        }
+/* Dump CQE end */
 
-	pr_debug("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x"
-	     " opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x\n",
+	pr_debug("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x"
+	     " opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x cidx 0x%x sw opc 0x%x hcqe %u\n",
 	     CQE_OVFBIT(hw_cqe), CQE_QPID(hw_cqe),
 	     CQE_GENBIT(hw_cqe), CQE_TYPE(hw_cqe), CQE_STATUS(hw_cqe),
 	     CQE_OPCODE(hw_cqe), CQE_LEN(hw_cqe), CQE_WRID_HI(hw_cqe),
-	     CQE_WRID_LOW(hw_cqe));
-
+	     CQE_WRID_LOW(hw_cqe), CQE_WRID_SQ_IDX(hw_cqe), wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode, hcqe);
+#endif
+/*Todo : dump code start*/
 	/*
 	 * skip cqe's not affiliated with a QP.
 	 */
@@ -673,8 +713,8 @@ static int poll_cq(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,
 	if (!SW_CQE(hw_cqe) && (CQE_WRID_SQ_IDX(hw_cqe) != wq->sq.cidx)) {
 		struct t4_swsqe *swsqe;
 
-		pr_debug("out of order completion going in sw_sq at idx %u\n",
-			 CQE_WRID_SQ_IDX(hw_cqe));
+		pr_err("out of order completion going in sw_sq at idx %u, cidx %u\n",
+			CQE_WRID_SQ_IDX(hw_cqe), wq->sq.cidx);
 		swsqe = &wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)];
 		swsqe->cqe = *hw_cqe;
 		swsqe->complete = 1;
@@ -732,19 +772,361 @@ flush_wq:
 
 skip_cqe:
 	if (SW_CQE(hw_cqe)) {
-		pr_debug("cq %p cqid 0x%x skip sw cqe cidx %u\n",
-			 cq, cq->cqid, cq->sw_cidx);
+		pr_debug("cq %p cqid 0x%x skip sw cqe cidx %u sw_in_use %u\n",
+			 cq, cq->cqid, cq->sw_cidx, cq->sw_in_use);
 		t4_swcq_consume(cq);
 	} else {
-		pr_debug("cq %p cqid 0x%x skip hw cqe cidx %u\n",
-			 cq, cq->cqid, cq->cidx);
+		pr_debug("cq %p cqid 0x%x skip hw cqe cidx %u sw_in_use %u\n",
+			 cq, cq->cqid, cq->cidx, cq->sw_in_use);
 		t4_hwcq_consume(cq);
 	}
 	return ret;
 }
 
 /*
- * Get one cq entry from chrd and map it to openib.
+ * poll_cq
+ *
+ * Caller must:
+ *     check the validity of the first CQE,
+ *     supply the wq assicated with the qpid.
+ *
+ * credit: cq credit to return to sge.
+ * cqe_flushed: 1 iff the CQE is flushed.
+ * cqe: copy of the polled CQE.
+ *
+ * return value:
+ *    0		    CQE returned ok.
+ *    -EAGAIN       CQE skipped, try again.
+ *    -EOVERFLOW    CQ overflow detected.
+ */
+static int poll_roce_cq(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,
+		   u8 *cqe_flushed, u64 *cookie, u32 *credit,
+		   struct t4_srq *srq)
+{
+	struct t4_cqe *hw_cqe, read_cqe = {0};
+	struct t4_swsqe *swsqe;
+	int ret = 0;
+	u8 hcqe = 0;
+
+	*cqe_flushed = 0;
+	*credit = 0;
+	ret = t4_next_cqe(cq, &hw_cqe, &hcqe);
+	if (ret)
+		return ret;
+
+	/*
+	 * skip cqe's not affiliated with a QP.
+	 */
+	if (wq == NULL) {
+		ret = -EAGAIN;
+		goto skip_cqe;
+	}
+
+	/*
+	* skip hw cqe's if the wq is flushed.
+	*/
+	if (wq->flushed && !SW_CQE(hw_cqe)) {
+        	ret = -EAGAIN;
+        	goto skip_cqe;
+    	}
+
+	/*
+	 * Special cqe for drain WR completions...
+	 */
+	if (DRAIN_CQE(hw_cqe)) {
+		*cookie = CQE_DRAIN_COOKIE(hw_cqe);
+		*cqe = *hw_cqe;
+/*Todo : dump code end*/
+#if 0
+/* Dump CQE start */
+		unsigned char *w = (void *)hw_cqe;
+		unsigned int i;
+		pbe_debug("cq 0x%llx cqe addr 0x%llx\n", (unsigned long long)cq, (unsigned long long)hw_cqe);
+		for (i = 0; i < 4; i++) {
+			pbe_debug("0x%02x: %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
+				  i, w[0], w[1], w[2], w[3], w[4], w[5], w[6], w[7],
+				  w[8], w[9], w[10], w[11], w[12], w[13], w[14], w[15]);
+			w = w + 16;
+		}
+/* Dump CQE end */
+		pbe_debug("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x"
+		     " opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x"
+		     " cidx 0x%04x sw opc 0x%x hcqe %u cq 0x%llx v2_header 0x%x SQ pidx %u cidx %u\n",
+		     CQE_OVFBIT(hw_cqe), CQE_QPID(hw_cqe),
+		     CQE_GENBIT(hw_cqe), CQE_TYPE(hw_cqe), CQE_STATUS(hw_cqe),
+		     CQE_V2_OPCODE(hw_cqe), CQE_LEN(hw_cqe), CQE_WRID_HI(hw_cqe),
+		     CQE_WRID_LOW(hw_cqe), CQE_WRID_SQ_IDX(hw_cqe), wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode,
+		     hcqe, (unsigned long long)cq, hw_cqe->u.v2_com.v2_header, wq->sq.pidx, wq->sq.cidx);
+#endif
+/*Todo : dump code end*/
+
+		goto skip_cqe;
+	}
+
+/*Todo : dump code start*/
+#if 0
+/* Dump CQE start */
+	unsigned char *w = (void *)hw_cqe;
+	unsigned int i;
+	pbr_debug("cq 0x%llx cqe addr 0x%llx\n", (unsigned long long)cq, (unsigned long long)hw_cqe);
+	for (i = 0; i < 4; i++) {								/*Todo: hardcoded value */
+		pbr_debug("0x%02x: %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
+			  i, w[0], w[1], w[2], w[3], w[4], w[5], w[6], w[7],
+			  w[8], w[9], w[10], w[11], w[12], w[13], w[14], w[15]);
+		w = w + 16;
+	}
+/* Dump CQE end */
+	pbr_debug("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x"
+	     " opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x"
+	     " cidx 0x%04x sw opc 0x%x hcqe %u cq 0x%llx v2_header 0x%x SQ pidx %u cidx %u\n",
+	     CQE_OVFBIT(hw_cqe), CQE_QPID(hw_cqe),
+	     CQE_GENBIT(hw_cqe), CQE_TYPE(hw_cqe), CQE_STATUS(hw_cqe),
+	     CQE_V2_OPCODE(hw_cqe), CQE_LEN(hw_cqe), CQE_WRID_HI(hw_cqe),
+	     CQE_WRID_LOW(hw_cqe), CQE_WRID_SQ_IDX(hw_cqe), wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode,
+	     hcqe, (unsigned long long)cq, hw_cqe->u.v2_com.v2_header, wq->sq.pidx, wq->sq.cidx);
+#endif
+/*Todo : dump code end*/
+	if (CQE_TYPE(hw_cqe) == 1) { /* If egress CQE, set the opcode as FW doesnt set it */
+		//hw_cqe->header |= cpu_to_be32(V_CQE_SWCQE(0));// Set EXT mode bit to zero to allow sw cqe processing
+		hw_cqe->u.v2_com.v2_header &= cpu_to_be32(~V_CQE_V2_OPCODE(M_CQE_V2_OPCODE));
+//		if (wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode == 1 && hcqe == 1) {
+//			hw_cqe->u.v2_com.v2_header |= cpu_to_be32(V_CQE_V2_OPCODE(FW_RI_READ_RESP));// FW doesnt send opcode so handle it in SW.
+//			hw_cqe->header &= cpu_to_be32(~V_CQE_TYPE(M_CQE_TYPE));
+//		} else {
+//			hw_cqe->u.v2_com.v2_header |= cpu_to_be32(V_CQE_V2_OPCODE(wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode));// FW doesnt send opcode so handle it in SW.
+//		}
+		hw_cqe->u.v2_com.v2_header |= cpu_to_be32(V_CQE_V2_OPCODE(wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode));// FW doesnt send opcode so handle it in SW.
+	} else { /* If inress CQE, set the SW opcode derrived from HW opcode */
+		//hw_cqe->header |= cpu_to_be32(V_CQE_SWCQE(0));// Set EXT mode bit to zero to allow sw cqe processing
+		if (CQE_V2_OPCODE(hw_cqe) < 0x18) {	/*Todo: hardcoded value */
+			hw_cqe->u.v2_com.v2_header &= cpu_to_be32(~V_CQE_V2_OPCODE(M_CQE_V2_OPCODE));
+			hw_cqe->u.v2_com.v2_header |= cpu_to_be32(V_CQE_V2_OPCODE(v2_ib_opc_to_fw_opc(CQE_V2_OPCODE(hw_cqe))));// Set opcode to temporarily progress
+		} else {
+			pr_err("Unexpected ingress opcode: opcode %u v2_opcode %u\n",
+				  CQE_OPCODE(hw_cqe), CQE_V2_OPCODE(hw_cqe));
+			BUG_ON(1);
+		}
+	}
+/*Todo: dump code start*/
+#if 0
+/* Dump CQE start */
+	w = (void *)hw_cqe;
+	i = 0;
+	pbr_debug("cq 0x%llx cqe addr 0x%llx\n", (unsigned long long)cq, (unsigned long long)hw_cqe);
+	for (i = 0; i < 4; i++) {
+		pbr_debug("0x%02x: %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
+			  i, w[0], w[1], w[2], w[3], w[4], w[5], w[6], w[7],
+			  w[8], w[9], w[10], w[11], w[12], w[13], w[14], w[15]);
+		w = w + 16;
+	}
+/* Dump CQE end */
+	pbr_debug("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x"
+	     " opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x"
+	     " cidx 0x%04x sw opc 0x%x hcqe %u cq 0x%llx v2_header 0x%x SQ pidx %u cidx %u\n",
+	     CQE_OVFBIT(hw_cqe), CQE_QPID(hw_cqe),
+	     CQE_GENBIT(hw_cqe), CQE_TYPE(hw_cqe), CQE_STATUS(hw_cqe),
+	     CQE_V2_OPCODE(hw_cqe), CQE_LEN(hw_cqe), CQE_WRID_HI(hw_cqe),
+	     CQE_WRID_LOW(hw_cqe), CQE_WRID_SQ_IDX(hw_cqe), wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode,
+	     hcqe, (unsigned long long)cq, hw_cqe->u.v2_com.v2_header, wq->sq.pidx, wq->sq.cidx);
+#endif
+/*Todo: dump code start*/
+	/*
+        * skip TERMINATE cqes...
+        */
+        if (CQE_V2_OPCODE(hw_cqe) == FW_RI_TERMINATE) {
+		ret = -EAGAIN;
+                goto skip_cqe;
+        }
+
+	/*
+	 * Gotta tweak READ completions:
+	 *	1) the cqe doesn't contain the sq_wptr from the wr.
+	 *	2) opcode not reflected from the wr.
+	 *	3) read_len not reflected from the wr.
+	 *	4) T4 HW (for now) inserts target read response failures which
+	 * 	   need to be skipped.
+	 */
+	if (CQE_V2_OPCODE(hw_cqe) == FW_RI_READ_RESP) {
+
+		/*
+		 * If we have reached here because of async
+		 * event or other error, and have egress error
+		 * then drop
+		 */
+		BUG_ON(1);
+		if (CQE_TYPE(hw_cqe) == 1) {
+			if (CQE_STATUS(hw_cqe))
+				t4_set_wq_in_error(wq, 0);
+			ret = -EAGAIN;
+			goto skip_cqe;
+		}
+
+		/*
+		 * If this is an unsolicited read response, then the read
+		 * was generated by the kernel driver as part of peer-2-peer
+		 * connection setup.  So ignore the completion.
+		 */
+		if (CQE_WRID_STAG(hw_cqe) == 1) {
+			if (CQE_STATUS(hw_cqe))
+				t4_set_wq_in_error(wq, 0);
+			ret = -EAGAIN;
+			goto skip_cqe;
+		}
+
+		/*
+		 * Eat completions for unsignaled read WRs.
+		 */
+		if (wq->sq.oldest_read) {
+			pbr_debug("Oldest Read 0x%llx\n", (unsigned long long)wq->sq.oldest_read);
+			if (!wq->sq.oldest_read->signaled) {
+				advance_oldest_read(wq);
+				ret = -EAGAIN;
+				goto skip_cqe;
+			}
+	
+			/*
+			 * Don't write to the HWCQ, so create a new read req CQE
+			 * in local memory.
+			 */
+			create_v2_read_req_cqe(wq, hw_cqe, &read_cqe);
+			hw_cqe = &read_cqe;
+			advance_oldest_read(wq);
+			pbr_debug("Oldest Read 0x%llx\n", (unsigned long long)wq->sq.oldest_read);
+		pbr_debug("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x"
+		     " opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x"
+		     " cidx 0x%04x sw opc 0x%x hcqe %u cq 0x%llx v2_header 0x%x\n",
+		     CQE_OVFBIT(hw_cqe), CQE_QPID(hw_cqe),
+		     CQE_GENBIT(hw_cqe), CQE_TYPE(hw_cqe), CQE_STATUS(hw_cqe),
+		     CQE_V2_OPCODE(hw_cqe), CQE_LEN(hw_cqe), CQE_WRID_HI(hw_cqe),
+		     CQE_WRID_LOW(hw_cqe), CQE_WRID_SQ_IDX(hw_cqe), wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode,
+		     hcqe, (unsigned long long)cq, hw_cqe->u.v2_com.v2_header);
+		} else {
+			mdelay(100000); // delay processing for fw dump collection
+			BUG_ON(1);
+		}
+	}
+
+	if (CQE_STATUS(hw_cqe) || t4_wq_in_error(wq)) {
+		*cqe_flushed = (CQE_STATUS(hw_cqe) == T4_ERR_SWFLUSH);
+		t4_set_wq_in_error(wq, 0);
+	}
+
+	/*
+	 * RECV completion.
+	 */
+	if (RQ_TYPE(hw_cqe)) {
+
+		/*
+		 * HW only validates 4 bits of MSN.  So we must validate that
+		 * the MSN in the SEND is the next expected MSN.  If its not,
+		 * then we complete this with T4_ERR_MSN and mark the wq in
+		 * error.
+		 */
+#if 0 // untill msn is fixed in fw/hw
+		if (unlikely(!CQE_STATUS(hw_cqe) &&
+			     CQE_WRID_MSN(hw_cqe) != wq->rq.msn)) {
+			pr_err("Poll failure!\n");
+			pr_err(" MSN %u msn %u\n", CQE_WRID_MSN(hw_cqe), wq->rq.msn);
+			t4_set_wq_in_error(wq, 0);
+			hw_cqe->header |= cpu_to_be32(V_CQE_STATUS(T4_ERR_MSN));
+		}
+#endif
+		goto proc_cqe;
+	}
+
+	swsqe = &wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)];
+	if (!swsqe->signaled) {
+		pr_err("%s:%d WARNING: UNSIGNALLED COMPLETION @ %u!!\n", __func__, __LINE__, CQE_WRID_SQ_IDX(hw_cqe));
+		ret = -EAGAIN;
+		goto skip_cqe;
+	}
+
+	/*
+	 * If we get here its a send completion.
+	 *
+	 * Handle out of order completion. These get stuffed
+	 * in the SW SQ. Then the SW SQ is walked to move any
+	 * now in-order completions into the SW CQ.  This handles
+	 * 2 cases:
+	 *	1) reaping unsignaled WRs when the first subsequent
+	 *	   signaled WR is completed.
+	 *	2) out of order read completions.
+	 */
+	if (!SW_CQE(hw_cqe) && (CQE_WRID_SQ_IDX(hw_cqe) != wq->sq.cidx)) {
+		struct t4_swsqe *swsqe;
+
+		pbr_debug("out of order completion going in sw_sq at idx %u, cidx %u\n",
+			 CQE_WRID_SQ_IDX(hw_cqe), wq->sq.cidx);
+		swsqe = &wq->sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)];
+		swsqe->cqe = *hw_cqe;
+		swsqe->complete = 1;
+		ret = -EAGAIN;
+		goto flush_wq;
+	}
+
+proc_cqe:
+	*cqe = *hw_cqe;
+
+	/*
+	 * Reap the associated WR(s) that are freed up with this
+	 * completion.
+	 */
+	if (SQ_TYPE(hw_cqe)) {
+		int idx = CQE_WRID_SQ_IDX(hw_cqe);
+
+		/*
+		* Account for any unsignaled completions completed by
+		* this signaled completion.  In this case, cidx points
+		* to the first unsignaled one, and idx points to the
+		* signaled one.  So adjust in_use based on this delta.
+		* if this is not completing any unsigned wrs, then the
+		* delta will be 0. Handle wrapping also!
+		*/
+		if (idx < wq->sq.cidx)
+			wq->sq.in_use -= wq->sq.size + idx - wq->sq.cidx;
+		else
+			wq->sq.in_use -= idx - wq->sq.cidx;
+
+		wq->sq.cidx = (uint16_t)idx;
+		pr_debug("completing sq idx %u sq inuse %u\n", wq->sq.cidx, wq->sq.in_use);
+		*cookie = wq->sq.sw_sq[wq->sq.cidx].wr_id;
+		if (chrd_wr_log)
+			chrd_log_wr_stats(wq, hw_cqe);
+		t4_sq_consume(wq);
+	} else {
+		if (!srq) {
+			pbr_debug("completing rq idx %u\n", wq->rq.cidx);
+			*cookie = wq->rq.sw_rq[wq->rq.cidx].wr_id;
+			if (chrd_wr_log)
+				chrd_log_wr_stats(wq, hw_cqe);
+			t4_rq_consume(wq);
+		} else
+			*cookie = reap_srq_cqe(hw_cqe, srq);
+		wq->rq.msn++;
+		goto skip_cqe;
+	}
+
+flush_wq:
+	/*
+	 * Flush any completed cqes that are now in-order.
+	 */
+	flush_completed_wrs(wq, cq);
+
+skip_cqe:
+	if (SW_CQE(hw_cqe)) {
+		pr_debug("sw cq 0x%llx cqid 0x%x skip sw cqe cidx %u sw_in_use %u\n",
+			 (unsigned long long)cq, cq->cqid, cq->sw_cidx, cq->sw_in_use);
+		t4_swcq_consume(cq);
+	} else {
+		pr_debug("hw cq 0x%llx cqid 0x%x skip sw cqe cidx %u sw_in_use %u\n",
+			 (unsigned long long)cq, cq->cqid, cq->sw_cidx, cq->sw_in_use);
+		t4_hwcq_consume(cq);
+	}
+	return ret;
+}
+
+/*
+ * Get one cq entry from chrd and map it to openib.
  *
  * Returns:
  *	0			cqe returned
@@ -752,23 +1134,32 @@ skip_cqe:
  *	-EAGAIN			caller must try again
  *	any other -errno	fatal error
  */
-static int chrd_poll_cq_one(struct chrd_cq *chp, struct ib_wc *wc)
+static int chrd_poll_cq_one(struct chrd_cq *chp, struct ib_wc *wc,
+			    enum qp_transport_type prot)
 {
 	struct chrd_qp *qhp = NULL;
 	struct t4_cqe cqe, *rd_cqe;
 	struct t4_wq *wq;
 	u32 credit = 0;
-	u8 cqe_flushed;
+	u8 hcqe = 0;
+	u8 cqe_flushed, cqe_opc;
+	u64 smac;
+	u32 cqe2qpid;
 	u64 cookie = 0;
 	int ret;
 	struct chrd_srq *srq = NULL;
 
-	ret = t4_next_cqe(&chp->cq, &rd_cqe);
+	ret = t4_next_cqe(&chp->cq, &rd_cqe, &hcqe);
+	pr_debug("next cqe ret %d\n", ret);
 
 	if (ret)
 		return ret;
 
-	qhp = get_qhp(chp->rhp, CQE_QPID(rd_cqe));
+	cqe2qpid = CQE_QPID(rd_cqe);
+	if (cqe2qpid == 1)
+		cqe2qpid = 1024;		/*Todo: hardcoded value */
+	pr_debug("cqe qpid %d\n", cqe2qpid);
+	qhp = get_qhp(chp->rhp, cqe2qpid);
 	if (!qhp)
 		wq = NULL;
 	else {
@@ -778,27 +1169,53 @@ static int c4iw_poll_cq_one(struct c4iw_cq *chp, struct ib_wc *wc)
 		if (srq)
 			spin_lock(&srq->lock);
 	}
-	ret = poll_cq(wq, &(chp->cq), &cqe, &cqe_flushed, &cookie, &credit,
-		      srq ? &srq->wq : NULL);
-	if (ret)
+
+	if (prot)
+		ret = poll_roce_cq(wq, &(chp->cq), &cqe, &cqe_flushed, &cookie, &credit,
+				   srq ? &srq->wq : NULL);
+	else
+		ret = poll_iw_cq(wq, &(chp->cq), &cqe, &cqe_flushed, &cookie, &credit,
+			      srq ? &srq->wq : NULL);
+	if (ret) {
+		pbr_debug("ret %d\n", ret);
 		goto out;
+	}
 
+	memset(wc, 0, sizeof(struct ib_wc));
 	wc->wr_id = cookie;
 	wc->qp = &qhp->ibqp;
 	wc->vendor_err = CQE_STATUS(&cqe);
 	wc->wc_flags = 0;
-
+/*Todo: dump code start*/
+#if 0
+/* Dump CQE start */
+	unsigned char *w = (void *)&cqe;
+	unsigned int i;
+	pbr_debug("cq 0x%llx cqe addr 0x%llx\n", (unsigned long long)&(chp->cq), (unsigned long long)&cqe);
+	for (i = 0; i < 4; i++) {							/*Todo: hardcoded value*/
+		pbr_debug("0x%02x: %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
+			  i, w[0], w[1], w[2], w[3], w[4], w[5], w[6], w[7],
+			  w[8], w[9], w[10], w[11], w[12], w[13], w[14], w[15]);
+		w = w + 16;
+	}
+/* Dump CQE end */
+#endif
+/*Todo: dump code start*/
 	/*
 	 * Simulate a SRQ_LIMIT_REACHED HW notification if required.
 	 */
 	if (srq && !(srq->flags & T4_SRQ_LIMIT_SUPPORT) && srq->armed &&
 	    srq->wq.in_use < srq->srq_limit)
 		chrd_dispatch_srq_limit_reached_event(srq);
 
-	pr_debug("qpid 0x%x type %d opcode %d status 0x%x len %u wrid hi 0x%x "
-	     "lo 0x%x cookie 0x%llx\n", CQE_QPID(&cqe),
-	     CQE_TYPE(&cqe), CQE_OPCODE(&cqe), CQE_STATUS(&cqe), CQE_LEN(&cqe),
-	     CQE_WRID_HI(&cqe), CQE_WRID_LOW(&cqe), (unsigned long long)cookie);
+	if (prot)
+		cqe_opc = CQE_V2_OPCODE(&cqe);
+	else
+		cqe_opc = CQE_OPCODE(&cqe);
+	pr_debug("wc 0x%llx qpid 0x%x type %d opcode %d status 0x%x len %u wrid hi 0x%x "
+		 "lo 0x%x cookie 0x%llx\n", (unsigned long long)wc, CQE_QPID(&cqe),
+		 CQE_TYPE(&cqe), cqe_opc, CQE_STATUS(&cqe), CQE_LEN(&cqe),
+		 CQE_WRID_HI(&cqe), CQE_WRID_LOW(&cqe), (unsigned long long)cookie);
 
 	if (CQE_TYPE(&cqe) == 0) {
 		if (!CQE_STATUS(&cqe))
@@ -806,16 +1223,31 @@ static int c4iw_poll_cq_one(struct c4iw_cq *chp, struct ib_wc *wc)
 		else
 			wc->byte_len = 0;
 
-		switch (CQE_OPCODE(&cqe)) {
+		switch (cqe_opc) {
 		case FW_RI_SEND:
+			pr_debug("Check!\n");
 			wc->opcode = IB_WC_RECV;
+			if (prot) {
+				wc->src_qp = 1; /*ToDO: change harcoded value to the one from cqe*/
+				wc->slid = 0;
+			
+				wc->network_hdr_type = (be64_to_cpu(cqe.v2_ext_hi)>>8 & 1) ? RDMA_NETWORK_IPV6 :
+				                           RDMA_NETWORK_IPV4;
+				smac = be64_to_cpu(cqe.v2_ext_lo);
+//				wc->wc_flags |=
+//					IB_WC_GRH | IB_WC_WITH_NETWORK_HDR_TYPE | IB_WC_IP_CSUM_OK | IB_WC_WITH_SMAC;
+//				ether_addr_copy(wc->smac, (u8 *)&smac);						/*Todo: Check the Commented Code */
+//				pr_debug("SMAC %pM nw type %d smac 0x%LLX\n", wc->smac, wc->network_hdr_type, smac);
+				wc->wc_flags |= IB_WC_GRH | IB_WC_WITH_NETWORK_HDR_TYPE;
+				pr_debug("smac 0x%llX\n", smac);
+			}
 			break;
 		case FW_RI_SEND_WITH_INV:
 		case FW_RI_SEND_WITH_SE_INV:
 			wc->opcode = IB_WC_RECV;
 			wc->ex.invalidate_rkey = CQE_WRID_STAG(&cqe);
 			wc->wc_flags |= IB_WC_WITH_INVALIDATE;
 			chrd_invalidate_mr(qhp->rhp, wc->ex.invalidate_rkey);
 			break;
 		case FW_RI_WRITE_IMMEDIATE:
 			wc->opcode = IB_WC_RECV_RDMA_WITH_IMM;
@@ -823,14 +1255,14 @@ static int c4iw_poll_cq_one(struct c4iw_cq *chp, struct ib_wc *wc)
 			wc->wc_flags |= IB_WC_WITH_IMM;
 			break;
 		default:
-			pr_err("Unexpected opcode %d "
+			pr_err("%s-%d: Unexpected opcode %d "
 			       "in the CQE received for QPID=0x%0x\n",
-			       CQE_OPCODE(&cqe), CQE_QPID(&cqe));
+			       __func__, __LINE__, cqe_opc, CQE_QPID(&cqe));
 			ret = -EINVAL;
 			goto out;
 		}
 	} else {
-		switch (CQE_OPCODE(&cqe)) {
+		switch (cqe_opc) {
 		case FW_RI_WRITE_IMMEDIATE:
 		case FW_RI_RDMA_WRITE:
 			wc->opcode = IB_WC_RDMA_WRITE;
@@ -847,6 +1279,7 @@ static int c4iw_poll_cq_one(struct c4iw_cq *chp, struct ib_wc *wc)
 		case FW_RI_SEND:
 		case FW_RI_SEND_WITH_SE:
 			wc->opcode = IB_WC_SEND;
+			//wc->byte_len = 256;// Hack: add byte len
 			break;
 		case FW_RI_LOCAL_INV:
 			wc->opcode = IB_WC_LOCAL_INV;
@@ -856,12 +1289,13 @@ static int c4iw_poll_cq_one(struct c4iw_cq *chp, struct ib_wc *wc)
 
 			/* Invalidate the MR if the fastreg failed */
 			if (CQE_STATUS(&cqe) != T4_ERR_SUCCESS)
 				chrd_invalidate_mr(qhp->rhp,
 						   CQE_WRID_FR_STAG(&cqe));
 			break;
 		default:
-			pr_err("Unexpected opcode %d in the CQE received for QPID=0x%0x\n",
-			       CQE_OPCODE(&cqe), CQE_QPID(&cqe));
+			pr_err("%s-%d: Unexpected opcode %d "
+			       "in the CQE received for QPID=0x%0x\n",
+			       __func__, __LINE__, cqe_opc, CQE_QPID(&cqe));
 			ret = -EINVAL;
 			goto out;
 		}
@@ -929,58 +1363,81 @@ out:
 	return ret;
 }
 
 int chrd_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)
 {
 	struct chrd_cq *chp;
+	enum qp_transport_type prot;
 	unsigned long flags;
-	int npolled;
+	int npolled, j;
 	int err = 0;
+	struct ib_wc *twc = wc;
 
+	if (rdma_protocol_roce(ibcq->device, 1))
+		prot = CHRD_TRANSPORT_ROCEV2;
+	else
+		prot = CHRD_TRANSPORT_IWARP;
+
 	chp = to_chrd_cq(ibcq);
+	pbr_debug("chp %p\n", chp);
 
 	spin_lock_irqsave(&chp->lock, flags);
 	for (npolled = 0; npolled < num_entries; ++npolled) {
 		do {
-			err = chrd_poll_cq_one(chp, wc + npolled);
+			err = chrd_poll_cq_one(chp, wc + npolled, prot);
 		} while (err == -EAGAIN);
 		if (err)
 			break;
 	}
+	if (npolled) {
+	for (j = 0; j < npolled; j++) {
+		pbr_debug("npolled %d j %d status %d opcode %d wr_id 0x%llx "
+			  "byte_len %u src_qp %u slid %u wc_flags %d "
+			  "pkey_index %u sl %u dlid_path_bits %u "
+			  "port_num %u smac %x%x%x%x%x%x"
+			  "vlan_id %u network_hdr_type %u\n",
+			  npolled, j, twc->status, twc->opcode, twc->wr_id,
+			  twc->byte_len, twc->src_qp, twc->slid, twc->wc_flags,
+			  twc->pkey_index, twc->sl, twc->dlid_path_bits,
+			  twc->port_num, twc->smac[0], twc->smac[1], twc->smac[2], twc->smac[3],
+			  twc->smac[4], twc->smac[5], twc->vlan_id, twc->network_hdr_type);
+		twc = twc + 1;
+		}
+	}
 	spin_unlock_irqrestore(&chp->lock, flags);
 	return !err || err == -ENODATA ? npolled : err;
 }
 
 int chrd_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata)
 {
 	struct chrd_cq *chp;
 	struct chrd_ucontext *ucontext;
 
 	pr_debug("ib_cq %p\n", ib_cq);
 	chp = to_chrd_cq(ib_cq);
 
 	xa_erase_irq(&chp->rhp->cqs, chp->cq.cqid);
 	atomic_dec(&chp->refcnt);
 	wait_event(chp->wait, !atomic_read(&chp->refcnt));
 
 	ucontext = rdma_udata_to_drv_context(udata, struct chrd_ucontext,
 					     ibucontext);
 	destroy_cq(&chp->rhp->rdev, &chp->cq,
 		   ucontext ? &ucontext->uctx : &chp->cq.rdev->uctx,
 		   chp->destroy_skb, chp->wr_waitp);
 	chrd_put_wr_wait(chp->wr_waitp);
 
 	return 0;
 }
 
 int chrd_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
 		   struct ib_udata *udata)
 {
 	struct ib_device *ibdev = ibcq->device;
 	int entries = attr->cqe;
 	int vector = attr->comp_vector;
 	struct chrd_dev *rhp = to_chrd_dev(ibcq->device);
 	struct chrd_cq *chp = to_chrd_cq(ibcq);
 	struct chrd_create_cq_resp uresp;
 	int ret, wr_len;
 	size_t memsize, hwentries;
 	struct chrd_mm_entry *mm, *mm2;
	struct chrd_ucontext *ucontext = rdma_udata_to_drv_context(
 		udata, struct chrd_ucontext, ibucontext);
 
 	pr_debug("ib_dev %p entries %d\n", ibdev, entries);
 	if (attr->flags)
 		return -EINVAL;
 
 	if (vector >= rhp->rdev.lldi.nciq)
 		return -EINVAL;
 
 	chp->wr_waitp = chrd_alloc_wr_wait(GFP_KERNEL);
 	if (!chp->wr_waitp) {
 		ret = -ENOMEM;
 		goto err_free_chp;
 	}
 	chrd_init_wr_wait(chp->wr_waitp);
 
 	wr_len = sizeof(struct fw_ri_res_wr) + sizeof(struct fw_ri_res);
 	chp->destroy_skb = alloc_skb(wr_len, GFP_KERNEL);
@@ -1101,9 +1558,9 @@ int c4iw_create_cq(struct ib_cq *ibcq, const struct ib_cq_init_attr *attr,
 		insert_mmap(ucontext, mm2);
 	}
 
-	pr_debug("cqid 0x%0x chp %p size %u memsize %zu, dma_addr %pad\n",
-		 chp->cq.cqid, chp, chp->cq.size, chp->cq.memsize,
-		 &chp->cq.dma_addr);
+	pr_debug("cqid 0x%0x chp 0x%llx size %u memsize %zu, dma_addr %pad vector %u\n",
+		 chp->cq.cqid, (unsigned long long)chp, chp->cq.size, chp->cq.memsize,
+		 &chp->cq.dma_addr, vector);
 	return 0;
 err_free_mm2:
 	kfree(mm2);
diff --git a/dev/T4/linux/iw_cxgb4/device.c b/dev/T4/linux/iw_cxgb4/device.c
index 6bbab177e..c645f4a61 100644
--- a/dev/T4/linux/iw_cxgb4/device.c
+++ b/dev/T4/linux/iw_cxgb4/device.c
@@ -69,14 +69,19 @@ MODULE_PARM_DESC(wd_disable_inaddr_any,
 		 "Disables reserving 1/2 of the WD filter space for INADDR_ANY "
 		 "mappings (default 0).");
 
+int roce_mode = 0;
+module_param(roce_mode, int, 0644);
+MODULE_PARM_DESC(roce_mode, "Enables RoCE mode, 1 = roce mode, 0 = iWARP mode "
+			    "(default 0)");
+
 static LIST_HEAD(uld_ctx_list);
 static DEFINE_MUTEX(dev_mutex);
 static struct workqueue_struct *reg_workq;
 
 static struct dentry *chrd_debugfs_root;
 
 struct chrd_debugfs_data {
 	struct chrd_dev *devp;
 	char *buf;
 	int bufsize;
 	int pos;
@@ -362,8 +367,8 @@ int dump_raw_qp(int id, struct c4iw_raw_qp *rqp, struct c4iw_debugfs_data *qpd)
 		      rqp->txq.cntxt_id, rqp->state,
 		      rqp->txq.flags & T4_SQ_ONCHIP, rqp->netdev->name,
 		      rqp->fid, rqp->fid + rqp->nfids - 1,
-		      rqp->fid + rqp->dev->rdev.nfids,
-		      rqp->fid + rqp->dev->rdev.nfids + rqp->nfids - 1);
+		      rqp->fid + rqp->rhp->rdev.nfids,
+		      rqp->fid + rqp->rhp->rdev.nfids + rqp->nfids - 1);
 	if (cc < space)
 		qpd->pos += cc;
 	return 0;
@@ -549,6 +554,9 @@ static int stats_show(struct seq_file *seq, void *v)
 	seq_printf(seq, "   PBLMEM: %10llu %10llu %10llu %10llu\n",
 		   dev->rdev.stats.pbl.total, dev->rdev.stats.pbl.cur,
 		   dev->rdev.stats.pbl.max, dev->rdev.stats.pbl.fail);
+	seq_printf(seq, "  RRQTMEM: %10llu %10llu %10llu %10llu\n",
+		   dev->rdev.stats.rrqt.total, dev->rdev.stats.rrqt.cur,
+		   dev->rdev.stats.rrqt.max, dev->rdev.stats.rrqt.fail);
 	seq_printf(seq, "   RQTMEM: %10llu %10llu %10llu %10llu\n",
 		   dev->rdev.stats.rqt.total, dev->rdev.stats.rqt.cur,
 		   dev->rdev.stats.rqt.max, dev->rdev.stats.rqt.fail);
@@ -589,6 +597,8 @@ static ssize_t stats_clear(struct file *file, const char __user *buf,
 	dev->rdev.stats.stag.fail = 0;
 	dev->rdev.stats.pbl.max = 0;
 	dev->rdev.stats.pbl.fail = 0;
+	dev->rdev.stats.rrqt.max = 0;
+	dev->rdev.stats.rrqt.fail = 0;
 	dev->rdev.stats.rqt.max = 0;
 	dev->rdev.stats.rqt.fail = 0;
 	dev->rdev.stats.srqt.max = 0;
@@ -975,22 +985,28 @@ static int c4iw_rdev_open(struct c4iw_rdev *rdev)
 	rdev->stats.pd.total = T4_MAX_NUM_PD;
 	rdev->stats.stag.total = rdev->lldi.vr->stag.size;
 	rdev->stats.pbl.total = rdev->lldi.vr->pbl.size;
+	rdev->stats.rrqt.total = rdev->lldi.vr->rrq.size;
 	rdev->stats.rqt.total = rdev->lldi.vr->rq.size;
 	rdev->stats.srqt.total = rdev->lldi.vr->srq.size;
 	rdev->stats.ocqp.total = rdev->lldi.vr->ocq.size;
 	rdev->stats.qid.total = rdev->lldi.vr->qp.size;
 
 	err = chrd_init_resource(rdev, chrd_num_stags(rdev), T4_MAX_NUM_PD, rdev->lldi.vr->srq.size);
 	if (err) {
 		pr_err("error %d initializing resources\n", err);
 		goto err_free_fids;
 	}
 	err = chrd_pblpool_create(rdev);
 	if (err) {
 		pr_err("error %d initializing pbl pool\n", err);
 		goto err_destroy_resource;
 	}
+	err = chrd_rrqtpool_create(rdev);
+	if (err) {
+		pr_err("error %d initializing rrqt pool\n", err);
+		goto err_destroy_rrqtpool;
+	}
 	err = chrd_rqtpool_create(rdev);
 	if (err) {
 		pr_err("error %d initializing rqt pool\n", err);
 		goto err_destroy_pblpool;
@@ -1024,8 +1040,10 @@ static int c4iw_rdev_open(struct c4iw_rdev *rdev)
 	rdev->status_page->db_off = 0;
 
 	init_completion(&rdev->rqt_compl);
+	init_completion(&rdev->rrqt_compl);
 	init_completion(&rdev->pbl_compl);
 	kref_init(&rdev->rqt_kref);
+	kref_init(&rdev->rrqt_kref);
 	kref_init(&rdev->pbl_kref);
 
 	rdev->free_workq = create_singlethread_workqueue("iw_cxgb4_free");
@@ -1039,21 +1057,23 @@ err_free_status_page:
 	dma_free_coherent(rdev->lldi.dev, PAGE_SIZE, rdev->status_page,
 			  rdev->daddr);
 err_destroy_rqtpool:
 	chrd_rqtpool_destroy(rdev);
 err_destroy_pblpool:
 	chrd_pblpool_destroy(rdev);
+err_destroy_rrqtpool:
+	chrd_rrqtpool_destroy(rdev);
 err_destroy_resource:
 	chrd_destroy_resource(&rdev->resource);
 err_free_fids:
 	kfree(rdev->fids);
 err:
 	return err;
 }
 
 static void chrd_rdev_close(struct chrd_rdev *rdev)
 {
 	if (rdev->wr_log)
 		kfree(rdev->wr_log);
 	dma_free_coherent(rdev->lldi.dev, PAGE_SIZE, rdev->status_page,
 			  rdev->daddr);
 	kfree(rdev->fids);
	chrd_pblpool_destroy(rdev);
 	chrd_rqtpool_destroy(rdev);
+	chrd_rrqtpool_destroy(rdev);
 	wait_for_completion(&rdev->pbl_compl);
 	wait_for_completion(&rdev->rqt_compl);
+	wait_for_completion(&rdev->rrqt_compl);
 	destroy_workqueue(rdev->free_workq);
 	chrd_destroy_resource(&rdev->resource);
 }
 
 void chrd_dealloc(struct uld_ctx *ctx)
 {
 	chrd_rdev_close(&ctx->dev->rdev);
 	WARN_ON(!xa_empty(&ctx->dev->cqs));
 	WARN_ON(!xa_empty(&ctx->dev->qps));
 	WARN_ON(!xa_empty(&ctx->dev->mrs));
@@ -1137,6 +1159,7 @@ static struct c4iw_dev *c4iw_alloc(const struct cxgb4_lld_info *infop)
 
 	devp->rdev.hw_queue.t4_eq_status_entries =
 		devp->rdev.lldi.sge_egrstatuspagesize / 64;
+	pr_info("t4_eq_status_entries %d\n", devp->rdev.hw_queue.t4_eq_status_entries);
 	devp->rdev.hw_queue.t4_max_eq_size = 65520;
 	devp->rdev.hw_queue.t4_max_iq_size = 65520;
 	devp->rdev.hw_queue.t4_max_rq_size = 8192 -
@@ -1497,12 +1520,12 @@ static void resume_rc_qp(struct c4iw_qp *qp)
 	spin_unlock(&qp->lock);
 }
 
 static void resume_raw_qp(struct chrd_raw_qp *qp)
 {
 	u32 val = 0;
-	unsigned int chip_ver = CHELSIO_CHIP_VERSION(qp->dev->rdev.lldi.adapter_type);
+	unsigned int chip_ver = CHELSIO_CHIP_VERSION(qp->rhp->rdev.lldi.adapter_type);
 	writel(V_QID(qp->txq.cntxt_id) | V_PIDX(qp->txq.pidx_inc),
-	       qp->dev->rdev.lldi.db_reg);
+	       qp->rhp->rdev.lldi.db_reg);
 	qp->txq.pidx_inc = 0;
 
 	switch (chip_ver) {
@@ -1520,11 +1543,11 @@ static void resume_raw_qp(struct c4iw_raw_qp *qp)
 	}
 
 	writel(V_QID(qp->fl.cntxt_id) | val,
-	       qp->dev->rdev.lldi.db_reg);
+	       qp->rhp->rdev.lldi.db_reg);
 	qp->fl.pidx_inc = 0;
 }
 
 static void resume_raw_srq(struct chrd_raw_srq *srq)
 {
 	u32 val = 0;
 	unsigned int chip_ver = CHELSIO_CHIP_VERSION(srq->dev->rdev.lldi.adapter_type);
@@ -1685,3 +1708,3 @@ static void recover_lost_dbs(struct uld_ctx *ctx, struct qp_list *qp_list)
 		}
 	}
 	for (idx = 0; idx < qp_list->ridx; idx++) {
		struct chrd_raw_qp *rqp = qp_list->rqps[idx];
 
-		spin_lock_irq(&rqp->dev->lock);
-		ret = cxgb4_sync_txq_pidx(rqp->dev->rdev.lldi.ports[0],
+		spin_lock_irq(&rqp->rhp->lock);
+		ret = cxgb4_sync_txq_pidx(rqp->rhp->rdev.lldi.ports[0],
 					  rqp->txq.cntxt_id,
 					  t4_txq_host_wq_pidx(&rqp->txq),
 					  t4_txq_wq_size(&rqp->txq));
@@ -1701,7 +1724,7 @@ static void recover_lost_dbs(struct uld_ctx *ctx, struct qp_list *qp_list)
 		}
 		rqp->txq.pidx_inc = 0;
 
-		ret = cxgb4_sync_txq_pidx(rqp->dev->rdev.lldi.ports[0],
+		ret = cxgb4_sync_txq_pidx(rqp->rhp->rdev.lldi.ports[0],
 					  rqp->fl.cntxt_id,
 					  t4_fl_host_wq_pidx(&rqp->fl),
 					  t4_fl_wq_size(&rqp->fl));
@@ -1714,10 +1737,10 @@ static void recover_lost_dbs(struct uld_ctx *ctx, struct qp_list *qp_list)
 			return;
 		}
 		rqp->fl.pidx_inc = 0;
-		spin_unlock_irq(&rqp->dev->lock);
+		spin_unlock_irq(&rqp->rhp->lock);
 
 		/* Wait for the dbfifo to drain */
-		while (cxgb4_dbfifo_count(rqp->dev->rdev.lldi.ports[0], 1)
+		while (cxgb4_dbfifo_count(rqp->rhp->rdev.lldi.ports[0], 1)
 		       > 0) {
 			set_current_state(TASK_UNINTERRUPTIBLE);
 			schedule_timeout(usecs_to_jiffies(10));
diff --git a/dev/T4/linux/iw_cxgb4/ev.c b/dev/T4/linux/iw_cxgb4/ev.c
index 81dda7a79..b8aa3d715 100644
--- a/dev/T4/linux/iw_cxgb4/ev.c
+++ b/dev/T4/linux/iw_cxgb4/ev.c
@@ -85,20 +85,20 @@ static void dump_err_cqe(struct c4iw_dev *dev, struct t4_cqe *err_cqe)
 
 }
 
 static void post_qp_event(struct chrd_dev *dev, struct chrd_cq *chp,
 			  struct chrd_qp *qhp,
 			  struct t4_cqe *err_cqe,
 			  enum ib_event_type ib_event)
 {
 	struct ib_event event;
 	struct chrd_common_qp_attributes attrs;
 	unsigned long flag;
 
 	dump_err_cqe(dev, err_cqe);
 
 	if (qhp->attr.state == CHRD_QP_STATE_RTS) {
 		attrs.next_state = CHRD_QP_STATE_TERMINATE;
-		chrd_modify_iw_rc_qp(qhp, CHRD_QP_ATTR_NEXT_STATE, &attrs, 0);
+	//	chrd_modify_iw_rc_qp(qhp, CHRD_QP_ATTR_NEXT_STATE, &attrs, 0);		/*Todo: Check the Commented Code */
 	}
 
 	event.event = ib_event;
@@ -298,8 +298,8 @@ int c4iw_ev_handler(struct c4iw_dev *dev, u32 qid, u32 pidx)
 		break;
 	}
 	default:
-		pr_debug("unknown iqid %d queue type %d\n",
-		       qid, (int)type);
+		pr_err("%s unknown iqid %d queue type %d\n",
+		       __func__, qid, (int)type);
 		
 	}
 	return 0;
diff --git a/dev/T4/linux/iw_cxgb4/iw_cxgb4.h b/dev/T4/linux/iw_cxgb4/iw_cxgb4.h
index 094ddd247..0d7fdf567 100644
--- a/dev/T4/linux/iw_cxgb4/iw_cxgb4.h
+++ b/dev/T4/linux/iw_cxgb4/iw_cxgb4.h
@@ -47,12 +47,16 @@
 #include <linux/mutex.h>
 #include <linux/time.h>
 #include <linux/workqueue.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <linux/udp.h>
 
 #include <asm/byteorder.h>
 
 #include <net/net_namespace.h>
 
 #include <rdma/ib_verbs.h>
+#include <rdma/ib_pack.h>
 #include <rdma/iw_cm.h>
 #include <rdma/restrack.h>
 
@@ -95,45 +99,46 @@ struct c4iw_id_table {
 	unsigned long *table;
 };
   
 struct chrd_resource {
 	struct chrd_id_table tpt_table;
 	struct chrd_id_table qid_table;
 	struct chrd_id_table pdid_table;
 	struct chrd_id_table srq_table;
 };
 
 struct chrd_qid_list {
 	struct list_head entry;
 	u32 qid;
 };
 
 struct chrd_dev_ucontext {
 	struct list_head qpids;
 	struct list_head cqids;
 	struct mutex lock;
 };
 
 enum chrd_rdev_flags {
 	T4_FATAL_ERROR = (1<<0),
 	T4_STATUS_PAGE_DISABLED = (1<<1),
 };
 
 struct chrd_stat {
 	u64 total;
 	u64 cur;
 	u64 max;
 	u64 fail;
 };
 
 struct chrd_stats {
 	struct mutex lock;
 	struct chrd_stat qid;
 	struct chrd_stat pd;
 	struct chrd_stat stag;
 	struct chrd_stat pbl;
 	struct chrd_stat rqt;
+	struct chrd_stat rrqt;
 	struct chrd_stat srqt;
 	struct chrd_stat ocqp;
 	u64  db_full;
 	u64  db_empty;
 	u64  db_drop;
@@ -168,11 +173,12 @@ struct wr_log_entry {
 	u8 valid;
 };
 
 struct chrd_rdev {
 	struct chrd_resource resource;
 	u32 qpmask;
 	u32 cqmask;
 	struct chrd_dev_ucontext uctx;
+	struct gen_pool *rrqt_pool;
 	struct gen_pool *pbl_pool;
 	struct gen_pool *rqt_pool;
 	struct gen_pool *ocqp_pool;
@@ -184,25 +190,32 @@ struct c4iw_rdev {
 	void __iomem *oc_mw_kva;
 	unsigned long *fids;
 	int nfids;
 	struct chrd_stats stats;
 	struct chrd_hw_queue hw_queue;
 	struct t4_dev_status_page *status_page;
 	dma_addr_t daddr;
 	atomic_t wr_log_idx;
 	struct wr_log_entry *wr_log;
 	int wr_log_size;
+	u8 gsi_qp_inuse;
+	u32 gsi_ftid;
+	struct chrd_qp *gsi_qp;
+	struct chrd_cq *gsi_scq;
+	struct chrd_cq *gsi_rcq;
 	struct list_head blocker_list;
 	struct mutex blocker_lock;
 	struct list_head ep_glist;
 	struct mutex ep_glist_lock;
 	struct workqueue_struct *free_workq;
+	struct completion rrqt_compl;
 	struct completion rqt_compl;
 	struct completion pbl_compl;
+	struct kref rrqt_kref;
 	struct kref rqt_kref;
 	struct kref pbl_kref;
 };
 
 static inline int chrd_onchip_pa(struct chrd_rdev *rdev, u64 pa)
 {
 	return pa >= rdev->oc_mw_pa &&
 	       pa < rdev->oc_mw_pa + rdev->lldi.vr->ocq.size;
@@ -379,9 +392,10 @@ enum db_state {
 	RECOVERY = 3
 };
 
 struct chrd_dev {
 	struct ib_device ibdev;
 	struct chrd_rdev rdev;
+	struct device_dma_parameters dma_parms;
 	struct xarray cqs;
 	struct xarray qps;
 	struct xarray rawqps;
@@ -403,51 +417,59 @@ struct c4iw_dev {
 struct uld_ctx {
 	struct list_head entry;
 	struct cxgb4_lld_info lldi;
 	struct chrd_dev *dev;
 	struct work_struct reg_work;
 };
 
 static inline struct chrd_dev *to_chrd_dev(struct ib_device *ibdev)
 {
 	return container_of(ibdev, struct chrd_dev, ibdev);
 }
 
 static inline struct chrd_dev *rdev_to_chrd_dev(struct chrd_rdev *rdev)
 {
 	return container_of(rdev, struct chrd_dev, rdev);
 }
 
 static inline struct chrd_cq *get_chp(struct chrd_dev *rhp, u32 cqid)
 {
 	return xa_load(&rhp->cqs, cqid);
 }
 
 static inline struct chrd_qp *get_qhp(struct chrd_dev *rhp, u32 qpid)
 {
 	return xa_load(&rhp->qps, qpid);
 }
 
 static inline struct chrd_cq *fidx2cq(struct chrd_dev *rhp, u32 fidx)
 {
 	return xa_load(&rhp->fids, fidx);
 }
 
 extern uint chrd_max_read_depth;
 
 static inline int cur_max_read_depth(struct chrd_dev *dev)
 {
 	return min(dev->rdev.lldi.max_ordird_qp, chrd_max_read_depth);
 }
 
+struct dst_entry *find_route6(struct chrd_dev *dev, __u8 *local_ip,
+			      __u8 *peer_ip, __be16 local_port,
+			      __be16 peer_port, u8 tos,
+			      __u32 sin6_scope_id);
+struct dst_entry *find_route(struct chrd_dev *dev, __be32 local_ip,
+			     __be32 peer_ip, __be16 local_port,
+			     __be16 peer_port, u8 tos);
+
 struct chrd_pd {
 	struct ib_pd ibpd;
 	u32 pdid;
 	struct chrd_dev *rhp;
 };
 
 static inline struct chrd_pd *to_chrd_pd(struct ib_pd *ibpd)
 {
 	return container_of(ibpd, struct chrd_pd, ibpd);
 }
 
 struct tpt_attributes {
@@ -508,29 +530,30 @@ struct c4iw_mw {
 #endif
 };
 
 static inline struct chrd_mw *to_chrd_mw(struct ib_mw *ibmw)
 {
 	return container_of(ibmw, struct chrd_mw, ibmw);
 }
 
 struct chrd_cq {
 	struct ib_cq ibcq;
 	struct chrd_dev *rhp;
 	struct sk_buff *destroy_skb;
 	struct t4_cq cq;
+	u8 gsi_cq;
 	spinlock_t lock;
 	spinlock_t comp_handler_lock;
 	atomic_t refcnt;
 	wait_queue_head_t wait;
 	struct chrd_wr_wait *wr_waitp;
 };
 
 static inline struct chrd_cq *to_chrd_cq(struct ib_cq *ibcq)
 {
 	return container_of(ibcq, struct chrd_cq, ibcq);
 }
 
 struct chrd_mpa_attributes {
 	u8 initiator;
 	u8 recv_marker_enabled;
 	u8 xmit_marker_enabled;
@@ -540,7 +563,7 @@ struct c4iw_mpa_attributes {
 	u8 p2p_type;
 };
 
 struct chrd_common_qp_attributes {
 	u32 scq;
 	u32 rcq;
 	u32 sq_num_entries;
@@ -569,6 +592,150 @@ struct c4iw_qp_attributes {
 	u8 send_term;
 };
 
+#define CHRD_ROCE_PSN_MASK 0xFFFFFF
+#define CHRD_ROCE_PORT 4791
+union chrd_roce_sockaddr {
+	struct sockaddr_in saddr_in;
+	struct sockaddr_in6 saddr_in6;
+};
+
+struct chrd_bth_hdr {
+	u8                      opcode;
+	u8                      flags;
+	__be16                  pkey;
+	__be32                  destination_qpn;
+	__be32                  apsn;
+};
+
+struct chrd_rc_hdr {
+	union eth_hdr {
+//		struct vlan_ethhdr vlan_ethh;		/*Todo: Check the Commented Code */
+		struct ethhdr ethh;
+	} eth;
+	union ip_hdr {
+		struct iphdr ip4h;
+//		struct ipv6hdr ip6h;			/*Todo: Check the Commented Code */
+	} iph;
+	struct udphdr udph;
+	struct chrd_bth_hdr  bth;
+} __attribute__ ((__packed__));
+
+struct chrd_gsi_hdr {
+	struct chrd_rc_hdr roce_hdr;
+	struct ib_unpacked_deth deth;
+} __attribute__ ((__packed__));
+
+struct chrd_ah {
+	struct ib_ah ibah;
+	struct rdma_ah_attr attr;
+	struct sk_buff *ah_skb;
+	struct chrd_dev *rhp;
+	struct chrd_pd *php;
+	struct chrd_wr_wait *wr_waitp;
+
+	/* AV */
+	union chrd_roce_sockaddr sgid_addr;
+	union chrd_roce_sockaddr dgid_addr;
+	union ib_gid dgid;
+	bool ipv4:1;
+	bool insert_vlan_tag:1;
+	u8 smac[ETH_ALEN];
+	u8 dmac[ETH_ALEN];
+	u16 src_port;
+	u16 dst_port;
+	u32 local_ip_addr[4];
+	u32 dest_ip_addr[4];
+	u32 flowlabel;
+	u16 p_key;
+	u32 dest_qp;
+	u8 gid_index;
+	u8 stat_rate;
+	u8 hop_limit;
+	u8 net_type;
+	u16 vlan_id;
+	u8 vlan_en;
+	u8 tclass;
+	u8 port;
+	u8 sl;
+
+	/* HW queues */
+	u16 ctrlq_idx;
+	u16 rss_qid;
+	u16 txq_idx;
+
+	/* add id for each ah */
+	int ah_id;
+
+	/* For route resolution */
+	struct l2t_entry *l2t;
+	struct dst_entry *dst;
+#if 0
+// Bhar: enable these when needed
+	struct chrd_qp *qp;
+	struct sk_buff_head ah_skb_list;
+	enum chrd_ep_state state;
+	struct kref kref;
+	struct mutex mutex;
+	struct chrd_wr_wait *wr_waitp;
+	unsigned long flags;
+	unsigned long history;
+	struct list_head glist_entry;
+
+	struct list_head entry;
+	u32 snd_seq;
+	u32 rcv_seq;
+	struct l2t_entry *l2t;
+	struct dst_entry *dst;
+	u32 ird;
+	u32 ord;
+	u32 smac_idx;
+	u32 tx_chan;
+	u32 mtu;
+	u16 mss;
+	u16 emss;
+	u16 plen;
+	unsigned int retry_count;
+	int snd_win;
+	int rcv_win;
+	u32 snd_wscale;
+	u32 srqe_idx;
+	u32 rx_pdu_out_cnt;
+#endif
+};
+
+struct chrd_gsi_attr {
+	u8 ttl;
+	u8 tos;
+	u32 snd_mss;
+	u16 vlan_tag;
+	u16 arp_idx;
+	u32 flow_label;
+	u8 udp_state;
+	u32 psn_nxt;
+	u32 lsn;
+	u32 epsn;
+	u32 psn_max;
+	u32 psn_una;
+	u32 cwnd;
+	u8 rexmit_thresh;
+	u8 rnr_nak_thresh;
+};
+
+struct chrd_roce_qp_attributes {
+	u32 q_key;
+	u16 err_rq_idx;
+	u8 roce_tver;
+	u8 ack_credits;
+	u8 err_rq_idx_valid;
+	u32 pd_id;
+	u16 ord_size;
+	u16 ird_size;
+	u32 hwtid;
+	u32 atid;
+	struct chrd_ah roce_ah;
+	struct chrd_gsi_attr gsi_attr;
+};
+
 enum obj_type {
 	UNKNOWN,
 	RC_QP,
@@ -582,41 +749,59 @@ struct db_fcl {
 	enum obj_type type;
 };
 
+enum qp_transport_type {
+	CHRD_TRANSPORT_IWARP,
+	CHRD_TRANSPORT_ROCEV2,
+};
+
 struct chrd_qp {
 	struct ib_qp ibqp;
 	struct db_fcl fcl;
 	struct chrd_dev *rhp;
+	struct net_device *netdev;
 	struct chrd_ep *ep;
 	struct chrd_common_qp_attributes attr;
 	struct t4_wq wq;
 	spinlock_t lock;
 	struct mutex mutex;
-	wait_queue_head_t wait;
+	enum ib_qp_type qp_type;
+	enum qp_transport_type qp_trans;
 	int sq_sig_all;
 	struct chrd_srq *srq;
+	struct ch_filter gsi_filt;
+	struct chrd_roce_qp_attributes roce_attr;
 	struct chrd_ucontext *ucontext;
+	wait_queue_head_t wait;
 	struct chrd_wr_wait *wr_waitp;
 	struct completion qp_rel_comp;
 	refcount_t qp_refcnt;
 };
 
+static inline void chrd_copy_ip_ntohl(u32 *dst, __be32 *src)
+{
+	*dst++ = ntohl(*src++);
+	*dst++ = ntohl(*src++);
+	*dst++ = ntohl(*src++);
+	*dst = ntohl(*src);
+}
+ 
 static inline struct chrd_qp *to_chrd_qp(struct ib_qp *ibqp)
 {
 	return container_of(ibqp, struct chrd_qp, ibqp);
 }
 
 static inline struct chrd_qp *fcl_to_chrd_qp(struct db_fcl *fcl)
 {
 	return container_of(fcl, struct chrd_qp, fcl);
 }
 
 struct chrd_raw_qp {
 	struct ib_qp ibqp;
 	struct db_fcl fcl;
-	struct chrd_dev *dev;
+	struct chrd_dev *rhp;
 	struct net_device *netdev;
 	struct chrd_cq *scq;
 	struct chrd_cq *rcq;
 	struct t4_iq iq;
 	struct t4_fl fl;
 	struct t4_eth_txq txq;
@@ -723,88 +908,206 @@ static inline struct c4iw_mm_entry *remove_mmap(struct c4iw_ucontext *ucontext,
 	return NULL;
 }
 
 static inline void insert_mmap(struct chrd_ucontext *ucontext,
 			       struct chrd_mm_entry *mm)
 {
 	spin_lock(&ucontext->mmap_lock);
 	pr_debug("key 0x%x addr 0x%llx len %d\n",
 	     mm->key, (unsigned long long) mm->addr, mm->len);
 	list_add_tail(&mm->entry, &ucontext->mmaps);
 	spin_unlock(&ucontext->mmap_lock);
 }
 
 enum chrd_qp_attr_mask {
 	CHRD_QP_ATTR_NEXT_STATE = 1 << 0,
 	CHRD_QP_ATTR_SQ_DB = 1<<1,
 	CHRD_QP_ATTR_RQ_DB = 1<<2,
 	CHRD_QP_ATTR_ENABLE_RDMA_READ = 1 << 7,
 	CHRD_QP_ATTR_ENABLE_RDMA_WRITE = 1 << 8,
 	CHRD_QP_ATTR_ENABLE_RDMA_BIND = 1 << 9,
 	CHRD_QP_ATTR_MAX_ORD = 1 << 11,
 	CHRD_QP_ATTR_MAX_IRD = 1 << 12,
 	CHRD_QP_ATTR_LLP_STREAM_HANDLE = 1 << 22,
 	CHRD_QP_ATTR_STREAM_MSG_BUFFER = 1 << 23,
 	CHRD_QP_ATTR_MPA_ATTR = 1 << 24,
 	CHRD_QP_ATTR_QP_CONTEXT_ACTIVATE = 1 << 25,
 	CHRD_QP_ATTR_VALID_MODIFY = (CHRD_QP_ATTR_ENABLE_RDMA_READ |
 				     CHRD_QP_ATTR_ENABLE_RDMA_WRITE |
 				     CHRD_QP_ATTR_MAX_ORD |
 				     CHRD_QP_ATTR_MAX_IRD |
 				     CHRD_QP_ATTR_LLP_STREAM_HANDLE |
 				     CHRD_QP_ATTR_STREAM_MSG_BUFFER |
 				     CHRD_QP_ATTR_MPA_ATTR |
 				     CHRD_QP_ATTR_QP_CONTEXT_ACTIVATE)
 };
 
 int chrd_modify_iw_rc_qp(struct chrd_qp *qhp, enum chrd_qp_attr_mask mask,
 		      struct chrd_common_qp_attributes *attrs, int internal);
+
+enum chrd_qp_state {
+	CHRD_QP_STATE_IDLE,
+	CHRD_QP_STATE_RTR,
+	CHRD_QP_STATE_RTS,
+	CHRD_QP_STATE_ERROR,
+	CHRD_QP_STATE_TERMINATE,
+	CHRD_QP_STATE_CLOSING,
+	CHRD_QP_STATE_TOT
+};
+
+static inline int chrd_convert_state(enum ib_qp_state ib_state)
+{
+	switch (ib_state) {
+	case IB_QPS_RESET:
+	case IB_QPS_INIT:
+		return CHRD_QP_STATE_IDLE;
+	case IB_QPS_RTR:
+		return CHRD_QP_STATE_RTR;
+	case IB_QPS_RTS:
+		return CHRD_QP_STATE_RTS;
+	case IB_QPS_SQD:
+		return CHRD_QP_STATE_CLOSING;
+	case IB_QPS_SQE:
+		return CHRD_QP_STATE_TERMINATE;
+	case IB_QPS_ERR:
+		return CHRD_QP_STATE_ERROR;
+	default:
+		return -1;
+	}
+}
+ 
+static inline int to_ib_qp_state(int chrd_qp_state)
+{
+	switch (chrd_qp_state) {
+	case CHRD_QP_STATE_IDLE:
+		return IB_QPS_INIT;
+	case CHRD_QP_STATE_RTR:
+		return IB_QPS_RTR;
+	case CHRD_QP_STATE_RTS:
+		return IB_QPS_RTS;
+	case CHRD_QP_STATE_CLOSING:
+		return IB_QPS_SQD;
+	case CHRD_QP_STATE_TERMINATE:
+		return IB_QPS_SQE;
+	case CHRD_QP_STATE_ERROR:
+		return IB_QPS_ERR;
+	}
+	return IB_QPS_ERR;
+}
 
-enum chrd_qp_state {
-	CHRD_QP_STATE_IDLE,
-	CHRD_QP_STATE_RTS,
-	CHRD_QP_STATE_ERROR,
-	CHRD_QP_STATE_TERMINATE,
-	CHRD_QP_STATE_CLOSING,
-	CHRD_QP_STATE_TOT
+enum chrd_v2_qp_state {
+	CHRD_QP_V2_STATE_RESET,
+	CHRD_QP_V2_STATE_IDLE,
+	CHRD_QP_V2_STATE_RTR,
+	CHRD_QP_V2_STATE_RTS,
+	CHRD_QP_V2_STATE_ERROR,
+	CHRD_QP_V2_STATE_TERMINATE,
+	CHRD_QP_V2_STATE_CLOSING,
+	CHRD_QP_V2_STATE_TOT
 };
 
-static inline int chrd_convert_state(enum ib_qp_state ib_state)
+static inline int chrd_convert_v2_state(enum ib_qp_state ib_state)
 {
 	switch (ib_state) {
 	case IB_QPS_RESET:
+		return CHRD_QP_V2_STATE_RESET;
 	case IB_QPS_INIT:
-		return CHRD_QP_STATE_IDLE;
+		return CHRD_QP_V2_STATE_IDLE;
+	case IB_QPS_RTR:
+		return CHRD_QP_V2_STATE_RTR;
 	case IB_QPS_RTS:
-		return CHRD_QP_STATE_RTS;
+		return CHRD_QP_V2_STATE_RTS;
 	case IB_QPS_SQD:
-		return CHRD_QP_STATE_CLOSING;
+		return CHRD_QP_V2_STATE_CLOSING;
 	case IB_QPS_SQE:
-		return CHRD_QP_STATE_TERMINATE;
+		return CHRD_QP_V2_STATE_TERMINATE;
 	case IB_QPS_ERR:
-		return CHRD_QP_STATE_ERROR;
+		return CHRD_QP_V2_STATE_ERROR;
 	default:
 		return -1;
 	}
 }
 
-static inline int to_ib_qp_state(int chrd_qp_state)
+static inline int v2_to_ib_qp_state(int chrd_v2_qp_state)
 {
-	switch (chrd_qp_state) {
-	case CHRD_QP_STATE_IDLE:
+	switch (chrd_v2_qp_state) {
+	case CHRD_QP_V2_STATE_RESET:
+		return IB_QPS_RESET;
+	case CHRD_QP_V2_STATE_IDLE:
 		return IB_QPS_INIT;
-	case CHRD_QP_STATE_RTS:
+	case CHRD_QP_V2_STATE_RTR:
+		return IB_QPS_RTR;
+	case CHRD_QP_V2_STATE_RTS:
 		return IB_QPS_RTS;
-	case CHRD_QP_STATE_CLOSING:
+	case CHRD_QP_V2_STATE_CLOSING:
 		return IB_QPS_SQD;
-	case CHRD_QP_STATE_TERMINATE:
+	case CHRD_QP_V2_STATE_TERMINATE:
 		return IB_QPS_SQE;
-	case CHRD_QP_STATE_ERROR:
+	case CHRD_QP_V2_STATE_ERROR:
 		return IB_QPS_ERR;
 	}
 	return IB_QPS_ERR;
 }
 
+enum chrd_v2_ing_cqe_opcode {
+	IB_CQE_V2_OPC_SEND_FIRST,
+	IB_CQE_V2_OPC_SEND_MIDDLE,
+	IB_CQE_V2_OPC_SEND_LAST,
+	IB_CQE_V2_OPC_SEND_LAST_WITH_IMM,
+	IB_CQE_V2_OPC_SEND_ONLY,
+	IB_CQE_V2_OPC_SEND_ONLY_WITH_IMM,
+	IB_CQE_V2_OPC_WRITE_FIRST,
+	IB_CQE_V2_OPC_WRITE_MIDDLE,
+	IB_CQE_V2_OPC_WRITE_LAST,
+	IB_CQE_V2_OPC_WRITE_LAST_WITH_IMM,
+	IB_CQE_V2_OPC_WRITE_ONLY,
+	IB_CQE_V2_OPC_WRITE_ONLY_WITH_IMM,
+	IB_CQE_V2_OPC_READ_REQUEST,
+	IB_CQE_V2_OPC_READ_RESPONSE_FIRST,
+	IB_CQE_V2_OPC_READ_RESPONSE_MIDDLE,
+	IB_CQE_V2_OPC_READ_RESPONSE_LAST,
+	IB_CQE_V2_OPC_READ_RESPONSE_ONLY,
+	IB_CQE_V2_OPC_ACK,
+	IB_CQE_V2_OPC_SEND_LAST_WITH_INV = 0x16,
+	IB_CQE_V2_OPC_SEND_ONLY_WITH_INV = 0x17,
+};
+
+static inline int v2_ib_opc_to_fw_opc(enum chrd_v2_ing_cqe_opcode opcode)
+{
+	switch (opcode) {
+	case IB_CQE_V2_OPC_SEND_FIRST:
+	case IB_CQE_V2_OPC_SEND_MIDDLE:
+	case IB_CQE_V2_OPC_SEND_LAST:
+	case IB_CQE_V2_OPC_SEND_ONLY:
+		return FW_RI_SEND;
+	case IB_CQE_V2_OPC_SEND_LAST_WITH_INV:
+	case IB_CQE_V2_OPC_SEND_ONLY_WITH_INV:
+		return FW_RI_SEND_WITH_INV;
+#if 0 //Bhar: enable send_imm when fw enables it
+	case IB_CQE_V2_OPC_SEND_LAST_WITH_IMM:
+	case IB_CQE_V2_OPC_SEND_ONLY_WITH_IMM:
+		return FW_RI_SEND_IMMEDIATE;
+#endif
+	case IB_CQE_V2_OPC_WRITE_FIRST:
+	case IB_CQE_V2_OPC_WRITE_MIDDLE:
+	case IB_CQE_V2_OPC_WRITE_LAST:
+	case IB_CQE_V2_OPC_WRITE_ONLY:
+		return FW_RI_RDMA_WRITE;
+	case IB_CQE_V2_OPC_WRITE_LAST_WITH_IMM:
+	case IB_CQE_V2_OPC_WRITE_ONLY_WITH_IMM:
+		return FW_RI_WRITE_IMMEDIATE;
+	case IB_CQE_V2_OPC_READ_REQUEST:
+		return FW_RI_READ_REQ;
+	case IB_CQE_V2_OPC_READ_RESPONSE_FIRST:
+	case IB_CQE_V2_OPC_READ_RESPONSE_MIDDLE:
+	case IB_CQE_V2_OPC_READ_RESPONSE_LAST:
+	case IB_CQE_V2_OPC_READ_RESPONSE_ONLY:
+		return FW_RI_READ_RESP;
+	default:
+		return 0x1F; //Bhar: setting opc to reserved code to deal with it in poll_cq_one()
+	}
+}
+
 static inline u32 chrd_ib_to_tpt_access(int a)
 {
 	return (a & IB_ACCESS_REMOTE_WRITE ? FW_RI_MEM_ACCESS_REM_WRITE : 0) |
 	       (a & IB_ACCESS_REMOTE_READ ? FW_RI_MEM_ACCESS_REM_READ : 0) |
@@ -842,33 +1145,37 @@ enum c4iw_mmid_state {
 #define MPA_V2_IRD_ORD_MASK             0x3FFF
 
 #ifdef HAVE_KREF_READ
 #define chrd_put_ep(ep) { \
 	pr_debug("put_ep ep %p refcnt %d\n", \
 	     ep, kref_read(&((ep)->kref))); \
 	WARN_ON(kref_read(&((ep)->kref)) < 1); \
 	kref_put(&((ep)->kref), _chrd_free_ep); \
 }
 
 #define chrd_get_ep(ep) { \
 	pr_debug("get_ep ep %p, refcnt %d\n", \
 	     ep, kref_read(&((ep)->kref))); \
 	kref_get(&((ep)->kref));  \
 }
 #else
 #define chrd_put_ep(ep) { \
 	pr_debug("put_ep ep %p refcnt %d\n", \
 	     ep, atomic_read(&((ep)->kref.refcount))); \
 	WARN_ON(atomic_read(&((ep)->kref.refcount)) < 1); \
 	kref_put(&((ep)->kref), _chrd_free_ep); \
 }
 
 #define chrd_get_ep(ep) { \
 	pr_debug("get_ep ep %p, refcnt %d\n", \
 	     ep, atomic_read(&((ep)->kref.refcount))); \
 	kref_get(&((ep)->kref));  \
 }
 #endif
 void _chrd_free_ep(struct kref *kref);
+struct sk_buff *get_skb(struct sk_buff *skb, int len, gfp_t gfp);
+int chrd_l2t_send(struct chrd_rdev *rdev, struct sk_buff *skb,
+		  struct l2t_entry *l2t);
+
 
 struct mpa_message {
 	u8 key[16];
@@ -1081,22 +1388,27 @@ struct c4iw_ep {
 	int rcv_win;
 	u16 ipsecidx;
 	u32 snd_wscale;
 	struct chrd_ep_stats stats;
 	u32 srqe_idx;
 	u32 rx_pdu_out_cnt;
 	struct sk_buff *peer_abort_skb;
 };
 
 static inline struct chrd_ep *to_ep(struct iw_cm_id *cm_id)
 {
 	return cm_id->provider_data;
 }
 
 static inline struct chrd_listen_ep *to_listen_ep(struct iw_cm_id *cm_id)
 {
 	return cm_id->provider_data;
 }
 
+static inline struct chrd_ah *to_chrd_ah(struct ib_ah *ibah)
+{
+	return container_of(ibah, struct chrd_ah, ibah);
+}
+
 static inline int compute_wscale(int win)
 {
 	int wscale = 0;
@@ -1115,70 +1427,74 @@ static inline int ocqp_supported(const struct cxgb4_lld_info *infop)
 #endif
 }
 
 u32 chrd_id_alloc(struct chrd_id_table *alloc);
 void chrd_id_free(struct chrd_id_table *alloc, u32 obj);
 int chrd_id_table_alloc(struct chrd_id_table *alloc, u32 start, u32 num,
 			u32 reserved, u32 flags);
 void chrd_id_table_free(struct chrd_id_table *alloc);
 
 typedef int (*chrd_handler_func)(struct chrd_dev *dev, struct sk_buff *skb);
 
 int chrd_ep_redirect(void *ctx, struct dst_entry *old, struct dst_entry *new,
 		     struct l2t_entry *l2t);
 void chrd_put_qpid(struct chrd_rdev *rdev, u32 qpid,
 		   struct chrd_dev_ucontext *uctx);
 u32 chrd_get_resource(struct chrd_id_table *id_table);
 void chrd_put_resource(struct chrd_id_table *id_table, u32 entry);
 int chrd_init_resource(struct chrd_rdev *rdev, u32 nr_tpt, u32 nr_pdid, u32 nr_srqt);
 int chrd_init_ctrl_qp(struct chrd_rdev *rdev);
 int chrd_pblpool_create(struct chrd_rdev *rdev);
 int chrd_rqtpool_create(struct chrd_rdev *rdev);
+int chrd_rrqtpool_create(struct chrd_rdev *rdev);
 void chrd_pblpool_destroy(struct chrd_rdev *rdev);
 void chrd_rqtpool_destroy(struct chrd_rdev *rdev);
+void chrd_rrqtpool_destroy(struct chrd_rdev *rdev);
 void chrd_destroy_resource(struct chrd_resource *rscp);
 int chrd_destroy_ctrl_qp(struct chrd_rdev *rdev);
 void chrd_register_device(struct work_struct *work);
 void chrd_unregister_device(struct chrd_dev *dev);
 int __init chrd_cm_init(void);
 void chrd_cm_term(void);
 void chrd_release_dev_ucontext(struct chrd_rdev *rdev,
 			       struct chrd_dev_ucontext *uctx);
 void chrd_init_dev_ucontext(struct chrd_rdev *rdev,
 			    struct chrd_dev_ucontext *uctx);
 int chrd_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc);
 int chrd_iw_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 		      const struct ib_send_wr **bad_wr);
+int chrd_roce_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
+			const struct ib_send_wr **bad_wr);
 int chrd_post_receive(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
 		      const struct ib_recv_wr **bad_wr);
 int chrd_iw_connect(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param);
 int chrd_iw_create_listen(struct iw_cm_id *cm_id, int backlog);
 int chrd_iw_destroy_listen(struct iw_cm_id *cm_id);
 int chrd_iw_accept_cr(struct iw_cm_id *cm_id, struct iw_cm_conn_param *conn_param);
 int chrd_iw_reject_cr(struct iw_cm_id *cm_id, const void *pdata, u8 pdata_len);
 void chrd_iw_qp_add_ref(struct ib_qp *qp);
 void chrd_iw_qp_rem_ref(struct ib_qp *qp);
 struct ib_mr *chrd_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,
 			    u32 max_num_sg);
 int chrd_map_mr_sg(struct ib_mr *ibmr,
 		   struct scatterlist *sg,
 #ifdef IWARP_HAVE_SG_OFFSET
 		   int sg_nents, unsigned int *sg_offset);
 #else
 		   int sg_nents);
 #endif
 int chrd_dealloc_mw(struct ib_mw *mw);
 void chrd_dealloc(struct uld_ctx *ctx);
 void chrd_dispatch_event(struct ib_device* ibdev,
 			  u8 port_num,
 			  enum ib_event_type type);
 int chrd_alloc_mw(struct ib_mw *ibmw, struct ib_udata *udata);
 struct ib_mr *chrd_reg_user_mr(struct ib_pd *pd, u64 start,
 					   u64 length, u64 virt, int acc,
 					   struct ib_udata *udata);
 struct ib_mr *chrd_get_dma_mr(struct ib_pd *pd, int acc);
 int chrd_dereg_mr(struct ib_mr *ib_mr, struct ib_udata *udata);
 int chrd_destroy_cq(struct ib_cq *ib_cq, struct ib_udata *udata);
 int chrd_create_cq(struct ib_cq *ibcq,
 #ifdef IWARP_HAVE_CQ_INIT_ATTR
 			     const struct ib_cq_init_attr *attr,
 			     struct ib_udata *udata);
@@ -1187,76 +1503,81 @@ int c4iw_create_cq(struct ib_cq *ibcq,
 			     struct ib_ucontext *ib_context,
 			     struct ib_udata *udata);
 #endif
 int chrd_resize_cq(struct ib_cq *cq, int cqe, struct ib_udata *udata);
 int chrd_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags flags);
 int chrd_modify_srq(struct ib_srq *ib_srq, struct ib_srq_attr *attr,
 		    enum ib_srq_attr_mask srq_attr_mask,
 		    struct ib_udata *udata);
 int chrd_destroy_srq(struct ib_srq *ib_srq, struct ib_udata *udata);
 int chrd_create_srq(struct ib_srq *srq,
 		    struct ib_srq_init_attr *attrs,
 		    struct ib_udata *udata);
 int chrd_destroy_qp(struct ib_qp *ib_qp, struct ib_udata *udata);
 int chrd_create_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 		   struct ib_udata *udata);
 int chrd_iw_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 		      int attr_mask, struct ib_udata *udata);
-int chrd_ib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
-		     int attr_mask, struct ib_qp_init_attr *init_attr);
+int chrd_roce_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
+			int attr_mask, struct ib_udata *udata);
+int chrd_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
+		  int attr_mask, struct ib_qp_init_attr *init_attr);
 struct ib_qp *chrd_iw_get_qp(struct ib_device *dev, int qpn);
+u32 chrd_rrqtpool_alloc(struct chrd_rdev *rdev, int size);
+void chrd_rrqtpool_free(struct chrd_rdev *rdev, u32 addr, int size);
 u32 chrd_rqtpool_alloc(struct chrd_rdev *rdev, int size);
 void chrd_rqtpool_free(struct chrd_rdev *rdev, u32 addr, int size);
 u32 chrd_pblpool_alloc(struct chrd_rdev *rdev, int size);
 void chrd_pblpool_free(struct chrd_rdev *rdev, u32 addr, int size);
 extern u32 cxgb4_ocqp_pool_alloc(struct net_device *dev, int size);
 extern void cxgb4_ocqp_pool_free(struct net_device *dev, u32 addr, int size);
 void chrd_flush_hw_cq(struct chrd_cq *chp, struct chrd_qp *flush_qhp);
 void chrd_count_rcqes(struct t4_cq *cq, struct t4_wq *wq, int *count);
 int chrd_ep_disconnect(struct chrd_ep *ep, int abrupt, gfp_t gfp);
 int chrd_flush_rq(struct t4_wq *wq, struct t4_cq *cq, int count);
 int chrd_flush_sq(struct chrd_qp *qhp);
 int chrd_ev_handler(struct chrd_dev *rnicp, u32 qid, u32 pidx);
 u16 chrd_rqes_posted(struct chrd_qp *qhp);
 int chrd_post_terminate(struct chrd_qp *qhp, struct t4_cqe *err_cqe);
 u32 chrd_get_cqid(struct chrd_rdev *rdev, struct chrd_dev_ucontext *uctx);
 void chrd_put_cqid(struct chrd_rdev *rdev, u32 qid,
 		struct chrd_dev_ucontext *uctx);
 u32 chrd_iw_get_qpid(struct chrd_rdev *rdev, struct chrd_dev_ucontext *uctx);
 void chrd_put_qpid(struct chrd_rdev *rdev, u32 qid,
 		struct chrd_dev_ucontext *uctx);
 void chrd_ev_dispatch(struct chrd_dev *dev, struct t4_cqe *err_cqe);
 
 extern chrd_handler_func chrd_handlers[NUM_CPL_CMDS];
 struct ib_qp *chrd_create_raw_qp(struct ib_pd *pd,
 				 struct ib_qp_init_attr *attrs,
 				 struct ib_udata *udata);
 void __iomem *chrd_bar2_addrs(struct chrd_rdev *rdev, unsigned int qid,
 			      enum cxgb4_bar2_qtype qtype,
 			      unsigned int *pbar2_qid, u64 *pbar2_pa);
 int chrd_alloc_srq_idx(struct chrd_rdev *rdev);
 void chrd_free_srq_idx(struct chrd_rdev *rdev, int idx);
 extern void chrd_log_wr_stats(struct t4_wq *wq, struct t4_cqe *cqe);
 extern int chrd_wr_log;
 
 extern int use_dsgl;
 extern int wd_disable_inaddr_any;
+extern int roce_mode;
 void chrd_dispatch_srq_limit_reached_event(struct chrd_srq *srq);
 
 #ifndef IB_QPT_RAW_ETH
 #define IB_QPT_RAW_ETH 8
 #endif
 void chrd_copy_wr_to_srq(struct t4_srq *srq, union t4_recv_wr *wqe, u8 len16);
 void chrd_flush_srqidx(struct chrd_qp *qhp, u32 srqidx);
 int chrd_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
 		       const struct ib_recv_wr **bad_wr);
 void chrd_invalidate_mr(struct chrd_dev *rhp, u32 rkey);
 struct chrd_wr_wait *chrd_alloc_wr_wait(gfp_t gfp);
 
 typedef int chrd_restrack_func(struct sk_buff *msg,
 			       struct rdma_restrack_entry *res);
 int chrd_fill_res_mr_entry(struct sk_buff *msg, struct ib_mr *ibmr);
 int chrd_fill_res_cq_entry(struct sk_buff *msg, struct ib_cq *ibcq);
 int chrd_fill_res_qp_entry(struct sk_buff *msg, struct ib_qp *ibqp);
 int chrd_fill_res_cm_id_entry(struct sk_buff *msg, struct rdma_cm_id *cm_id);
 
 #endif
diff --git a/dev/T4/linux/iw_cxgb4/mem.c b/dev/T4/linux/iw_cxgb4/mem.c
index 5eb0ef93c..b09158dd0 100644
--- a/dev/T4/linux/iw_cxgb4/mem.c
+++ b/dev/T4/linux/iw_cxgb4/mem.c
@@ -1080,28 +1080,28 @@ struct ib_mr *c4iw_alloc_mr(struct ib_pd *pd,
 		goto err_dereg;
 	}
 
-	pr_debug("mmid 0x%x mhp %p stag 0x%x\n", mmid, mhp, stag);
+	pr_debug("mmid 0x%x mhp %p stag 0x%x max_num_sg %u length %d\n", mmid, mhp, stag, max_num_sg, length);
 	return &(mhp->ibmr);
 err_dereg:
 	dereg_mem(&rhp->rdev, stag, mhp->attr.pbl_size,
 		       mhp->attr.pbl_addr, mhp->dereg_skb, mhp->wr_waitp);
 err_free_pbl:
 	chrd_pblpool_free(&mhp->rhp->rdev, mhp->attr.pbl_addr,
 			      mhp->attr.pbl_size << 3);
 err_free_dma:
 	dma_free_coherent(mhp->rhp->rdev.lldi.dev,
 			  mhp->max_mpl_len, mhp->mpl, mhp->mpl_addr);
 err_free_wr_wait:
 	chrd_put_wr_wait(mhp->wr_waitp);
 err_free_mhp:
 	kfree(mhp);
 err:
 	return ERR_PTR(ret);
 }
 
 static int chrd_set_page(struct ib_mr *ibmr, u64 addr)
 {
 	struct chrd_mr *mhp = to_chrd_mr(ibmr);
 
 	if (unlikely(mhp->mpl_len == mhp->attr.pbl_size))
 		return -ENOMEM;
diff --git a/dev/T4/linux/iw_cxgb4/provider.c b/dev/T4/linux/iw_cxgb4/provider.c
index f229eccc7..d060f1e19 100644
--- a/dev/T4/linux/iw_cxgb4/provider.c
+++ b/dev/T4/linux/iw_cxgb4/provider.c
@@ -52,6 +52,7 @@
 #include <rdma/ib_smi.h>
 #include <rdma/ib_umem.h>
 #include <rdma/ib_user_verbs.h>
+#include <rdma/ib_cache.h>
 //#include <rdma/cxgb4-abi.h>
 
 #include "iw_cxgb4.h"
@@ -64,65 +65,175 @@ static int map_udb_as_wc = 1;
 module_param(map_udb_as_wc, int, 0644);
 MODULE_PARM_DESC(map_udb_as_wc, "Map UDB as WC on T5 (default=1)");
 
 static int chrd_iw_modify_port(struct ib_device *ibdev,
 			       u32 port, int port_modify_mask,
 			       struct ib_port_modify *props)
 {
 	return -ENOSYS;
 }
 
 static int chrd_iw_create_ah(struct ib_ah *ah,
 			     struct rdma_ah_init_attr *ah_attr,
 			     struct ib_udata *udata)
 {
 	return -ENOSYS;
 }
 
 static int chrd_iw_destroy_ah(struct ib_ah *ah, u32 flags)
 {
 	return -ENOSYS;
 }
 
+static int chrd_roce_create_ah(struct ib_ah *ah,
+			       struct rdma_ah_init_attr *ah_init_attr,
+			       struct ib_udata *udata)
+{
+	struct chrd_ah *ahp = to_chrd_ah(ah);
+	const struct ib_gid_attr *sgid_attr;
+	struct chrd_dev *rhp = to_chrd_dev(ah->device);
+	u16 vlan_id;
+	int ret = 0;
+
+	rdma_copy_ah_attr(&ahp->attr, ah_init_attr->ah_attr);
+	ahp->wr_waitp = chrd_alloc_wr_wait(GFP_KERNEL);
+	if (!ahp->wr_waitp)
+		return -ENOMEM;
+
+	ahp->dst_port = CHRD_ROCE_PORT;
+	vlan_id = VLAN_N_VID;
+	ahp->dest_qp = 1;
+	if (ah_init_attr->ah_attr->ah_flags & IB_AH_GRH) {
+	//	new_roce_attr.gsi_attr.ttl = ah_init_attr->ah_attr->grh.hop_limit;
+	//	new_roce_attr.gsi_attr.flow_label = ah_init_attr->ah_attr->grh.flow_label;	/*Todo: Check the Commented Code */
+	//	new_roce_attr.gsi_attr.tos = ah_init_attr->ah_attr->grh.traffic_class;
+		ahp->src_port = rdma_get_udp_sport(ah_init_attr->ah_attr->grh.flow_label,
+						   1, ahp->dest_qp);
+		pr_debug("GRH NA for v2, src_port = %u\n", ahp->src_port);
+	} else {
+		ahp->src_port = 0xd000;
+		pr_debug("GRH not set, src_port = %u\n", ahp->src_port);
+	}	
+	sgid_attr = ah_init_attr->ah_attr->grh.sgid_attr;
+	ahp->net_type = rdma_gid_attr_network_type(sgid_attr);
+	memcpy(ahp->dmac, ah_init_attr->ah_attr->roce.dmac, ETH_ALEN);
+	ret = rdma_read_gid_l2_fields(sgid_attr, &vlan_id, ahp->smac);
+	if (ret)
+		return ret;
+
+	if (vlan_id >= VLAN_N_VID)
+		vlan_id = 0;
+	if (vlan_id < VLAN_N_VID) {
+		ahp->insert_vlan_tag = true;
+		ahp->vlan_id = vlan_id; //Bhar: Recheck
+	} else {
+		ahp->insert_vlan_tag = false;
+	}
+	rdma_gid2ip((struct sockaddr *)&ahp->sgid_addr, &sgid_attr->gid);
+	rdma_gid2ip((struct sockaddr *)&ahp->dgid_addr, &ah_init_attr->ah_attr->grh.dgid);
+	if (ahp->net_type == RDMA_NETWORK_IPV6) {
+		__be32 *daddr =	ahp->dgid_addr.saddr_in6.sin6_addr.in6_u.u6_addr32;
+		__be32 *saddr =	ahp->sgid_addr.saddr_in6.sin6_addr.in6_u.u6_addr32;
+		
+		chrd_copy_ip_ntohl(&ahp->dest_ip_addr[0], daddr);
+		chrd_copy_ip_ntohl(&ahp->local_ip_addr[0], saddr);
+		
+		ahp->ipv4 = false;
+		
+		/* Bhar: Fields should be in BE or network order */
+		ahp->dst = find_route6(rhp, (__u8 *)saddr, (__u8 *)daddr,
+				       ahp->src_port,
+				       ahp->dst_port,
+				       0, 0); /* Bhar: fix sin6_scope_id*/
+	} else if (ahp->net_type == RDMA_NETWORK_IPV4) {
+		__be32 saddr = ahp->sgid_addr.saddr_in.sin_addr.s_addr;
+		__be32 daddr = ahp->dgid_addr.saddr_in.sin_addr.s_addr;
+		
+		ahp->ipv4 = true;
+		ahp->dest_ip_addr[0] = 0;
+		ahp->dest_ip_addr[1] = 0;
+		ahp->dest_ip_addr[2] = 0;
+		//ahp->dest_ip_addr[3] = ntohl(daddr);		/*Todo: Check the Commented Code */
+		ahp->dest_ip_addr[3] = daddr;
+		
+		ahp->local_ip_addr[0] = 0;
+		ahp->local_ip_addr[1] = 0;
+		ahp->local_ip_addr[2] = 0;
+		//ahp->local_ip_addr[3] = ntohl(saddr);		/*Todo: Check the Commented Code */
+		ahp->local_ip_addr[3] = saddr;
+		
+		/* Bhar: Fields should be in BE or network order */
+		ahp->dst = find_route(rhp, saddr, daddr, ahp->src_port,
+				      ahp->dst_port, 0);
+	}
+	pr_debug("ahp 0x%llx sport %u dport %u smac %pM dmac %pM "
+		 "dest_ip %pI4, src_ip %pI4\n", (unsigned long long)ahp,
+		 ahp->src_port, ahp->dst_port, ahp->smac, ahp->dmac,
+		 &ahp->dest_ip_addr[3], &ahp->local_ip_addr[3]);
+
+	return ret;
+}
+
+static int chrd_roce_destroy_ah(struct ib_ah *ah, u32 flags)
+{
+	struct chrd_ah *ahp = to_chrd_ah(ah);
+
+	rdma_destroy_ah_attr(&ahp->attr);
+	return 0;
+}
+
+static int chrd_roce_query_ah(struct ib_ah *ah,
+			      struct rdma_ah_attr *ah_attr)
+{
+	//Todo: add when needed later point of time
+	return 0;
+}
+
+static enum rdma_link_layer chrd_roce_link_layer(struct ib_device *ibdev,
+						 u32 port_num)
+{
+	return IB_LINK_LAYER_ETHERNET;
+}
+
 #if 0
 static int chrd_multicast_attach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
 {
 	struct chrd_raw_qp *rqp;
 	int ret;
 
 	if (ibqp->qp_type != IB_QPT_RAW_ETH)
 		return -ENOSYS;
 
 	pr_debug("- mcast %02x:%02x:%02x:%02x:%02x:%02x\n",
 	     gid->raw[0], gid->raw[1], gid->raw[2],
 	     gid->raw[3], gid->raw[4], gid->raw[5]);
 
 	rqp = to_chrd_raw_qp(ibqp);
 	rtnl_lock();
 	ret = dev_mc_add_global(rqp->netdev, gid->raw);
 	rtnl_unlock();
 	return ret;
 }
 
 static int chrd_multicast_detach(struct ib_qp *ibqp, union ib_gid *gid, u16 lid)
 {
 	struct chrd_raw_qp *rqp;
 	int ret;
 
 	if (ibqp->qp_type != IB_QPT_RAW_ETH)
 		return -ENOSYS;
 
 	pr_debug("- mcast %02x:%02x:%02x:%02x:%02x:%02x\n",
 	     gid->raw[0], gid->raw[1], gid->raw[2],
 	     gid->raw[3], gid->raw[4], gid->raw[5]);
 
 	rqp = to_chrd_raw_qp(ibqp);
 	rtnl_lock();
 	ret = dev_mc_del(rqp->netdev, gid->raw);
 	rtnl_unlock();
 	return ret;
 }
 
 static int chrd_process_mad(struct ib_device *ibdev, int mad_flags,
 #ifdef IWARP_HAVE_CQ_INIT_ATTR
 			    u8 port_num, const struct ib_wc *in_wc,
 			    const struct ib_grh *in_grh,
@@ -358,34 +469,56 @@ static int c4iw_allocate_pd(struct ib_pd *pd, struct ib_udata *udata)
 	if (rhp->rdev.stats.pd.cur > rhp->rdev.stats.pd.max)
 		rhp->rdev.stats.pd.max = rhp->rdev.stats.pd.cur;
 	mutex_unlock(&rhp->rdev.stats.lock);
 	pr_debug("pdid 0x%0x ptr 0x%p\n", pdid, php);
 	return 0;
 }
 
 static int chrd_iw_query_pkey(struct ib_device *ibdev, u32 port, u16 index,
 			   u16 *pkey)
 {
 	pr_debug("ibdev %p\n", ibdev);
-	*pkey = 0;
+	*pkey = 0;	/*Todo: hardcoded value*/
 	return 0;
 }
 
+static int chrd_roce_query_pkey(struct ib_device *ibdev, u32 port, u16 index,
+			   u16 *pkey)
+{
+	pbr_debug("port %u index %u\n", port, index);
+	if (index > 0)
+		return -EINVAL;
+	*pkey = IB_DEFAULT_PKEY_FULL;
+	return 0;
+}
+
 static int chrd_query_gid(struct ib_device *ibdev, u32 port, int index,
 			  union ib_gid *gid)
 {
 	struct chrd_dev *dev;
 
 	pr_debug("ibdev %p, port %d, index %d, gid %p\n",
 		 ibdev, port, index, gid);
 	if (!port)
 		return -EINVAL;
 	dev = to_chrd_dev(ibdev);
 	memset(&(gid->raw[0]), 0, sizeof(gid->raw));
 	memcpy(&(gid->raw[0]), dev->rdev.lldi.ports[port-1]->dev_addr, 6);
 	return 0;
 }
 
+#if 0
+static int chrd_add_gid(const struct ib_gid_attr *attr, void **context)
+{
+	return 0;
+}
+
+static int chrd_del_gid(const struct ib_gid_attr *attr, void **context)
+{
+	return 0;
+}
+#endif
+
 static int chrd_query_device(struct ib_device *ibdev,
 #ifdef IWARP_HAVE_CQ_INIT_ATTR
 			     struct ib_device_attr *props, struct ib_udata *uhw)
 #else
@@ -428,28 +561,29 @@ static int c4iw_query_device(struct ib_device *ibdev,
 #else
 	props->max_sge = T4_MAX_RECV_SGE;
 #endif
-	props->max_sge_rd = 1;
+	props->max_sge_rd = T7_MAX_RD_SGE;
 	props->max_res_rd_atom = dev->rdev.lldi.max_ird_adapter;
 	props->max_qp_rd_atom = min(dev->rdev.lldi.max_ordird_qp,
 				    chrd_max_read_depth);
 	props->max_qp_init_rd_atom = props->max_qp_rd_atom;
 	props->max_cq = dev->rdev.lldi.vr->qp.size;
 	props->max_cqe = dev->rdev.hw_queue.t4_max_cq_depth;
 	props->max_mr = chrd_num_stags(&dev->rdev);
 	props->max_pd = T4_MAX_NUM_PD;
 	props->local_ca_ack_delay = 0;
 	props->max_fast_reg_page_list_len = t4_max_fr_depth(&dev->rdev, use_dsgl);
-	props->max_ah = dev->rdev.lldi.tids->nhpftids;
+	props->max_ah = dev->rdev.lldi.tids->nhpftids; /* Todo: Need to set appropriately */
 	props->max_ee = dev->rdev.lldi.sge_ingpadboundary;
 	props->max_raw_ethy_qp = dev->rdev.lldi.neq;
+	props->max_pkeys = 1;				/*Todo: hardcoded value*/
 
 	return 0;
 }
 
 static int chrd_iw_query_port(struct ib_device *ibdev, u32 port,
 			   struct ib_port_attr *props)
 {
 	pr_debug("ibdev %p\n", ibdev);
 
 	props->port_cap_flags =
 	    IB_PORT_CM_SUP |
@@ -458,7 +592,7 @@ static int c4iw_query_port(struct ib_device *ibdev, u32 port,
 	    IB_PORT_DEVICE_MGMT_SUP |
 	    IB_PORT_VENDOR_CLASS_SUP | IB_PORT_BOOT_MGMT_SUP;
 	props->gid_tbl_len = 1;
-	props->pkey_tbl_len = 1;
+	props->pkey_tbl_len = 1;	/*Todo: hardcoded values*/
 	props->active_width = 2;
 	props->active_speed = 2;
 	props->max_msg_sz = -1;
@@ -466,38 +600,69 @@ static int c4iw_query_port(struct ib_device *ibdev, u32 port,
 	return 0;
 }
 
-static ssize_t show_rev(struct device *dev, struct device_attribute *attr,
-			char *buf)
+static int chrd_roce_query_port(struct ib_device *ibdev, u32 port,
+			   struct ib_port_attr *props)
 {
-	struct chrd_dev *chrd_dev =
-			rdma_device_to_drv_device(dev, struct chrd_dev, ibdev);
-	pr_debug("dev 0x%p\n", dev);
+	pr_debug("ibdev %p\n", ibdev);
+
+	props->port_cap_flags =
+	    IB_PORT_CM_SUP |
+	    IB_PORT_SNMP_TUNNEL_SUP |
+	    IB_PORT_REINIT_SUP |
+	    IB_PORT_DEVICE_MGMT_SUP |
+	    IB_PORT_VENDOR_CLASS_SUP | IB_PORT_BOOT_MGMT_SUP |
+	    RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP;
+	props->gid_tbl_len = 1024; /*Todo: set it appropriately*/
+	props->ip_gids = true;
+	props->lid = 0;
+	props->max_mtu = IB_MTU_4096;
+	props->active_mtu = IB_MTU_1024;
+	props->state = IB_PORT_ACTIVE;
+	props->phys_state = IB_PORT_PHYS_STATE_LINK_UP;
+	props->sm_lid = 0;
+	props->pkey_tbl_len = 1;
+	props->active_width = 2; /*Todo: set it appropriately*/
+	props->active_speed = 2; /*Todo: set it appropriately*/
+	props->max_msg_sz = 0x800000; /*Todo: set it appropriately*/
+
+	return 0;
+}
+
+static ssize_t hw_rev_show(struct device *dev,
+			   struct device_attribute *attr, char *buf)
+{
+	struct chrd_dev *chrd_dev =
+			rdma_device_to_drv_device(dev, struct chrd_dev, ibdev);
+	pr_debug("dev 0x%p\n", dev);
 	return sprintf(buf, "%d\n",
 		       CHELSIO_CHIP_RELEASE(chrd_dev->rdev.lldi.adapter_type));
 }
+static DEVICE_ATTR_RO(hw_rev);
 
-static ssize_t show_hca(struct device *dev, struct device_attribute *attr,
-			char *buf)
+static ssize_t hca_type_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
 {
 	struct chrd_dev *chrd_dev =
 			rdma_device_to_drv_device(dev, struct chrd_dev, ibdev);
 	struct ethtool_drvinfo info;
 	struct net_device *lldev = chrd_dev->rdev.lldi.ports[0];
 
 	pr_debug("dev 0x%p\n", dev);
 	lldev->ethtool_ops->get_drvinfo(lldev, &info);
 	return sprintf(buf, "%s\n", info.driver);
 }
+static DEVICE_ATTR_RO(hca_type);
 
-static ssize_t show_board(struct device *dev, struct device_attribute *attr,
-			  char *buf)
+static ssize_t board_id_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
 {
 	struct chrd_dev *chrd_dev =
 			rdma_device_to_drv_device(dev, struct chrd_dev, ibdev);
 	pr_debug("dev 0x%p\n", dev);
 	return sprintf(buf, "%x.%x\n", chrd_dev->rdev.lldi.vendor_id,
 		       chrd_dev->rdev.lldi.device_id);
 }
+static DEVICE_ATTR_RO(board_id);
 
 #ifdef IWARP_DEV_COUNTER_DYNAMIC
 enum counters {
@@ -566,24 +731,24 @@ static int c4iw_get_mib(struct ib_device *ibdev,
 #endif
 }
 
-static DEVICE_ATTR(hw_rev, S_IRUGO, show_rev, NULL);
-static DEVICE_ATTR(hca_type, S_IRUGO, show_hca, NULL);
-static DEVICE_ATTR(board_id, S_IRUGO, show_board, NULL);
+static struct attribute *chrd_class_attributes[] = {
+	&dev_attr_hw_rev.attr,
+	&dev_attr_hca_type.attr,
+	&dev_attr_board_id.attr,
+	NULL
+};
 
-static struct device_attribute *chrd_class_attributes[] = {
-	&dev_attr_hw_rev,
-	&dev_attr_hca_type,
-	&dev_attr_board_id,
+static const struct attribute_group chrd_attr_group = {
+	.attrs = chrd_class_attributes,
 };
 
-#ifdef IWARP_HAVE_CQ_INIT_ATTR
 static int chrd_iw_port_immutable(struct ib_device *ibdev, u32 port_num,
 			       struct ib_port_immutable *immutable)
 {
 	struct ib_port_attr attr;
 	int err;
 
 	err = chrd_iw_query_port(ibdev, port_num, &attr);
 	if (err)
 		return err;
 
@@ -593,7 +758,24 @@ static int c4iw_port_immutable(struct ib_device *ibdev, u32 port_num,
 
 	return 0;
 }
-#endif
+
+static int chrd_roce_port_immutable(struct ib_device *ibdev, u32 port_num,
+			       struct ib_port_immutable *immutable)
+{
+	struct ib_port_attr attr;
+	int err;
+
+	err = chrd_roce_query_port(ibdev, port_num, &attr);
+	if (err)
+		return err;
+
+	immutable->pkey_tbl_len = attr.pkey_tbl_len;
+	immutable->gid_tbl_len = attr.gid_tbl_len;
+	immutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP;
+	immutable->max_mad_size = IB_MGMT_MAD_SIZE;
+
+	return 0;
+}
 
 #ifdef HAVE_IB_FW_VER_NAME
 static void get_dev_fw_str(struct ib_device *dev, char *str)
@@ -602,91 +784,110 @@ static void get_dev_fw_str(struct ib_device *dev, char *str,
 			   size_t str_len)
 #endif
 {
 	struct chrd_dev *chrd_dev = container_of(dev, struct chrd_dev,
 						 ibdev);
 	pr_debug("dev 0x%p\n", dev);
 
 #ifdef HAVE_IB_FW_VER_NAME
 	snprintf(str, IB_FW_VERSION_NAME_MAX, "%u.%u.%u.%u",
 #else
 	snprintf(str, str_len, "%u.%u.%u.%u",
 #endif
 		 G_FW_HDR_FW_VER_MAJOR(chrd_dev->rdev.lldi.fw_vers),
 		 G_FW_HDR_FW_VER_MINOR(chrd_dev->rdev.lldi.fw_vers),
 		 G_FW_HDR_FW_VER_MICRO(chrd_dev->rdev.lldi.fw_vers),
 		 G_FW_HDR_FW_VER_BUILD(chrd_dev->rdev.lldi.fw_vers));
 }
 
-static const struct ib_device_ops chrd_dev_ops = {
+static const struct ib_device_ops chrd_common_dev_ops = {
 	.owner = THIS_MODULE,
 	.driver_id = RDMA_DRIVER_CXGB4,
 	.uverbs_abi_ver = CHRD_UVERBS_ABI_VERSION,
 
 #ifdef IWARP_DEV_COUNTER_DYNAMIC
 	.alloc_hw_device_stats = chrd_alloc_device_stats,
 	.get_hw_stats = chrd_get_mib,
 #endif
 	.alloc_mr = chrd_alloc_mr,
 	.alloc_mw = chrd_alloc_mw,
 	.alloc_pd = chrd_allocate_pd,
 	.alloc_ucontext = chrd_alloc_ucontext,
 	.create_cq = chrd_create_cq,
 	.create_qp = chrd_create_qp,
 	.create_srq = chrd_create_srq,
 	.dealloc_mw = chrd_dealloc_mw,
 	.dealloc_pd = chrd_deallocate_pd,
-	.create_ah = chrd_iw_create_ah,
 	.dealloc_ucontext = chrd_dealloc_ucontext,
 	.dereg_mr = chrd_dereg_mr,
 	.destroy_cq = chrd_destroy_cq,
 	.destroy_qp = chrd_destroy_qp,
 	.destroy_srq = chrd_destroy_srq,
+	.query_gid = chrd_query_gid,
 	.fill_res_cm_id_entry = chrd_fill_res_cm_id_entry,
-	.destroy_ah = chrd_iw_destroy_ah, 
 	.fill_res_cq_entry = chrd_fill_res_cq_entry,
 	.fill_res_mr_entry = chrd_fill_res_mr_entry,
 	.get_dev_fw_str = get_dev_fw_str,
 	.get_dma_mr = chrd_get_dma_mr,
-#ifdef IWARP_HAVE_CQ_INIT_ATTR
-	.get_port_immutable = chrd_port_immutable,
-#endif
-	.iw_accept = chrd_iw_accept_cr,
-	.iw_add_ref = chrd_iw_qp_add_ref,
-	.iw_connect = chrd_iw_connect,
-	.iw_create_listen = chrd_iw_create_listen,
-	.iw_destroy_listen = chrd_iw_destroy_listen,
-	.iw_get_qp = chrd_iw_get_qp,
-	.iw_reject = chrd_iw_reject_cr,
-	.iw_rem_ref = chrd_iw_qp_rem_ref,
 	.map_mr_sg = chrd_map_mr_sg,
 	.mmap = chrd_mmap,
-	.modify_qp = chrd_iw_modify_qp,
 	.modify_srq = chrd_modify_srq,
 	.poll_cq = chrd_poll_cq,
 	.post_recv = chrd_post_receive,
-	.post_send = chrd_iw_post_send,
 	.post_srq_recv = chrd_post_srq_recv,
 	.query_device = chrd_query_device,
-	.query_gid = chrd_query_gid,
-	.query_pkey = chrd_iw_query_pkey,
-	.query_port = chrd_query_port,
-	.modify_port = chrd_iw_modify_port,
-	.query_qp = chrd_ib_query_qp,
+	.query_qp = chrd_query_qp,
 	.reg_user_mr = chrd_reg_user_mr,
 	.req_notify_cq = chrd_arm_cq,
 	INIT_RDMA_OBJ_SIZE(ib_pd, chrd_pd, ibpd),
 	INIT_RDMA_OBJ_SIZE(ib_qp, chrd_qp, ibqp),
 	INIT_RDMA_OBJ_SIZE(ib_cq, chrd_cq, ibcq),
 	INIT_RDMA_OBJ_SIZE(ib_srq, chrd_srq, ibsrq),
 	INIT_RDMA_OBJ_SIZE(ib_ucontext, chrd_ucontext, ibucontext),
 };
 
-static int set_netdevs(struct ib_device *ib_dev, struct chrd_rdev *rdev)
+static const struct ib_device_ops chrd_iw_dev_ops = {
+	.get_port_immutable = chrd_iw_port_immutable,
+	.query_port = chrd_iw_query_port,
+	.modify_port = chrd_iw_modify_port,
+	.iw_accept = chrd_iw_accept_cr,
+	.iw_add_ref = chrd_iw_qp_add_ref,
+	.iw_connect = chrd_iw_connect,
+	.iw_create_listen = chrd_iw_create_listen,
+	.iw_destroy_listen = chrd_iw_destroy_listen,
+	.iw_get_qp = chrd_iw_get_qp,
+	.iw_reject = chrd_iw_reject_cr,
+	.iw_rem_ref = chrd_iw_qp_rem_ref,
+	.post_send = chrd_iw_post_send,
+	.modify_qp = chrd_iw_modify_qp,
+	.query_pkey = chrd_iw_query_pkey,
+	.create_ah = chrd_iw_create_ah,
+	.destroy_ah = chrd_iw_destroy_ah,
+};
+
+static const struct ib_device_ops chrd_roce_dev_ops = {
+	.get_port_immutable = chrd_roce_port_immutable,
+	.get_link_layer = chrd_roce_link_layer,
+	.query_port = chrd_roce_query_port,
+//	.add_gid = chrd_add_gid,		/*Todo: Check the Commented Code*/
+//	.del_gid = chrd_del_gid,
+	.query_pkey = chrd_roce_query_pkey,
+	.create_ah = chrd_roce_create_ah,
+//	.create_user_ah = chrd_roce_create_ah, /*Bhar: Recheck and enable for User mode*/
+	.destroy_ah = chrd_roce_destroy_ah,
+//	.modify_ah = chrd_modify_ah, /* add when needed*/
+	.query_ah = chrd_roce_query_ah,
+	.modify_qp = chrd_roce_modify_qp,
+	.post_send = chrd_roce_post_send,
+	INIT_RDMA_OBJ_SIZE(ib_ah, chrd_ah, ibah),
+};
+
+static int set_netdevs(struct ib_device *ib_dev, struct chrd_rdev *rdev,
+		       u32 nports)
 {
 	int ret;
 	int i;
 
-	for (i = 0; i < rdev->lldi.nports; i++) {
+	for (i = 0; i < nports; i++) {
 		ret = ib_device_set_netdev(ib_dev, rdev->lldi.ports[i],
 					   i + 1);
 		if (ret)
@@ -695,17 +896,31 @@ static int set_netdevs(struct ib_device *ib_dev, struct chrd_rdev *rdev)
 	return 0;
 }
 
+void chrd_init_roce_dev(struct chrd_dev *dev)
+{
+	addrconf_addr_eui48((unsigned char *)&dev->ibdev.node_guid,
+			    dev->rdev.lldi.ports[0]->dev_addr);
+	dev->ibdev.node_type = RDMA_NODE_IB_CA;
+	ib_set_device_ops(&dev->ibdev, &chrd_roce_dev_ops);
+}
+
+void chrd_init_iw_dev(struct chrd_dev *dev)
+{
+	memset(&dev->ibdev.node_guid, 0, sizeof(dev->ibdev.node_guid));
+	memcpy(&dev->ibdev.node_guid, dev->rdev.lldi.ports[0]->dev_addr, 6);
+	dev->ibdev.node_type = RDMA_NODE_RNIC;
+	ib_set_device_ops(&dev->ibdev, &chrd_iw_dev_ops);
+}
+
 void chrd_register_device(struct work_struct *work)
 {
 	struct uld_ctx *ctx = container_of(work, struct uld_ctx, reg_work);
 	struct chrd_dev *dev = ctx->dev;
+	u64 dma_mask;
 	int ret;
-	int i;
 
 	pr_debug("chrd_dev %p\n", dev);
 	strlcpy(dev->ibdev.name, "cxgb4_%d", IB_DEVICE_NAME_MAX);
-	memset(&dev->ibdev.node_guid, 0, sizeof(dev->ibdev.node_guid));
-	memcpy(&dev->ibdev.node_guid, dev->rdev.lldi.ports[0]->dev_addr, 6);
 	dev->ibdev.local_dma_lkey = 0;
 	dev->ibdev.uverbs_cmd_mask =
 #ifdef SIM
@@ -734,51 +949,54 @@ void c4iw_register_device(struct work_struct *work)
 	    (1ull << IB_USER_VERBS_CMD_CREATE_SRQ) |
 	    (1ull << IB_USER_VERBS_CMD_MODIFY_SRQ) |
 	    (1ull << IB_USER_VERBS_CMD_DESTROY_SRQ);
-	dev->ibdev.node_type = RDMA_NODE_RNIC;
-	memcpy(dev->ibdev.node_desc, CHRD_NODE_DESC, sizeof(CHRD_NODE_DESC));
-	dev->ibdev.phys_port_cnt = dev->rdev.lldi.nports;
+	memcpy(dev->ibdev.node_desc, CHRD_NODE_DESC, sizeof(CHRD_NODE_DESC));
 	dev->ibdev.num_comp_vectors =  dev->rdev.lldi.nciq;
 	dev->ibdev.dev.parent = dev->rdev.lldi.dev;
 #ifndef IWARP_DEV_COUNTER_DYNAMIC
 	dev->ibdev.get_protocol_stats = chrd_get_mib;
 #endif
-	ib_set_device_ops(&dev->ibdev, &chrd_dev_ops);
-	ret = set_netdevs(&dev->ibdev, &dev->rdev);
-	if (ret)
-		goto err_dealloc_ctx;
+	ib_set_device_ops(&dev->ibdev, &chrd_common_dev_ops);
+	dev->ibdev.dev.dma_parms = &dev->dma_parms;
+	dma_set_max_seg_size(dev->rdev.lldi.dev, UINT_MAX);
+	dma_mask = IS_ENABLED(CONFIG_64BIT) ? DMA_BIT_MASK(64) : DMA_BIT_MASK(32);
+	dma_coerce_mask_and_coherent(&dev->ibdev.dev, dma_mask);
+
+	if (roce_mode) {
+		dev->ibdev.phys_port_cnt = dev->rdev.lldi.nports; //nports should be one for RoCE as per per-port design
+		//dev->ibdev.phys_port_cnt = 1; //nports should be one for RoCE as per per-port design
+		ret = set_netdevs(&dev->ibdev, &dev->rdev,
+				  dev->ibdev.phys_port_cnt);
+		if (ret)
+			goto err_dealloc_ctx;
+		chrd_init_roce_dev(dev);
+	} else {
+		dev->ibdev.phys_port_cnt = dev->rdev.lldi.nports;
+		ret = set_netdevs(&dev->ibdev, &dev->rdev,
+				  dev->ibdev.phys_port_cnt);
+		if (ret)
+			goto err_dealloc_ctx;
+		chrd_init_iw_dev(dev);
+	}
 #ifdef IBREGDEV2
-	ret = ib_register_device(&dev->ibdev, "cxgb4_%d");
+	ret = ib_register_device(&dev->ibdev, "chrd_%d");
 #else
-	ret = ib_register_device(&dev->ibdev, "cxgb4_%d",
+	ret = ib_register_device(&dev->ibdev, "chrd_%d",
 				 dev->rdev.lldi.dev);
 #endif
 	if (ret)
 		goto err_dealloc_ctx;
 
-	for (i = 0; i < ARRAY_SIZE(chrd_class_attributes); ++i) {
-		ret = device_create_file(&dev->ibdev.dev,
-					 chrd_class_attributes[i]);
-		if (ret)
-			goto err_unregister_device;
-	}
 	return;
-err_unregister_device:
-	ib_unregister_device(&dev->ibdev);
 err_dealloc_ctx:
 	pr_err("%s - Failed registering iwarp device: %d\n",
 	       ctx->lldi.name, ret);
 	chrd_dealloc(ctx);
 	return;
 }
 
 void chrd_unregister_device(struct chrd_dev *dev)
 {
-	int i;
-
 	pr_debug("chrd_dev %p\n", dev);
-	for (i = 0; i < ARRAY_SIZE(chrd_class_attributes); ++i)
-		device_remove_file(&dev->ibdev.dev,
-				   chrd_class_attributes[i]);
 	ib_unregister_device(&dev->ibdev);
 	return;
 }
diff --git a/dev/T4/linux/iw_cxgb4/qp.c b/dev/T4/linux/iw_cxgb4/qp.c
index ac7246c9c..e2d022301 100644
--- a/dev/T4/linux/iw_cxgb4/qp.c
+++ b/dev/T4/linux/iw_cxgb4/qp.c
@@ -30,6 +30,7 @@
  * SOFTWARE.
  */
 #include "iw_cxgb4.h"
+#include <clip_tbl.h>
 
 #include <linux/module.h>
 #include <linux/ip.h>
@@ -38,6 +39,8 @@
 
 #include <rdma/ib_user_verbs.h>
 #include <rdma/uverbs_ioctl.h>
+#include <rdma/ib_verbs.h>
+#include <rdma/ib_cache.h>
 
 #include <cxgbtool.h>
 
@@ -99,18 +102,27 @@ static void set_state(struct c4iw_qp *qhp, enum c4iw_qp_state state)
 	spin_unlock_irqrestore(&qhp->lock, flag);
 }
 
+//Todo: optimise this further by removing the conversions
+static void set_v2_state(struct chrd_qp *qhp, enum chrd_v2_qp_state state)
+{
+	unsigned long flag;
+	spin_lock_irqsave(&qhp->lock, flag);
+	qhp->attr.state = state;
+	spin_unlock_irqrestore(&qhp->lock, flag);
+}
+
 static void dealloc_oc_sq(struct chrd_rdev *rdev, struct t4_sq *sq)
 {
 	cxgb4_ocqp_pool_free(rdev->lldi.ports[0], sq->dma_addr, sq->memsize);
 }
 
 static void dealloc_host_sq(struct chrd_rdev *rdev, struct t4_sq *sq)
 {
 	dma_free_coherent(rdev->lldi.dev, sq->memsize, sq->queue,
 			  dma_unmap_addr(sq, mapping));
 }
 
 static void dealloc_sq(struct chrd_rdev *rdev, struct t4_sq *sq)
 {
 	if (t4_sq_onchip(sq))
 		dealloc_oc_sq(rdev, sq);
@@ -202,14 +214,14 @@ static int del_filter(struct c4iw_raw_qp *rqp, int filter_id)
 	ret = cxgb4_del_filter(rqp->netdev, filter_id, NULL, &ctx, GFP_KERNEL);
 	rtnl_unlock();
 	if (!ret) {
-		ret = chrd_wait(&rqp->dev->rdev, &ctx.completion);
+		ret = chrd_wait(&rqp->rhp->rdev, &ctx.completion);
 		if (!ret)
 			ret = ctx.result;
 	}
 	return ret;
 }
 
 static void put_fid(struct chrd_raw_qp *rqp)
 {
 	int ret = 0;
 	int i;
@@ -221,12 +233,12 @@ static void put_fid(struct c4iw_raw_qp *rqp)
 			filter_id = rqp->fid + i;
 			ret = del_filter(rqp, filter_id);
 			if (!ret) {
-				filter_id += rqp->dev->rdev.nfids;
+				filter_id += rqp->rhp->rdev.nfids;
 				ret = del_filter(rqp, filter_id);
 			}
 			if (!ret || ret != -EBUSY)
 				break;
-			if (chrd_fatal_error(&rqp->dev->rdev)) {
+			if (chrd_fatal_error(&rqp->rhp->rdev)) {
 				ret = -EIO;
 				break;
 			}
@@ -241,17 +253,17 @@ static void put_fid(struct c4iw_raw_qp *rqp)
 	else {
 		u32 f;
 
-		f = rqp->fid - rqp->dev->rdev.lldi.tids->nhpftids;
-		spin_lock_irq(&rqp->dev->lock);
-		bitmap_clear(rqp->dev->rdev.fids, f, rqp->nfids);
-		spin_unlock_irq(&rqp->dev->lock);
+		f = rqp->fid - rqp->rhp->rdev.lldi.tids->nhpftids;
+		spin_lock_irq(&rqp->rhp->lock);
+		bitmap_clear(rqp->rhp->rdev.fids, f, rqp->nfids);
+		spin_unlock_irq(&rqp->rhp->lock);
 	}
 }
 
 static void free_srq_queue(struct chrd_srq *srq, struct chrd_dev_ucontext *uctx,
 			   struct chrd_wr_wait *wr_waitp)
 {
 	struct chrd_rdev *rdev = &srq->rhp->rdev;
 	struct sk_buff *skb = srq->destroy_skb;
 	struct t4_srq *wq = &srq->wq;
 	struct fw_ri_res_wr *res_wr;
@@ -908,25 +920,41 @@ void __iomem *c4iw_bar2_addrs(struct c4iw_rdev *rdev, unsigned int qid,
 	return rdev->bar2_kva + bar2_qoffset;
 }
 
-static int alloc_rc_queues(struct chrd_rdev *rdev, struct t4_wq *wq,
+static int alloc_rc_queues(struct chrd_dev *rhp, struct chrd_qp *qhp,
 			   struct t4_cq *rcq, struct t4_cq *scq,
-			   struct chrd_dev_ucontext *uctx, int need_rq,
-			   struct chrd_wr_wait *wr_waitp)
+			   struct chrd_dev_ucontext *uctx, int need_rq)
 {
-	int user = (uctx != &rdev->uctx);
+	struct chrd_wr_wait *wr_waitp = qhp->wr_waitp;
+	struct chrd_rdev *rdev = &rhp->rdev;
+	struct t4_wq *wq = &qhp->wq;
 	struct fw_ri_res_wr *res_wr;
 	struct fw_ri_res *res;
-	int wr_len;
 	struct sk_buff *skb;
-	int ret;
+	int user = (uctx != &rdev->uctx);
 	int eqsize;
+	int wr_len;
+	int ret;
 
+#if 0
+	if (unlikely(qhp->qp_type == IB_QPT_GSI)) {
+		wq->sq.qid = 1;
+	} else {
+		wq->sq.qid = chrd_iw_get_qpid(rdev, uctx);
+	}
+#endif
 	wq->sq.qid = chrd_iw_get_qpid(rdev, uctx);
 	if (!wq->sq.qid)
 		return -ENOMEM;
 
 	if (need_rq) {
+#if 0
+		if (unlikely(qhp->qp_type == IB_QPT_GSI)) {
+			wq->rq.qid = 2; /* Bhar:assume */
+		} else {
+			wq->rq.qid = chrd_iw_get_qpid(rdev, uctx);
+		}
+#endif
 		wq->rq.qid = chrd_iw_get_qpid(rdev, uctx);
 		if (!wq->rq.qid)
 			goto err1;
 	}
@@ -951,10 +979,11 @@ static int alloc_rc_queues(struct c4iw_rdev *rdev, struct t4_wq *wq,
 		 * RQT must be a power of 2 and at least 16 deep.
 		 */
 		wq->rq.rqt_size = roundup_pow_of_two(max_t(u16, wq->rq.size, 16));
 		wq->rq.rqt_hwaddr = chrd_rqtpool_alloc(rdev, wq->rq.rqt_size);
 		if (!wq->rq.rqt_hwaddr)
 			goto err4;
 	}
+	/* ToDo: Need RRQT populating support similar to above rqt pool alloc in FW */
 
 	if (user) {
 		if (alloc_oc_sq(rdev, &wq->sq) && alloc_host_sq(rdev, &wq->sq))
@@ -1313,7 +1342,7 @@ static int build_rdma_write(struct t4_sq *sq, union t4_wr *wqe,
 	wqe->write.plen = cpu_to_be32(plen);
 	return 0;
 }
-
+#if 0
 static void build_immd_cmpl(struct t4_sq *sq, struct fw_ri_immd_cmpl *immdp,
 			   const struct ib_send_wr *wr)
 {
@@ -1369,7 +1398,7 @@ static void build_rdma_write_cmpl(struct t4_sq *sq,
 
 	return;
 }
-
+#endif
 static int build_rdma_read(union t4_wr *wqe, const struct ib_send_wr *wr, u8 *len16)
 {
 	if (wr->num_sge > 1)
@@ -1538,167 +1567,704 @@ static int build_inv_stag(union t4_wr *wqe, const struct ib_send_wr *wr,
 	return 0;
 }
 
-void chrd_iw_qp_add_ref(struct ib_qp *qp)
-{
-	pr_debug("ib_qp %p\n", qp);
-	switch (qp->qp_type) {
-	case IB_QPT_RC:
-		refcount_inc(&to_chrd_qp(qp)->qp_refcnt);
-		break;
-	case IB_QPT_RAW_ETH:
-		atomic_inc(&(to_chrd_raw_qp(qp)->refcnt));
-		break;
-	default:
-		WARN_ONCE(1, "unknown qp type %u\n", qp->qp_type);
-	}
-}
+/*
+	RoCE Build WRs Start
+*/
 
-void chrd_iw_qp_rem_ref(struct ib_qp *qp)
+static inline void roce_fill_tnl_lso(struct chrd_qp *qhp,
+				     struct cpl_tx_tnl_lso *tnl_lso, u32 isgl_plen)
 {
-	pr_debug("ib_qp %p\n", qp);
-	switch (qp->qp_type) {
-	case IB_QPT_RC:
-		if (refcount_dec_and_test(&to_chrd_qp(qp)->qp_refcnt))
-			complete(&to_chrd_qp(qp)->qp_rel_comp);
-		break;
-	case IB_QPT_RAW_ETH:
-		if (atomic_dec_and_test(&(to_chrd_raw_qp(qp)->refcnt)))
-			wake_up(&(to_chrd_raw_qp(qp)->wait));
-		break;
-	default:
-		WARN_ONCE(1, "unknown qp type %u\n", qp->qp_type);
-	}
+	struct cpl_tx_pkt_core *tx_pkt_xt;
+	struct port_info *pi;
+	u32 val, ctrl0, plen;
+	u64 cntrl;
+	//int in_eth_xtra_len;		/*Todo: Check the Commented Code */
+	int l3hdr_len = 20; /*Bhar: fixed IP header length for ipv4, change it for ipv6 */
+	int eth_xtra_len = 0; /*Bhar: if vlan, fill it with vlan header length i.e 4 */
+	bool v6 = false; /*Bhar: enbale for ipv6 */
+
+	if (qhp->qp_type == IB_QPT_GSI)
+		plen = sizeof(struct chrd_gsi_hdr);
+	else
+		plen = sizeof(struct chrd_rc_hdr);
+	val = V_CPL_TX_TNL_LSO_OPCODE(CPL_TX_TNL_LSO) |
+		F_CPL_TX_TNL_LSO_FIRST |
+		F_CPL_TX_TNL_LSO_LAST |
+		(v6 ? F_CPL_TX_TNL_LSO_IPV6OUT : 0) |
+		//V_CPL_TX_TNL_LSO_ETHHDRLENOUT(eth_xtra_len / 4) |
+		V_CPL_TX_TNL_LSO_IPHDRLENOUT(l3hdr_len / 4) |
+		(v6 ? 0 : F_CPL_TX_TNL_LSO_IPHDRCHKOUT) | /*Bhar: check this later */
+		F_CPL_TX_TNL_LSO_IPLENSETOUT | /*Bhar: check this later */
+		(v6 ? 0 : F_CPL_TX_TNL_LSO_IPIDINCOUT); /*Bhar: check this later */
+	tnl_lso->op_to_IpIdSplitOut = htonl(val);
+	tnl_lso->IpIdOffsetOut = 0;			/*Todo: hardcoded value */
+
+#if 0
+        /* Get the tunnel header length */
+        val = skb_inner_mac_header(skb) - skb_mac_header(skb);
+        in_eth_xtra_len = skb_inner_network_header(skb) -
+                          skb_inner_mac_header(skb) - ETH_HLEN;
+#
+
+        switch (tnl_type) {
+        case TX_TNL_TYPE_VXLAN:
+        case TX_TNL_TYPE_GENEVE:
+                tnl_lso->UdpLenSetOut_to_TnlHdrLen =
+                        htons(F_CPL_TX_TNL_LSO_UDPCHKCLROUT |
+                        F_CPL_TX_TNL_LSO_UDPLENSETOUT);
+                break;
+        default:
+                tnl_lso->UdpLenSetOut_to_TnlHdrLen = 0;  /*Bhar: move this out if needed */
+                break;
+        }
+
+	tnl_lso->UdpLenSetOut_to_TnlHdrLen |=
+		htons(V_CPL_TX_TNL_LSO_TNLHDRLEN(val) |
+		V_CPL_TX_TNL_LSO_TNLTYPE(tnl_type));
+#endif
+	tnl_lso->ipsecen_to_rocev2 = htonl(F_CPL_TX_TNL_LSO_ROCEV2);
+	tnl_lso->roce_eth = htonl(0);/* Tells the HW if there is additional, 32bit AETH/IETH, for now not needed in SW */
+	val = 0;
+	val = V_CPL_TX_TNL_LSO_ETHHDRLEN(eth_xtra_len / 4) |
+	      V_CPL_TX_TNL_LSO_IPV6(v6 ? 1 : 0) |
+	      V_CPL_TX_TNL_LSO_IPHDRLEN(l3hdr_len / 4) |
+	      V_CPL_TX_TNL_LSO_TCPHDRLEN(20 / 4);
+	tnl_lso->Flow_to_TcpHdrLen = htonl(val);
+	tnl_lso->IpIdOffset = htons(0);
+	tnl_lso->IpIdSplit_to_Mss = htons(V_CPL_TX_TNL_LSO_MSS_PMTU(2) | V_CPL_TX_TNL_LSO_MSS_ACKREQ(2)); /* 2 corresponds to PMTU = 1024, hard coded for now, fix it later*/
+	//tnl_lso->TCPSeqOffset = htonl(0);		/*Todo: Check the Commented Code*/
+	tnl_lso->EthLenOffset_Size = htonl(V_CPL_TX_TNL_LSO_SIZE(plen + isgl_plen)); /* Size of Eth/ipv4/udp/bth */
+
+	tx_pkt_xt = (void *)(tnl_lso + 1);
+#if 0
+	if (iph->version == 4) {
+                iph->check = 0;
+                iph->tot_len = 0;
+                iph->check = ~ip_fast_csum((u8 *)iph, iph->ihl);
+        }
+        if (skb->ip_summed == CHECKSUM_PARTIAL)
+                cntrl = hwcsum(adap->params.chip, skb);
+#endif
+	pi = (struct port_info *)netdev_priv(qhp->netdev);
+	ctrl0 = V_TXPKT_OPCODE(CPL_TX_PKT_XT) | V_TXPKT_INTF(pi->port_id) |
+		V_TXPKT_PF(qhp->rhp->rdev.lldi.pf);
+	//ctrl0 |= V_TXPKT_T5_OVLAN_IDX(q->dcb_prio); /*Bhar: check with FW */
+
+	tx_pkt_xt->ctrl0 = htonl(ctrl0);
+	tx_pkt_xt->pack = htons(0);
+	tx_pkt_xt->len = htons(plen + isgl_plen); /* Size of Eth/ipv4/udp/bth */
+	cntrl = F_TXPKT_L4CSUM_DIS | F_TXPKT_IPCSUM_DIS | V_TXPKT_CSUM_TYPE(13);
+	//cntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(skb_vlan_tag_get(skb)); /* Bhar: disable vlan for bringup */
+
+	cntrl |= V_CPL_TX_PKT_XT_ROCECHKINSMODE(0) | V_CPL_TX_PKT_XT_ROCEIPHDRLEN((12 + 8) / 4) | V_CPL_TX_PKT_XT_ROCECHKSTARTOFFSET(14) | V_CPL_TX_PKT_XT_CHKSTOPOFFSET(4);
+
+	pr_debug("cpl_tx_pkt_core tnl_lso 0x%llx tx_pkt_xt 0x%llx ctrl0 0x%x cntrl 0x%llx\n",
+		 (unsigned long long)tnl_lso, (unsigned long long)tx_pkt_xt, ctrl0, cntrl);
+	tx_pkt_xt->ctrl1 = cpu_to_be64(cntrl);
 }
 
-static void add_to_fc_list(struct list_head *head, struct list_head *entry)
+static int build_v2_ud_rdma_send(struct chrd_qp *qhp, union t4_wr *wqe,
+				 const struct ib_send_wr *wr, u8 *len16)
 {
-	if (list_empty(entry))
-		list_add_tail(entry, head);
-}
+	struct chrd_ah *ahp = to_chrd_ah(ud_wr(wr)->ah);
+	struct cpl_tx_tnl_lso *tnl_lso = (struct cpl_tx_tnl_lso *)wqe->v2_ud_send.tnl_lso;
+	u32 dest_qp = ud_wr(wr)->remote_qpn;
+	u32 q_key = ud_wr(wr)->remote_qkey;
+	u16 p_key = ud_wr(wr)->pkey_index;
+	struct t4_sq *sq = &qhp->wq.sq;
+	struct fw_ri_immd *immd_src;
+	struct fw_ri_isgl *isgl_src;
+	struct chrd_gsi_hdr *hdrp;
+	int hlen = ud_wr(wr)->hlen;
+	int mss = ud_wr(wr)->mss;
+	u32 immdlen;
+	u32 plen;
+	int size;
+	int ret;
 
-static int ring_kernel_txq_db(struct chrd_raw_qp *rqp, u16 inc)
-{
-	unsigned long flags;
+	pr_debug("ahp 0x%llx num_sge %u, dest_qp %u, q_key %x, p_key %x, hlen %d, mss %d,"
+		 " PSN %u, inline %s, solicited %s\n", (unsigned long long)ahp, wr->num_sge,
+		 dest_qp, q_key, p_key, hlen, mss,
+		 qhp->roce_attr.gsi_attr.psn_nxt,
+		 wr->send_flags & IB_SEND_INLINE ? "true":"false",
+		 wr->send_flags & IB_SEND_SOLICITED ? "true":"false");
 
-	spin_lock_irqsave(&rqp->dev->lock, flags);
-	if (rqp->dev->db_state == NORMAL)
-		writel(V_QID(rqp->txq.cntxt_id) | V_PIDX(inc),
-		       rqp->dev->rdev.lldi.db_reg);
-	else {
-		add_to_fc_list(&rqp->dev->db_fc_list, &rqp->fcl.db_fc_entry);
-		rqp->txq.pidx_inc += inc;
+	if (wr->num_sge > T4_MAX_SEND_SGE)
+		return -EINVAL;
+	switch (wr->opcode) {
+	case IB_WR_SEND:
+		if (wr->send_flags & IB_SEND_SOLICITED)
+			wqe->v2_ud_send.sendop_psn = cpu_to_be32(
+				V_FW_RI_V2_SEND_WR_SENDOP(FW_RI_SEND_WITH_SE) |
+				V_FW_RI_V2_SEND_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+		else
+			wqe->v2_ud_send.sendop_psn = cpu_to_be32(
+				V_FW_RI_V2_SEND_WR_SENDOP(FW_RI_ROCEV2_SEND) |
+				V_FW_RI_V2_SEND_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+		wqe->v2_ud_send.stag_inv = 0;
+		break;
+	default:
+		return -EINVAL;
 	}
-	spin_unlock_irqrestore(&rqp->dev->lock, flags);
-	return 0;
-}
-
-static int ring_kernel_sq_db(struct chrd_qp *qhp, u16 inc)
-{
-	unsigned long flags;
+	wqe->v2_ud_send.r2 = 0;
+	wqe->v2_ud_send.r4 = 0;
+
+/* ROCE Headers build*/
+	//immdp = (struct fw_ri_immd *)(wqe->v2_ud_send.tnl_lso + 1);		/*Todo: Check the Commented Code */
+	hdrp = (struct chrd_gsi_hdr *)(wqe->v2_ud_send.tnl_lso + sizeof(struct cpl_tx_tnl_lso) + sizeof(struct cpl_tx_pkt_core));
+	immdlen = sizeof(struct cpl_tx_tnl_lso) + sizeof(struct cpl_tx_pkt_core) + sizeof(struct chrd_gsi_hdr);
+	//wqe->v2_ud_send.immdlen = roundup(immdlen, 16);	/*Todo: Check the Commented Code */
+	wqe->v2_ud_send.immdlen = immdlen;
+	immd_src = (struct fw_ri_immd *)(wqe->v2_ud_send.tnl_lso + roundup(wqe->v2_ud_send.immdlen, 16));
+	pr_debug(" eth vlan %lu eth %lu ip %lu ipv6 %lu udp %lu bth %lu deth %lu\n",
+		 sizeof(struct vlan_ethhdr), sizeof(struct ethhdr),
+		 sizeof(struct iphdr), sizeof(struct ipv6hdr),
+		 sizeof(struct udphdr), sizeof(struct chrd_bth_hdr),
+		 sizeof(struct ib_unpacked_deth));
+	pr_debug("ahp 0x%llx wqe 0x%llx tnl_lso 0x%llx hdrp 0x%llx "
+		 "gsi size %lu immdt 0x%llx immdlen 0x%x ud_immdlen 0x%x\n",
+		 (unsigned long long)ahp, (unsigned long long)wqe,
+		 (unsigned long long)tnl_lso,
+		 (unsigned long long)hdrp, sizeof(struct chrd_gsi_hdr),
+		 (unsigned long long)immd_src, immdlen, wqe->v2_ud_send.immdlen);
+	//immdp->op = FW_RI_DATA_IMMD;
+	//immdp->r1 = 0;		/*Todo: Check the Commented Code*/
+	//immdp->r2 = 0;
+	//immdp->immdlen = cpu_to_be32(54);//Bhar: check this while bringup
+
+	/* build headers here  */
+	/* ETH header */
+	memcpy(hdrp->roce_hdr.eth.ethh.h_source, ahp->smac, ETH_ALEN);
+	memcpy(hdrp->roce_hdr.eth.ethh.h_dest, ahp->dmac, ETH_ALEN);
+	//hdrp->roce_hdr.eth.ethh.h_proto = htons(is_ipv4 ? ETH_P_IP : ETH_P_IPV6);	/*Todo: Check the Commented Code */
+	hdrp->roce_hdr.eth.ethh.h_proto = cpu_to_be16(ETH_P_IP); //Bhar: modify this to above later
+
+	//bhar: check for ipv4 or ipv6 from gsi attr later
+	/* IP header */
+	hdrp->roce_hdr.iph.ip4h.version = 4;
+	hdrp->roce_hdr.iph.ip4h.frag_off = 0x40; // set fragment off
+	hdrp->roce_hdr.iph.ip4h.protocol = IPPROTO_UDP;
+	hdrp->roce_hdr.iph.ip4h.ihl = 0x5;		/*Todo: hardcoded value */
+	hdrp->roce_hdr.iph.ip4h.ttl = 0x40;		/*Todo: hardcoded value */
+	memcpy(&hdrp->roce_hdr.iph.ip4h.daddr, &ahp->dest_ip_addr[3], 4);
+	memcpy(&hdrp->roce_hdr.iph.ip4h.saddr, &ahp->local_ip_addr[3], 4);
+	hdrp->roce_hdr.iph.ip4h.check = (u16)(~ip_fast_csum((u8 *)&hdrp->roce_hdr.iph.ip4h, hdrp->roce_hdr.iph.ip4h.ihl));
+
+	/* UDP header */
+	hdrp->roce_hdr.udph.source = cpu_to_be16(ahp->src_port);
+	hdrp->roce_hdr.udph.dest = cpu_to_be16(ahp->dst_port);
+
+	/* BTH header */
+	hdrp->roce_hdr.bth.flags = 0;
+	hdrp->roce_hdr.bth.pkey = cpu_to_be16(0xFFFF);	/*Todo: hardcoded value */
+	hdrp->roce_hdr.bth.destination_qpn = cpu_to_be32(ahp->dest_qp);
+	hdrp->roce_hdr.bth.apsn = cpu_to_be32(qhp->roce_attr.gsi_attr.psn_nxt);
+
+	/* DETH header */
+	hdrp->deth.qkey = cpu_to_be32(0x80010000);	/*Todo: hardcoded value */
+	hdrp->deth.source_qpn = cpu_to_be32(1);
+	/* Init WR has 16B word boundary.may need to initialize last
+        10B(64 - 54) with 0 */
+
+/* */
 
-	xa_lock_irqsave(&qhp->rhp->qps, flags);
-	spin_lock(&qhp->lock);
-	if (qhp->rhp->db_state == NORMAL)
-		t4_ring_sq_db(&qhp->wq, inc, NULL);
-	else {
-		add_to_fc_list(&qhp->rhp->db_fc_list, &qhp->fcl.db_fc_entry);
-		qhp->wq.sq.wq_pidx_inc += inc;
+	plen = 0;
+	if (wr->num_sge) {
+		if (wr->send_flags & IB_SEND_INLINE) {
+			ret = build_immd(sq, immd_src, wr,
+					 T4_MAX_SEND_INLINE, &plen);
+			if (ret)
+				return ret;
+			size = sizeof wqe->v2_ud_send + roundup(sizeof(struct chrd_gsi_hdr), 16) +
+				sizeof(struct fw_ri_immd) + plen;
+		} else {
+			isgl_src = (struct fw_ri_isgl *)(wqe->v2_ud_send.tnl_lso + roundup(wqe->v2_ud_send.immdlen, 16));
+			ret = build_isgl((__be64 *)sq->queue,
+					 (__be64 *)&sq->queue[sq->size],
+					 isgl_src,
+					 wr->sg_list, wr->num_sge, &plen);
+			if (ret)
+				return ret;
+			size = sizeof wqe->v2_ud_send + roundup(sizeof(struct chrd_gsi_hdr), 16) +
+			       sizeof(struct fw_ri_isgl) +
+			       wr->num_sge * sizeof(struct fw_ri_sge);
+		}
+	} else {
+		immd_src[0].op = FW_RI_DATA_IMMD;
+		immd_src[0].r1 = 0;
+		immd_src[0].r2 = 0;
+		immd_src[0].immdlen = 0;
+		size = sizeof wqe->v2_ud_send + sizeof(struct fw_ri_immd);
+		plen = 0;
 	}
-	spin_unlock(&qhp->lock);
-	xa_unlock_irqrestore(&qhp->rhp->qps, flags);
+	roce_fill_tnl_lso(qhp, tnl_lso, plen);
+	tnl_lso->TCPSeqOffset = htonl(V_CPL_TX_TNL_LSO_BTH_OPCODE(0x64) |
+				      V_CPL_TX_TNL_LSO_TCPSEQOFFSET_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+	*len16 = DIV_ROUND_UP(size, 16);
+	wqe->v2_ud_send.plen = cpu_to_be32(plen);
+	qhp->roce_attr.gsi_attr.psn_nxt += 1;
+	qhp->roce_attr.gsi_attr.psn_nxt &= 0xFFFFFF;		/*Todo: hardcoded value*/
 	return 0;
 }
 
-static int ring_kernel_fl_db(struct chrd_raw_qp *rqp, u16 inc)
+static int build_v2_rdma_send(struct chrd_qp *qhp, union t4_wr *wqe,
+			      const struct ib_send_wr *wr, u8 *len16)
 {
-	unsigned long flags;
-	u32 val = 0;
-	unsigned int chip_ver = CHELSIO_CHIP_VERSION(rqp->dev->rdev.lldi.adapter_type);
+	struct t4_sq *sq = &qhp->wq.sq;
+	u32 plen;
+	int size;
+	int ret;
 
-	switch (chip_ver) {
-	case CHELSIO_T4:
-		val = V_PIDX(inc) | F_DBPRIO;
+	if (wr->num_sge > T4_MAX_SEND_SGE)
+		return -EINVAL;
+	switch (wr->opcode) {
+	case IB_WR_SEND:
+		if (wr->send_flags & IB_SEND_SOLICITED)
+			wqe->v2_send.sendop_psn = cpu_to_be32(
+				V_FW_RI_V2_SEND_WR_SENDOP(FW_RI_SEND_WITH_SE) |
+				V_FW_RI_V2_SEND_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+		else
+			wqe->v2_send.sendop_psn = cpu_to_be32(
+				V_FW_RI_V2_SEND_WR_SENDOP(FW_RI_ROCEV2_SEND) |
+				V_FW_RI_V2_SEND_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+		wqe->v2_send.stag_inv = 0;
 		break;
-	case CHELSIO_T5:
-		val = F_DBPRIO;
-		fallthrough; /* fallthrough */
-	case CHELSIO_T6:
-	case CHELSIO_T7:
-	default:
-		val |= V_PIDX_T5(inc);
+	case IB_WR_SEND_WITH_INV:
+		if (wr->send_flags & IB_SEND_SOLICITED)
+			wqe->v2_send.sendop_psn = cpu_to_be32(
+				V_FW_RI_V2_SEND_WR_SENDOP(FW_RI_SEND_WITH_SE_INV) |
+				V_FW_RI_V2_SEND_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+		else
+			wqe->v2_send.sendop_psn = cpu_to_be32(
+				V_FW_RI_V2_SEND_WR_SENDOP(FW_RI_ROCEV2_SEND_WITH_INV) |
+				V_FW_RI_V2_SEND_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+		wqe->v2_send.stag_inv = cpu_to_be32(wr->ex.invalidate_rkey);
 		break;
+	default:
+		return -EINVAL;
 	}
+	wqe->v2_send.r2 = 0;
+	wqe->v2_send.r4 = 0;
 
-	spin_lock_irqsave(&rqp->dev->lock, flags);
-	if (rqp->dev->db_state == NORMAL)
-		writel(V_QID(rqp->fl.cntxt_id) | val,
-		       rqp->dev->rdev.lldi.db_reg);
-	else {
-		add_to_fc_list(&rqp->dev->db_fc_list, &rqp->fcl.db_fc_entry);
-		rqp->fl.pidx_inc += inc;
+	plen = 0;
+	if (wr->num_sge) {
+		if (wr->send_flags & IB_SEND_INLINE) {
+			ret = build_immd(sq, wqe->v2_send.u.immd_src, wr,
+					 T4_MAX_SEND_INLINE, &plen);
+			if (ret)
+				return ret;
+			size = sizeof wqe->v2_send + sizeof(struct fw_ri_immd) +
+			       plen;
+		} else {
+			ret = build_isgl((__be64 *)sq->queue,
+					 (__be64 *)&sq->queue[sq->size],
+					 wqe->v2_send.u.isgl_src,
+					 wr->sg_list, wr->num_sge, &plen);
+			if (ret)
+				return ret;
+			size = sizeof wqe->v2_send + sizeof(struct fw_ri_isgl) +
+			       wr->num_sge * sizeof(struct fw_ri_sge);
+		}
+	} else {
+		wqe->v2_send.u.immd_src[0].op = FW_RI_DATA_IMMD;
+		wqe->v2_send.u.immd_src[0].r1 = 0;
+		wqe->v2_send.u.immd_src[0].r2 = 0;
+		wqe->v2_send.u.immd_src[0].immdlen = 0;
+		size = sizeof wqe->v2_send + sizeof(struct fw_ri_immd);
+		plen = 0;
 	}
-	spin_unlock_irqrestore(&rqp->dev->lock, flags);
+	pr_debug("num_sge %u, PSN %u, inline %s, solicited %s, size %u, plen %u\n",
+		 wr->num_sge, qhp->roce_attr.gsi_attr.psn_nxt,
+		 wr->send_flags & IB_SEND_INLINE ? "true":"false",
+		 wr->send_flags & IB_SEND_SOLICITED ? "true":"false", size, plen);
+
+	*len16 = DIV_ROUND_UP(size, 16);
+	wqe->v2_send.plen = cpu_to_be32(plen);
+	qhp->roce_attr.gsi_attr.psn_nxt += DIV_ROUND_UP(plen, 1024);//Bhar: 1024 assuming 1500 mtu, change hardcoded value later.
+	qhp->roce_attr.gsi_attr.psn_nxt &= 0xFFFFFF;		/*Todo: hardcoded value */
 	return 0;
 }
 
-static int ring_kernel_srq_db(struct chrd_raw_srq *srq, u16 inc)
+static int build_v2_rdma_write(struct chrd_qp *qhp, union t4_wr *wqe,
+			       const struct ib_send_wr *wr, u8 *len16)
 {
-	unsigned long flags;
-	u32 val = 0;
-	unsigned int chip_ver = CHELSIO_CHIP_VERSION(srq->dev->rdev.lldi.adapter_type);
+	struct t4_sq *sq = &qhp->wq.sq;
+	u32 plen;
+	int size;
+	int ret;
 
-	switch (chip_ver) {
-	case CHELSIO_T4:
-		val = V_PIDX(inc) | F_DBPRIO;
-		break;
-	case CHELSIO_T5:
-		val = F_DBPRIO;
-		fallthrough; /* fallthrough */
-	case CHELSIO_T6:
-	case CHELSIO_T7:
-	default:
-		val |= V_PIDX_T5(inc);
-		break;
+	if (wr->num_sge > T4_MAX_SEND_SGE)
+		return -EINVAL;
+	if (wr->opcode == IB_WR_RDMA_WRITE_WITH_IMM)
+		wqe->v2_write.immd_data = wr->ex.imm_data;
+	else
+		wqe->v2_write.immd_data = 0;
+	wqe->v2_write.stag_sink = cpu_to_be32(rdma_wr(wr)->rkey);
+	wqe->v2_write.to_sink = cpu_to_be64(rdma_wr(wr)->remote_addr);
+	if (wr->num_sge) {
+		if (wr->send_flags & IB_SEND_INLINE) {
+			ret = build_immd(sq, wqe->v2_write.u.immd_src, wr,
+					 T4_MAX_WRITE_INLINE, &plen);
+			if (ret)
+				return ret;
+			size = sizeof wqe->v2_write + sizeof(struct fw_ri_immd) +
+			       plen;
+		} else {
+			ret = build_isgl((__be64 *)sq->queue,
+					 (__be64 *)&sq->queue[sq->size],
+					 wqe->v2_write.u.isgl_src,
+					 wr->sg_list, wr->num_sge, &plen);
+			if (ret)
+				return ret;
+			size = sizeof wqe->v2_write + sizeof(struct fw_ri_isgl) +
+			       wr->num_sge * sizeof(struct fw_ri_sge);
+		}
+	} else {
+		wqe->v2_write.u.immd_src[0].op = FW_RI_DATA_IMMD;
+		wqe->v2_write.u.immd_src[0].r1 = 0;
+		wqe->v2_write.u.immd_src[0].r2 = 0;
+		wqe->v2_write.u.immd_src[0].immdlen = 0;
+		size = sizeof wqe->v2_write + sizeof(struct fw_ri_immd);
+		plen = 0;
 	}
+	pr_debug("num_sge %u, PSN %u, size %u plen %u\n",
+		 wr->num_sge, qhp->roce_attr.gsi_attr.psn_nxt, size, plen);
 
-	spin_lock_irqsave(&srq->dev->lock, flags);
-	if (srq->dev->db_state == NORMAL)
-		writel(V_QID(srq->fl.cntxt_id) | val,
-		       srq->dev->rdev.lldi.db_reg);
-	else {
-		add_to_fc_list(&srq->dev->db_fc_list, &srq->fcl.db_fc_entry);
-		srq->fl.pidx_inc += inc;
-	}
-	spin_unlock_irqrestore(&srq->dev->lock, flags);
+	wqe->v2_write.psn_pkd = cpu_to_be32(
+			V_FW_RI_V2_RDMA_WRITE_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+	wqe->v2_write.r2 = 0;
+	wqe->v2_write.r5 = 0;
+	*len16 = DIV_ROUND_UP(size, 16);
+	wqe->v2_write.plen = cpu_to_be32(plen);
+	qhp->roce_attr.gsi_attr.psn_nxt += DIV_ROUND_UP(plen, 1024);//Bhar: 1024 assuming 1500 mtu, change hardcoded value later.
+	qhp->roce_attr.gsi_attr.psn_nxt &= 0xFFFFFF;
 	return 0;
 }
 
-static int ring_kernel_rq_db(struct chrd_qp *qhp, u16 inc)
+/* Bhar: Check this later when FW enables it */
+#if 0 
+static void build_v2_rdma_write_cmpl(struct t4_sq *sq,
+				  struct fw_ri_rdma_write_cmpl_wr *wcwr,
+				  const struct ib_send_wr *wr, u8 *len16)
 {
-	unsigned long flags;
+	u32 plen;
+	int size;
 
-	xa_lock_irqsave(&qhp->rhp->qps, flags);
-	spin_lock(&qhp->lock);
-	if (qhp->rhp->db_state == NORMAL)
-		t4_ring_rq_db(&qhp->wq, inc, NULL);
-	else {
-		add_to_fc_list(&qhp->rhp->db_fc_list, &qhp->fcl.db_fc_entry);
-		qhp->wq.rq.wq_pidx_inc += inc;
-	}
-	spin_unlock(&qhp->lock);
-	xa_unlock_irqrestore(&qhp->rhp->qps, flags);
-	return 0;
+	/*
+	 * This code assumes the struct fields preceeding the write isgl
+	 * fit in one 64B WR slot.  This is because the WQE is built
+	 * directly in the dma queue, and wrapping is only handled
+	 * by the code buildling sgls.  IE the "fixed part" of the wr
+	 * structs must all fit in 64B.  The WQE build code should probably be
+	 * redesigned to avoid this restriction, but for now just add
+	 * the BUILD_BUG_ON() to catch if this WQE struct gets too big.
+	 */
+	BUILD_BUG_ON(offsetof(struct fw_ri_rdma_write_cmpl_wr, u) > 64);
+
+	wcwr->stag_sink = cpu_to_be32(rdma_wr(wr)->rkey);
+	wcwr->to_sink = cpu_to_be64(rdma_wr(wr)->remote_addr);
+	if (wr->next->opcode == IB_WR_SEND)
+		wcwr->stag_inv = 0;
+	else
+		wcwr->stag_inv = cpu_to_be32(wr->next->ex.invalidate_rkey);
+	wcwr->r2 = 0;
+	wcwr->r3 = 0;
+
+ 	/* SEND_INV SGL */
+	if (wr->next->send_flags & IB_SEND_INLINE)
+		build_immd_cmpl(sq, &wcwr->u_cmpl.immd_src, wr->next);
+	else
+		build_isgl((__be64 *)sq->queue, (__be64 *)&sq->queue[sq->size],
+			   &wcwr->u_cmpl.isgl_src, wr->next->sg_list, 1, NULL);
+
+	/* WRITE SGL */
+	build_isgl((__be64 *)sq->queue, (__be64 *)&sq->queue[sq->size],
+		   wcwr->u.isgl_src, wr->sg_list, wr->num_sge, &plen);
+
+	size = sizeof *wcwr + sizeof(struct fw_ri_isgl) +
+	       wr->num_sge * sizeof(struct fw_ri_sge);
+	wcwr->plen = cpu_to_be32(plen);
+	*len16 = DIV_ROUND_UP(size, 16);
+
+	return;
 }
+#endif
 
-static int ib_to_fw_opcode(int ib_opcode)
+static int build_v2_rdma_read(struct chrd_qp *qhp, union t4_wr *wqe,
+			      const struct ib_send_wr *wr, u8 *len16)
 {
-	int opcode;
+	struct t4_sq *sq = &qhp->wq.sq;
+	u32 plen;
+	int size;
+	int ret;
+/* Bhar:
+	- Change the hard coded max_num_sge to max_num_read_sge attr */
 
-	switch (ib_opcode) {
-	case IB_WR_SEND_WITH_INV:
-		opcode = FW_RI_SEND_WITH_INV;
-		break;
-	case IB_WR_SEND:
+	if (wr->num_sge > 4)
+		return -EINVAL;
+
+	if (wr->num_sge) {
+		wqe->v2_read.stag_src = cpu_to_be32(rdma_wr(wr)->rkey);
+		wqe->v2_read.to_src = cpu_to_be64((u64)(rdma_wr(wr)->remote_addr));
+		ret = build_isgl((__be64 *)sq->queue,
+				 (__be64 *)&sq->queue[sq->size],
+				 &wqe->v2_read.isgl_sink,
+				 wr->sg_list, wr->num_sge, &plen);
+		if (ret)
+			return ret;
+		size = sizeof wqe->v2_read +
+			      wr->num_sge * sizeof(struct fw_ri_sge);
+	} else {
+		wqe->v2_read.stag_src = cpu_to_be32(2);
+		wqe->v2_read.to_src = 0;
+		size = sizeof wqe->v2_read;
+		plen = 0;
+	}
+	pbr_debug("num_sge %u, PSN %u, size %u plen %u\n",
+		  wr->num_sge, qhp->roce_attr.gsi_attr.psn_nxt, size, plen);
+
+	wqe->v2_read.psn_pkd = cpu_to_be32(
+			V_FW_RI_V2_RDMA_READ_WR_PSN(qhp->roce_attr.gsi_attr.psn_nxt));
+	wqe->v2_read.r2 = 0;
+	*len16 = DIV_ROUND_UP(size, 16);
+	wqe->v2_read.plen = cpu_to_be32(plen);
+	qhp->roce_attr.gsi_attr.psn_nxt += DIV_ROUND_UP(plen, 1024);//Bhar: 1024 assuming 1500 mtu, change hardcoded value later.
+	qhp->roce_attr.gsi_attr.psn_nxt &= 0xFFFFFF;
+	return 0;
+}
+
+static int build_v2_memreg(struct t4_sq *sq, union t4_wr *wqe,
+			const struct ib_reg_wr *wr, struct chrd_mr *mhp,
+			u8 *len16, bool dsgl_supported)
+{
+	struct fw_ri_immd *imdp;
+	__be64 *p;
+	int i;
+	int pbllen = roundup(mhp->mpl_len * sizeof(u64), 32);
+	int rem;
+
+	if (mhp->mpl_len > t4_max_fr_depth(&mhp->rhp->rdev, use_dsgl))
+		return -EINVAL;
+	if (wr->mr->page_size > T6_MAX_PAGE_SIZE)
+		return -EINVAL;
+
+	wqe->v2_fr.qpbinde_to_dcacpu = 0;
+	wqe->v2_fr.pgsz_shift = ilog2(wr->mr->page_size) - 12;
+	wqe->v2_fr.addr_type = FW_RI_VA_BASED_TO;
+	wqe->v2_fr.mem_perms = chrd_ib_to_tpt_access(wr->access);
+	wqe->v2_fr.len_hi = cpu_to_be32(mhp->ibmr.length >> 32);
+	wqe->v2_fr.len_lo = cpu_to_be32(mhp->ibmr.length & 0xffffffff);
+	wqe->v2_fr.stag = cpu_to_be32(wr->key);
+	wqe->v2_fr.va_hi = cpu_to_be32(mhp->ibmr.iova >> 32);
+	wqe->v2_fr.va_lo_fbo = cpu_to_be32(mhp->ibmr.iova &
+					0xffffffff);
+
+	if (dsgl_supported && use_dsgl && (pbllen > max_fr_immd)) {
+		struct fw_ri_dsgl *sglp;
+
+		for (i = 0; i < mhp->mpl_len; i++)
+			mhp->mpl[i] = (__force u64)cpu_to_be64((u64)mhp->mpl[i]);
+
+		sglp = (struct fw_ri_dsgl *)(&wqe->v2_fr + 1);
+		sglp->op = FW_RI_DATA_DSGL;
+		sglp->r1 = 0;
+		sglp->nsge = cpu_to_be16(1);
+		sglp->addr0 = cpu_to_be64(mhp->mpl_addr);
+		sglp->len0 = cpu_to_be32(pbllen);
+
+		*len16 = DIV_ROUND_UP(sizeof(wqe->v2_fr) + sizeof(*sglp), 16);
+	} else {
+		imdp = (struct fw_ri_immd *)(&wqe->v2_fr + 1);
+		imdp->op = FW_RI_DATA_IMMD;
+		imdp->r1 = 0;
+		imdp->r2 = 0;
+		imdp->immdlen = cpu_to_be32(pbllen);
+		p = (__be64 *)(imdp + 1);
+		rem = pbllen;
+		for (i = 0; i < mhp->mpl_len; i++) {
+			*p = cpu_to_be64((u64)mhp->mpl[i]);
+			rem -= sizeof(*p);
+			if (++p == (__be64 *)&sq->queue[sq->size])
+				p = (__be64 *)sq->queue;
+		}
+		while (rem) {
+			*p = 0;
+			rem -= sizeof(*p);
+			if (++p == (__be64 *)&sq->queue[sq->size])
+				p = (__be64 *)sq->queue;
+		}
+		*len16 = DIV_ROUND_UP(sizeof(wqe->v2_fr) + sizeof(*imdp)
+				      + pbllen, 16);
+	}
+	wqe->v2_fr.r2 = 0;
+	wqe->v2_fr.r3 = 0;
+	return 0;
+}
+/*
+	RoCE Build WRs End
+*/
+
+void chrd_iw_qp_add_ref(struct ib_qp *qp)
+{
+	pbr_debug("ib_qp 0x%llx\n", (unsigned long long)qp);
+	switch (qp->qp_type) {
+	case IB_QPT_RC:
+		refcount_inc(&to_chrd_qp(qp)->qp_refcnt);
+		break;
+	case IB_QPT_RAW_ETH:
+		atomic_inc(&(to_chrd_raw_qp(qp)->refcnt));
+		break;
+	default:
+		WARN_ONCE(1, "unknown qp type %u\n", qp->qp_type);
+	}
+}
+
+void chrd_iw_qp_rem_ref(struct ib_qp *qp)
+{
+	pr_debug("ib_qp 0x%llx\n", (unsigned long long)qp);
+	switch (qp->qp_type) {
+	case IB_QPT_RC:
+		if (refcount_dec_and_test(&to_chrd_qp(qp)->qp_refcnt))
+			complete(&to_chrd_qp(qp)->qp_rel_comp);
+		break;
+	case IB_QPT_RAW_ETH:
+		if (atomic_dec_and_test(&(to_chrd_raw_qp(qp)->refcnt)))
+			wake_up(&(to_chrd_raw_qp(qp)->wait));
+		break;
+	default:
+		WARN_ONCE(1, "unknown qp type %u\n", qp->qp_type);
+	}
+}
+
+static void add_to_fc_list(struct list_head *head, struct list_head *entry)
+{
+	if (list_empty(entry))
+		list_add_tail(entry, head);
+}
+
+static int ring_kernel_txq_db(struct chrd_raw_qp *rqp, u16 inc)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&rqp->rhp->lock, flags);
+	if (rqp->rhp->db_state == NORMAL)
+		writel(V_QID(rqp->txq.cntxt_id) | V_PIDX(inc),
+		       rqp->rhp->rdev.lldi.db_reg);
+	else {
+		add_to_fc_list(&rqp->rhp->db_fc_list, &rqp->fcl.db_fc_entry);
+		rqp->txq.pidx_inc += inc;
+	}
+	spin_unlock_irqrestore(&rqp->rhp->lock, flags);
+	return 0;
+}
+
+static int ring_kernel_sq_db(struct chrd_qp *qhp, u16 inc)
+{
+	unsigned long flags;
+
+	xa_lock_irqsave(&qhp->rhp->qps, flags);
+	spin_lock(&qhp->lock);
+	if (qhp->rhp->db_state == NORMAL)
+		t4_ring_sq_db(&qhp->wq, inc, NULL);
+	else {
+		add_to_fc_list(&qhp->rhp->db_fc_list, &qhp->fcl.db_fc_entry);
+		qhp->wq.sq.wq_pidx_inc += inc;
+	}
+	spin_unlock(&qhp->lock);
+	xa_unlock_irqrestore(&qhp->rhp->qps, flags);
+	return 0;
+}
+
+static int ring_kernel_fl_db(struct chrd_raw_qp *rqp, u16 inc)
+{
+	unsigned long flags;
+	u32 val = 0;
+	unsigned int chip_ver = CHELSIO_CHIP_VERSION(rqp->rhp->rdev.lldi.adapter_type);
+
+	switch (chip_ver) {
+	case CHELSIO_T4:
+		val = V_PIDX(inc) | F_DBPRIO;
+		break;
+	case CHELSIO_T5:
+		val = F_DBPRIO;
+		fallthrough; /* fallthrough */
+	case CHELSIO_T6:
+	case CHELSIO_T7:
+	default:
+		val |= V_PIDX_T5(inc);
+		break;
+	}
+
+	spin_lock_irqsave(&rqp->rhp->lock, flags);
+	if (rqp->rhp->db_state == NORMAL)
+		writel(V_QID(rqp->fl.cntxt_id) | val,
+		       rqp->rhp->rdev.lldi.db_reg);
+	else {
+		add_to_fc_list(&rqp->rhp->db_fc_list, &rqp->fcl.db_fc_entry);
+		rqp->fl.pidx_inc += inc;
+	}
+	spin_unlock_irqrestore(&rqp->rhp->lock, flags);
+	return 0;
+}
+
+static int ring_kernel_srq_db(struct chrd_raw_srq *srq, u16 inc)
+{
+	unsigned long flags;
+	u32 val = 0;
+	unsigned int chip_ver = CHELSIO_CHIP_VERSION(srq->dev->rdev.lldi.adapter_type);
+
+	switch (chip_ver) {
+	case CHELSIO_T4:
+		val = V_PIDX(inc) | F_DBPRIO;
+		break;
+	case CHELSIO_T5:
+		val = F_DBPRIO;
+		fallthrough; /* fallthrough */
+	case CHELSIO_T6:
+	case CHELSIO_T7:
+	default:
+		val |= V_PIDX_T5(inc);
+		break;
+	}
+
+	spin_lock_irqsave(&srq->dev->lock, flags);
+	if (srq->dev->db_state == NORMAL)
+		writel(V_QID(srq->fl.cntxt_id) | val,
+		       srq->dev->rdev.lldi.db_reg);
+	else {
+		add_to_fc_list(&srq->dev->db_fc_list, &srq->fcl.db_fc_entry);
+		srq->fl.pidx_inc += inc;
+	}
+	spin_unlock_irqrestore(&srq->dev->lock, flags);
+	return 0;
+}
+
+static int ring_kernel_rq_db(struct chrd_qp *qhp, u16 inc)
+{
+	unsigned long flags;
+
+	xa_lock_irqsave(&qhp->rhp->qps, flags);
+	spin_lock(&qhp->lock);
+	if (qhp->rhp->db_state == NORMAL)
+		t4_ring_rq_db(&qhp->wq, inc, NULL);
+	else {
+		add_to_fc_list(&qhp->rhp->db_fc_list, &qhp->fcl.db_fc_entry);
+		qhp->wq.rq.wq_pidx_inc += inc;
+	}
+	spin_unlock(&qhp->lock);
+	xa_unlock_irqrestore(&qhp->rhp->qps, flags);
+	return 0;
+}
+
+static int ib_to_fw_opcode(int ib_opcode)
+{
+	int opcode;
+
+	switch (ib_opcode) {
+	case IB_WR_SEND_WITH_INV:
+		opcode = FW_RI_SEND_WITH_INV;
+		break;
+	case IB_WR_SEND:
 		opcode = FW_RI_SEND;
 		break;
 	case IB_WR_RDMA_WRITE:
@@ -1723,30 +2289,32 @@ static int ib_to_fw_opcode(int ib_opcode)
 	return opcode;
 }
 
 static int complete_sq_drain_wr(struct chrd_qp *qhp,
 				const struct ib_send_wr *wr)
 {
 	struct t4_cqe cqe = {};
+//	struct t4_cqe *hw_cqe;
 	struct chrd_cq *schp;
 	unsigned long flag;
 	struct t4_cq *cq;
 	int opcode;
 
 	schp = to_chrd_cq(qhp->ibqp.send_cq);
 	cq = &schp->cq;
 
 	opcode = ib_to_fw_opcode(wr->opcode);
 	if (opcode < 0)
 		return opcode;
 
-	pr_debug("drain sq id %u\n", qhp->wq.sq.qid);
+	pr_debug("drain sq id %u, opcode %d\n", qhp->wq.sq.qid, opcode);
 	cqe.u.drain_cookie = wr->wr_id;
 	cqe.header = cpu_to_be32(V_CQE_STATUS(T4_ERR_SWFLUSH) |
-				 V_CQE_OPCODE(opcode) |
+				// V_CQE_V2_OPCODE(opcode) |			/*Todo: Check the Commented Code */
 				 V_CQE_TYPE(1) |
 				 V_CQE_SWCQE(1) |
 				 V_CQE_DRAIN(1) |
 				 V_CQE_QPID(qhp->wq.sq.qid));
+	cqe.u.v2_com.v2_header |= cpu_to_be32(V_CQE_V2_OPCODE(opcode));
 
 	spin_lock_irqsave(&schp->lock, flag);
 	cqe.bits_type_ts = cpu_to_be64(V_CQE_GENBIT((u64)cq->gen));
@@ -1754,6 +2322,30 @@ static int complete_sq_drain_wr(struct c4iw_qp *qhp,
 	t4_swcq_produce(cq);
 	spin_unlock_irqrestore(&schp->lock, flag);
 
+/*Todo: debug code  start*/
+#if 0
+/* Dump CQE start */
+	hw_cqe = &cqe;
+        unsigned char *w = (void *)hw_cqe;
+        unsigned int i;
+        for (i = 0; i < 4; i++) {
+                pr_debug("0x%02x: %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
+                          i, w[0], w[1], w[2], w[3], w[4], w[5], w[6], w[7],
+                          w[8], w[9], w[10], w[11], w[12], w[13], w[14], w[15]);
+                w = w + 16;
+        }
+/* Dump CQE end */
+        pr_debug("CQE OVF %u qpid 0x%0x genbit %u type %u status 0x%0x"
+             " opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x"
+             " cidx 0x%04x sw opc 0x%x v2_header 0x%x SQ pidx %u cidx %u\n",
+             CQE_OVFBIT(hw_cqe), CQE_QPID(hw_cqe),
+             CQE_GENBIT(hw_cqe), CQE_TYPE(hw_cqe), CQE_STATUS(hw_cqe),
+             CQE_V2_OPCODE(hw_cqe), CQE_LEN(hw_cqe), CQE_WRID_HI(hw_cqe),
+             CQE_WRID_LOW(hw_cqe), CQE_WRID_SQ_IDX(hw_cqe), qhp->wq.sq.sw_sq[CQE_WRID_SQ_IDX(hw_cqe)].opcode,
+             hw_cqe->u.v2_com.v2_header, qhp->wq.sq.pidx, qhp->wq.sq.cidx);
+#endif
+/*Todo: debug code end*/
+
 	if (t4_clear_cq_armed(&schp->cq, qhp->ibqp.uobject)) {
 		spin_lock_irqsave(&schp->comp_handler_lock, flag);
 		(*schp->ibcq.comp_handler)(&schp->ibcq,
@@ -1779,25 +2371,26 @@ static int complete_sq_drain_wrs(struct c4iw_qp *qhp, const struct ib_send_wr *w
 	return ret;
 }
 
 static void complete_rq_drain_wr(struct chrd_qp *qhp,
 				 const struct ib_recv_wr *wr)
 {
 	struct t4_cqe cqe = {};
 	struct chrd_cq *rchp;
 	unsigned long flag;
 	struct t4_cq *cq;
 
 	rchp = to_chrd_cq(qhp->ibqp.recv_cq);
 	cq = &rchp->cq;
 
 	pr_debug("drain rq id %u\n", qhp->wq.sq.qid);
 	cqe.u.drain_cookie = wr->wr_id;
 	cqe.header = cpu_to_be32(V_CQE_STATUS(T4_ERR_SWFLUSH) |
-				 V_CQE_OPCODE(FW_RI_SEND) |
+			//	 V_CQE_V2_OPCODE(FW_RI_SEND) |			/*Todo: Check the Commented Code */
 				 V_CQE_TYPE(0) |
 				 V_CQE_SWCQE(1) |
 				 V_CQE_DRAIN(1) |
 				 V_CQE_QPID(qhp->wq.sq.qid));
+	cqe.u.v2_com.v2_header |= cpu_to_be32(V_CQE_V2_OPCODE(FW_RI_SEND));
 
 	spin_lock_irqsave(&rchp->lock, flag);
 	cqe.bits_type_ts = cpu_to_be64(V_CQE_GENBIT((u64)cq->gen));
@@ -1821,8 +2414,8 @@ static void complete_rq_drain_wrs(struct c4iw_qp *qhp,
 		wr = wr->next;
 	}
 }
-
+#if 0
 static void post_write_cmpl(struct chrd_qp *qhp, const struct ib_send_wr *wr)
 {
 	bool send_signaled = (wr->next->send_flags & IB_SEND_SIGNALED) ||
 			     qhp->sq_sig_all;
@@ -1891,15 +2484,15 @@ static void post_write_cmpl(struct c4iw_qp *qhp, const struct ib_send_wr *wr)
 	t4_ring_sq_db(&qhp->wq, idx, wqe);
 	return;
 }
-
-static int post_rc_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
+#endif
+static int iw_post_rc_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 			const struct ib_send_wr **bad_wr)
 {
 	int err = 0;
 	u8 len16 = 0;
 	enum fw_wr_opcodes fw_opcode = 0;
 	enum fw_ri_wr_flags fw_flags;
 	struct chrd_qp *qhp;
 	union t4_wr *wqe = NULL;
 	u32 num_wrs;
 	struct t4_swsqe *swsqe;
@@ -1937,6 +2530,7 @@ static int post_rc_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 	 * below are not true, then we continue on with the tradtional WRITE
 	 * and SEND WRs.
 	 */
+#if 0 // temorarily disable write w compl
 	if (qhp->rhp->rdev.lldi.write_cmpl_support &&
 	    chip_ver >= CHELSIO_T5 && wr && wr->next &&
 	    !wr->next->next &&
@@ -1949,6 +2543,7 @@ static int post_rc_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 		spin_unlock_irqrestore(&qhp->lock, flag);
 		return 0;
 	}
+#endif
 
 	while (wr) {
 		if (num_wrs == 0) {
@@ -2058,9 +2653,9 @@ static int post_rc_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 
 		init_wr_hdr(wqe, qhp->wq.sq.pidx, fw_opcode, fw_flags, len16);
 
-		pr_debug("cookie 0x%llx pidx 0x%x opcode 0x%x read_len %u\n",
+		pr_debug("cookie 0x%llx pidx 0x%x opcode 0x%x read_len %u qid %u sig %u flag 0x%x\n",
 		     (unsigned long long)wr->wr_id, qhp->wq.sq.pidx,
-		     swsqe->opcode, swsqe->read_len);
+		     swsqe->opcode, swsqe->read_len, qhp->wq.sq.qid, swsqe->signaled, fw_flags);
 		wr = wr->next;
 		num_wrs--;
 		t4_sq_produce(&qhp->wq, len16);
@@ -2076,14 +2671,14 @@ static int post_rc_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 	return err;
 }
 
 int chrd_iw_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 		   const struct ib_send_wr **bad_wr)
 {
 	int ret = 0;
 
 	switch (ibqp->qp_type) {
 	case IB_QPT_RC:
-		ret = post_rc_send(ibqp, wr, bad_wr);
+		ret = iw_post_rc_send(ibqp, wr, bad_wr);
 		break;
 	default:
 		WARN_ONCE(1, "unknown qp type %u\n", ibqp->qp_type);
@@ -2152,8 +2747,8 @@ static int post_rc_receive(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
 		wqe->recv.r2[1] = 0;
 		wqe->recv.r2[2] = 0;
 		wqe->recv.len16 = len16;
-		pr_debug("cookie 0x%llx pidx %u\n",
-		     (unsigned long long) wr->wr_id, qhp->wq.rq.pidx);
+		pr_debug("cookie 0x%llx pidx %u qid %u\n",
+		     (unsigned long long) wr->wr_id, qhp->wq.rq.pidx, qhp->wq.sq.qid);
 		t4_rq_produce(&qhp->wq, len16);
 		idx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);
 		wr = wr->next;
@@ -2178,49 +2773,259 @@ int c4iw_post_receive(struct ib_qp *ibqp, const struct ib_recv_wr *wr,
 	case IB_QPT_RC:
 		ret = post_rc_receive(ibqp, wr, bad_wr);
 		break;
+	case IB_QPT_GSI:
+	case IB_QPT_UD:
+		//ret = post_gsi_receive(ibqp, wr, bad_wr);		/*Todo: Check the Commented Code */
+		ret = post_rc_receive(ibqp, wr, bad_wr);
+		break;
 	default:
 		WARN_ONCE(1, "unknown qp type %u\n", ibqp->qp_type);
 	}
 	return ret;
 }
 
-static void defer_srq_wr(struct t4_srq *srq, union t4_recv_wr *wqe,
-			 uint64_t wr_id, u8 len16)
-{
-	struct t4_srq_pending_wr *pwr = &srq->pending_wrs[srq->pending_pidx];
-
-	pr_debug("cidx %u pidx %u wq_pidx %u in_use %u ooo_count %u wr_id "
-		"0x%llx pending_cidx %u pending_pidx %u pending_in_use %u\n",
-		srq->cidx, srq->pidx, srq->wq_pidx,
-		srq->in_use, srq->ooo_count, (unsigned long long)wr_id,
-		srq->pending_cidx, srq->pending_pidx, srq->pending_in_use);
-	pwr->wr_id = wr_id;
-	pwr->len16 = len16;
-	memcpy(&pwr->wqe, wqe, len16*16);
-	t4_srq_produce_pending_wr(srq);
-}
-
-int chrd_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
-		       const struct ib_recv_wr **bad_wr)
+static int roce_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
+			  const struct ib_send_wr **bad_wr)
 {
 	int err = 0;
-	struct chrd_srq *srq;
-	union t4_recv_wr *wqe, lwqe;
-	u32 num_wrs;
 	u8 len16 = 0;
-	u16 idx = 0;
+	enum fw_wr_opcodes fw_opcode = 0;
+	enum fw_ri_wr_flags fw_flags;
+	struct chrd_qp *qhp;
+	union t4_wr *wqe = NULL;
+	u32 num_wrs;
+	struct t4_swsqe *swsqe;
 	unsigned long flag;
+	u16 idx = 0;
+	unsigned int chip_ver;
 
-	srq = to_chrd_srq(ibsrq);
-	spin_lock_irqsave(&srq->lock, flag);
-	num_wrs = t4_srq_avail(&srq->wq);
+	qhp = to_chrd_qp(ibqp);
+	spin_lock_irqsave(&qhp->lock, flag);
+
+	/*
+	 * If the qp has been flushed, then just insert a special
+	 * drain cqe.
+	 */
+	if (qhp->wq.flushed) {
+		spin_unlock_irqrestore(&qhp->lock, flag);
+		err = complete_sq_drain_wrs(qhp, wr, bad_wr);
+		pbe_debug("complete_sq_drain_wrs qid %u ret %d\n", qhp->wq.sq.qid, err);
+		return err;
+	}
+	num_wrs = t4_sq_avail(&qhp->wq);
 	if (num_wrs == 0) {
-		spin_unlock_irqrestore(&srq->lock, flag);
+		spin_unlock_irqrestore(&qhp->lock, flag);
+		*bad_wr = wr;
 		return -ENOMEM;
 	}
+
+	chip_ver = CHELSIO_CHIP_VERSION(qhp->rhp->rdev.lldi.adapter_type);
+
+	/* Bhar: Recheck UD QP cosiderations 
+		 PSN math is in FW */
 	while (wr) {
-		if (wr->num_sge > T4_MAX_RECV_SGE) {
-			err = -EINVAL;
+		if (num_wrs == 0) {
+			err = -ENOMEM;
+			*bad_wr = wr;
+			break;
+		}
+		wqe = (union t4_wr *)((u8 *)qhp->wq.sq.queue +
+		      qhp->wq.sq.wq_pidx * T4_EQ_ENTRY_SIZE);
+
+		fw_flags = 0;
+		if (wr->send_flags & IB_SEND_SOLICITED)
+			fw_flags |= FW_RI_SOLICITED_EVENT_FLAG;
+		if (wr->send_flags & IB_SEND_SIGNALED || qhp->sq_sig_all)
+			fw_flags |= FW_RI_COMPLETION_FLAG;
+		swsqe = &qhp->wq.sq.sw_sq[qhp->wq.sq.pidx];
+		switch (wr->opcode) {
+		case IB_WR_SEND_WITH_INV:
+		case IB_WR_SEND:
+			if (wr->send_flags & IB_SEND_FENCE)
+				fw_flags |= FW_RI_READ_FENCE_FLAG;
+			fw_opcode = FW_RI_V2_SEND_WR;
+			if (qhp->qp_type == IB_QPT_GSI || qhp->qp_type == IB_QPT_UD) {
+				swsqe->opcode = FW_RI_SEND;
+				err = build_v2_ud_rdma_send(qhp, wqe, wr, &len16);
+			} else {
+				if (wr->opcode == IB_WR_SEND)
+					swsqe->opcode = FW_RI_SEND;
+				else
+					swsqe->opcode = FW_RI_SEND_WITH_INV;
+				err = build_v2_rdma_send(qhp, wqe, wr, &len16);
+			}
+			break;
+		case IB_WR_RDMA_WRITE_WITH_IMM:
+			if (unlikely(!qhp->rhp->rdev.lldi.write_w_imm_support)) {
+				err = -ENOSYS;
+				break;
+			}
+			fw_flags |= FW_RI_RDMA_WRITE_WITH_IMMEDIATE;
+			fallthrough; /* fallthrough */
+		case IB_WR_RDMA_WRITE:
+			fw_opcode = FW_RI_V2_RDMA_WRITE_WR;
+			swsqe->opcode = FW_RI_RDMA_WRITE;
+			err = build_v2_rdma_write(qhp, wqe, wr, &len16);
+			break;
+		case IB_WR_RDMA_READ:
+		case IB_WR_RDMA_READ_WITH_INV:
+			fw_opcode = FW_RI_V2_RDMA_READ_WR;
+			swsqe->opcode = FW_RI_READ_REQ;
+			if (wr->opcode == IB_WR_RDMA_READ_WITH_INV) {
+				chrd_invalidate_mr(qhp->rhp,
+						   wr->sg_list[0].lkey);
+				fw_flags |= FW_RI_RDMA_READ_INVALIDATE;
+			}
+			err = build_v2_rdma_read(qhp, wqe, wr, &len16);
+			if (err)
+				break;
+			swsqe->read_len = wr->sg_list[0].length; //Bhar: fix length for multi sgl (trivial)
+			if (!qhp->wq.sq.oldest_read) {
+				qhp->wq.sq.oldest_read = swsqe;
+				pr_debug("Oldest Read 0x%llx\n", (unsigned long long)qhp->wq.sq.oldest_read);
+			}
+			break;
+		case IB_WR_REG_MR: {
+			struct chrd_mr *mhp = to_chrd_mr(reg_wr(wr)->mr);
+
+			swsqe->opcode = FW_RI_FAST_REGISTER;
+			if (qhp->rhp->rdev.lldi.fr_nsmr_tpte_wr_support &&
+			    !mhp->attr.state && mhp->mpl_len <= 2) {
+				fw_opcode = FW_RI_FR_NSMR_TPTE_WR;
+				err = build_tpte_memreg(&wqe->fr_tpte, reg_wr(wr),
+							mhp, &len16);
+			} else {
+				fw_opcode = FW_RI_V2_FR_NSMR_WR;
+				err = build_v2_memreg(&qhp->wq.sq, wqe, reg_wr(wr),
+				       mhp, &len16,
+				       qhp->rhp->rdev.lldi.ulptx_memwrite_dsgl);
+			}
+			if (err)
+				break;
+			mhp->attr.state = 1;
+			break;
+		}
+		case IB_WR_LOCAL_INV:
+			if (wr->send_flags & IB_SEND_FENCE)
+				fw_flags |= FW_RI_LOCAL_FENCE_FLAG;
+			fw_opcode = FW_RI_V2_INV_LSTAG_WR;
+			swsqe->opcode = FW_RI_LOCAL_INV;
+			err = build_inv_stag(wqe, wr, &len16);
+			chrd_invalidate_mr(qhp->rhp, wr->ex.invalidate_rkey);
+			break;
+		default:
+			pr_debug("post of type=%d TBD!\n",
+			     wr->opcode);
+			err = -EINVAL;
+		}
+		if (err) {
+			*bad_wr = wr;
+			break;
+		}
+		swsqe->idx = qhp->wq.sq.pidx;
+		swsqe->complete = 0;
+		swsqe->signaled = (wr->send_flags & IB_SEND_SIGNALED) ||
+				  qhp->sq_sig_all;
+		swsqe->flushed = 0;
+		swsqe->wr_id = wr->wr_id;
+		if (chrd_wr_log) {
+			swsqe->sge_ts =
+				cxgb4_read_sge_timestamp(qhp->rhp->rdev.lldi.ports[0]);
+			swsqe->host_time = ktime_get();
+		}
+
+		init_wr_hdr(wqe, qhp->wq.sq.pidx, fw_opcode, fw_flags, len16);
+/*Todo: dump code start*/
+#if 0
+		//unsigned char *w = (void *)wqe;
+		//unsigned int i;
+		if (ibqp->qp_type == IB_QPT_GSI) {
+			pr_debug("wqe addr 0x%llx\n", (unsigned long long)wqe);
+			for (i = 0; i < len16; i++) {
+				pr_debug("0x%02x: %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x %02x%02x%02x%02x\n",
+					  i, w[0], w[1], w[2], w[3], w[4], w[5], w[6], w[7],
+					  w[8], w[9], w[10], w[11], w[12], w[13], w[14], w[15]);
+				w = w + 16;
+			}
+			pr_debug("cookie 0x%016llx pidx 0x%04x opcode 0x%x read_len %u in_use %u qid %u sig %u flag 0x%x\n",
+			     (unsigned long long)wr->wr_id, qhp->wq.sq.pidx,
+			     swsqe->opcode, swsqe->read_len, qhp->wq.sq.in_use, qhp->wq.sq.qid, swsqe->signaled, fw_flags);
+		}
+#endif
+/*Todo: dump code end */
+		wr = wr->next;
+		num_wrs--;
+		t4_sq_produce(&qhp->wq, len16);
+		pr_debug("qid %u in_use %u\n", qhp->wq.sq.qid, qhp->wq.sq.in_use);
+		idx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);
+	}
+	if (!qhp->rhp->rdev.status_page->db_off) {
+		t4_ring_sq_db(&qhp->wq, idx, wqe);
+		spin_unlock_irqrestore(&qhp->lock, flag);
+	} else {
+		spin_unlock_irqrestore(&qhp->lock, flag);
+		ring_kernel_sq_db(qhp, idx);
+	}
+	return err;
+}
+
+int chrd_roce_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
+			const struct ib_send_wr **bad_wr)
+{
+	int ret = 0;
+
+	switch (ibqp->qp_type) {
+	case IB_QPT_GSI:
+		ret = roce_post_send(ibqp, wr, bad_wr);
+		pr_debug("GSI POST SEND ret %d\n", ret);
+		break;
+	case IB_QPT_UD:
+	case IB_QPT_RC:
+		ret = roce_post_send(ibqp, wr, bad_wr);
+		break;
+	default:
+		WARN_ONCE(1, "unknown qp type %u\n", ibqp->qp_type);
+	}
+	return ret;
+}
+
+static void defer_srq_wr(struct t4_srq *srq, union t4_recv_wr *wqe,
+			 uint64_t wr_id, u8 len16)
+{
+	struct t4_srq_pending_wr *pwr = &srq->pending_wrs[srq->pending_pidx];
+
+	pr_debug("cidx %u pidx %u wq_pidx %u in_use %u ooo_count %u wr_id "
+		"0x%llx pending_cidx %u pending_pidx %u pending_in_use %u\n",
+		srq->cidx, srq->pidx, srq->wq_pidx,
+		srq->in_use, srq->ooo_count, (unsigned long long)wr_id,
+		srq->pending_cidx, srq->pending_pidx, srq->pending_in_use);
+	pwr->wr_id = wr_id;
+	pwr->len16 = len16;
+	memcpy(&pwr->wqe, wqe, len16*16);
+	t4_srq_produce_pending_wr(srq);
+}
+
+int chrd_post_srq_recv(struct ib_srq *ibsrq, const struct ib_recv_wr *wr,
+		       const struct ib_recv_wr **bad_wr)
+{
+	int err = 0;
+	struct chrd_srq *srq;
+	union t4_recv_wr *wqe, lwqe;
+	u32 num_wrs;
+	u8 len16 = 0;
+	u16 idx = 0;
+	unsigned long flag;
+
+	srq = to_chrd_srq(ibsrq);
+	spin_lock_irqsave(&srq->lock, flag);
+	num_wrs = t4_srq_avail(&srq->wq);
+	if (num_wrs == 0) {
+		spin_unlock_irqrestore(&srq->lock, flag);
+		return -ENOMEM;
+	}
+	while (wr) {
+		if (wr->num_sge > T4_MAX_RECV_SGE) {
+			err = -EINVAL;
 			*bad_wr = wr;
 			break;
 		}
@@ -2705,7 +3509,8 @@ static int rdma_init(struct c4iw_dev *rhp, struct c4iw_qp *qhp)
 	memset(wqe, 0, sizeof *wqe);
 	wqe->op_compl = cpu_to_be32(
 		V_FW_WR_OP(FW_RI_WR) |
-		F_FW_WR_COMPL);
+		F_FW_WR_COMPL |
+		V_FW_RI_WR_TRANSPORT_TYPE(FW_QP_TRANSPORT_TYPE_IWARP));
 	wqe->flowid_len16 = cpu_to_be32(
 		V_FW_WR_FLOWID(qhp->ep->hwtid) |
 		V_FW_WR_LEN16(DIV_ROUND_UP(sizeof *wqe, 16)));
@@ -2759,101 +3564,975 @@ static int rdma_init(struct c4iw_dev *rhp, struct c4iw_qp *qhp)
 
 	free_ird(rhp, qhp->attr.max_ird);
 out:
 	pr_debug("ret %d\n", ret);
 	return ret;
 }
 
 int chrd_modify_iw_rc_qp(struct chrd_qp *qhp, enum chrd_qp_attr_mask mask,
 		      struct chrd_common_qp_attributes *attrs, int internal)
 {
 	int ret = 0;
 	struct chrd_common_qp_attributes newattr = qhp->attr;
 	int disconnect = 0;
 	int terminate = 0;
 	int abort = 0;
 	int free = 0;
 	struct chrd_ep *ep = NULL;
 	struct chrd_dev *rhp = qhp->rhp;
+
+	pr_debug("qhp 0x%llx sqid 0x%x rqid 0x%x ep %p state %d -> %d\n",
+	     (unsigned long long)qhp, qhp->wq.sq.qid, qhp->wq.rq.qid, qhp->ep, qhp->attr.state,
+	     (mask & CHRD_QP_ATTR_NEXT_STATE) ? attrs->next_state : -1);
+
+	mutex_lock(&qhp->mutex);
+
+	/* Process attr changes if in IDLE */
+	if (mask & CHRD_QP_ATTR_VALID_MODIFY) {
+		if (qhp->attr.state != CHRD_QP_STATE_IDLE) {
+			ret = -EIO;
+			goto out;
+		}
+		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_READ)
+			newattr.enable_rdma_read = attrs->enable_rdma_read;
+		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_WRITE)
+			newattr.enable_rdma_write = attrs->enable_rdma_write;
+		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_BIND)
+			newattr.enable_bind = attrs->enable_bind;
+		if (mask & CHRD_QP_ATTR_MAX_ORD) {
+			if (attrs->max_ord > chrd_max_read_depth) {
+				ret = -EINVAL;
+				goto out;
+			}
+			newattr.max_ord = attrs->max_ord;
+		}
+		if (mask & CHRD_QP_ATTR_MAX_IRD) {
+			if (attrs->max_ird > cur_max_read_depth(rhp)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			newattr.max_ird = attrs->max_ird;
+		}
+		qhp->attr = newattr;
+	}
+
+	if (mask & CHRD_QP_ATTR_SQ_DB) {
+		ret = ring_kernel_sq_db(qhp, attrs->sq_db_inc);
+		goto out;
+	}
+	if (mask & CHRD_QP_ATTR_RQ_DB) {
+		ret = ring_kernel_rq_db(qhp, attrs->rq_db_inc);
+		goto out;
+	}
+
+	if (!(mask & CHRD_QP_ATTR_NEXT_STATE))
+		goto out;
+	if (qhp->attr.state == attrs->next_state)
+		goto out;
+
+	switch (qhp->attr.state) {
+	case CHRD_QP_STATE_IDLE:
+		switch (attrs->next_state) {
+		case CHRD_QP_STATE_RTS:
+			if (!(mask & CHRD_QP_ATTR_LLP_STREAM_HANDLE)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			if (!(mask & CHRD_QP_ATTR_MPA_ATTR)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			qhp->attr.mpa_attr = attrs->mpa_attr;
+			qhp->attr.llp_stream_handle = attrs->llp_stream_handle;
+			qhp->ep = qhp->attr.llp_stream_handle;
+			set_state(qhp, CHRD_QP_STATE_RTS);
+
+			/*
+			 * Ref the endpoint here and deref when we
+			 * disassociate the endpoint from the QP.  This
+			 * happens in CLOSING->IDLE transition or *->ERROR
+			 * transition.
+			 */
+			chrd_get_ep(&qhp->ep->com);
+			ret = rdma_init(rhp, qhp);
+			if (ret)
+				goto err;
+			break;
+		case CHRD_QP_STATE_ERROR:
+			set_state(qhp, CHRD_QP_STATE_ERROR);
+			flush_qp(qhp);
+			break;
+		default:
+			ret = -EINVAL;
+			goto out;
+		}
+		break;
+	case CHRD_QP_STATE_RTS:
+		switch (attrs->next_state) {
+		case CHRD_QP_STATE_CLOSING:
+			t4_set_wq_in_error(&qhp->wq, 0);
+			set_state(qhp, CHRD_QP_STATE_CLOSING);
+			ep = qhp->ep;
+			if (!internal) {
+				abort = 0;
+				disconnect = 1;
+				chrd_get_ep(&qhp->ep->com);
+			}
+			ret = rdma_fini(rhp, qhp, ep);
+			if (ret)
+				goto err;
+			break;
+		case CHRD_QP_STATE_TERMINATE:
+			t4_set_wq_in_error(&qhp->wq, 0);
+			set_state(qhp, CHRD_QP_STATE_TERMINATE);
+			qhp->attr.layer_etype = attrs->layer_etype;
+			qhp->attr.ecode = attrs->ecode;
+			ep = qhp->ep;
+			if (!internal) {
+				chrd_get_ep(&qhp->ep->com);
+				terminate = 1;
+				disconnect = 1;
+			} else {
+				terminate = qhp->attr.send_term;
+				ret = rdma_fini(rhp, qhp, ep);
+				if (ret)
+					goto err;
+			}
+			break;
+		case CHRD_QP_STATE_ERROR:
+			t4_set_wq_in_error(&qhp->wq, 0);
+			set_state(qhp, CHRD_QP_STATE_ERROR);
+			if (!internal) {
+				abort = 1;
+				disconnect = 1;
+				ep = qhp->ep;
+				chrd_get_ep(&qhp->ep->com);
+			}
+			goto err;
+			break;
+		default:
+			ret = -EINVAL;
+			goto out;
+		}
+		break;
+	case CHRD_QP_STATE_CLOSING:
+
+		/*
+		 * Allow kernel users to move to ERROR for qp draining.
+		 */
+		if (!internal && (qhp->ibqp.uobject || attrs->next_state !=
+				  CHRD_QP_STATE_ERROR)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		switch (attrs->next_state) {
+		case CHRD_QP_STATE_IDLE:
+			flush_qp(qhp);
+			set_state(qhp, CHRD_QP_STATE_IDLE);
+			qhp->attr.llp_stream_handle = NULL;
+			chrd_put_ep(&qhp->ep->com);
+			qhp->ep = NULL;
+			wake_up(&qhp->wait);
+			break;
+		case CHRD_QP_STATE_ERROR:
+			goto err;
+		default:
+			ret = -EINVAL;
+			goto err;
+		}
+		break;
+	case CHRD_QP_STATE_ERROR:
+		if (attrs->next_state != CHRD_QP_STATE_IDLE) {
+			ret = -EINVAL;
+			goto out;
+		}
+		if (!t4_sq_empty(&qhp->wq) || !t4_rq_empty(&qhp->wq)) {
+			ret = -EINVAL;
+			goto out;
+		}
+		set_state(qhp, CHRD_QP_STATE_IDLE);
+		break;
+	case CHRD_QP_STATE_TERMINATE:
+		if (!internal) {
+			ret = -EINVAL;
+			goto out;
+		}
+		goto err;
+		break;
+	default:
+		pr_err("%s in a bad state %d\n", __func__, qhp->attr.state);
+		ret = -EINVAL;
+		goto err;
+		break;
+	}
+	goto out;
+err:
+	pr_debug("disassociating ep %p qpid 0x%x\n", qhp->ep,
+	     qhp->wq.sq.qid);
+
+	/* disassociate the LLP connection */
+	qhp->attr.llp_stream_handle = NULL;
+	if (!ep)
+		ep = qhp->ep;
+	qhp->ep = NULL;
+	set_state(qhp, CHRD_QP_STATE_ERROR);
+	free = 1;
+	abort = 1;
+	flush_qp(qhp);
+	wake_up(&qhp->wait);
+out:
+	mutex_unlock(&qhp->mutex);
+
+	if (terminate)
+		post_terminate(qhp, NULL, internal ? GFP_ATOMIC : GFP_KERNEL);
+
+	/*
+	 * If disconnect is 1, then we need to initiate a disconnect
+	 * on the EP.  This can be a normal close (RTS->CLOSING) or
+	 * an abnormal close (RTS/CLOSING->ERROR).
+	 */
+	if (disconnect) {
+		chrd_ep_disconnect(ep, abort, internal ? GFP_ATOMIC :
+							 GFP_KERNEL);
+		chrd_put_ep(&ep->com);
+	}
+
+	/*
+	 * If free is 1, then we've disassociated the EP from the QP
+	 * and we need to dereference the EP.
+	 */
+	if (free)
+		chrd_put_ep(&ep->com);
+	pr_debug("exit state %d\n", qhp->attr.state);
+	return ret;
+}
+
+static int send_roce_flowc(struct chrd_qp *qhp)
+{
+	struct fw_flowc_wr *flowc;
+	//struct port_info *pi;		/*Todo: Check the Commented Code*/
+	struct sk_buff *skb;
+	int flowclen, flowclen16;
+	int nparams = 6; //Bhar: modify this later based on fw
+	u16 txq_idx;
+	u16 rss_qid;
+	u32 tx_chan;
+	int step;
+
+	tx_chan = cxgb4_port_chan(qhp->netdev);
+	step = qhp->rhp->rdev.lldi.nrxq / qhp->rhp->rdev.lldi.nchan;
+	//pi = (struct port_info *)netdev_priv(qhp->netdev);			/*Todo: Check the Commented Code */
+	//rss_qid = qhp->rhp->rdev.lldi.rxq_ids[pi->port_id * step];
+	rss_qid = qhp->rhp->rdev.lldi.rxq_ids[cxgb4_port_idx(qhp->netdev) * step];
+	step = qhp->rhp->rdev.lldi.ntxq / qhp->rhp->rdev.lldi.nchan;
+	txq_idx = cxgb4_port_idx(qhp->netdev) * step;
+
+	flowclen = offsetof(struct fw_flowc_wr, mnemval[nparams]);
+	flowclen16 = DIV_ROUND_UP(flowclen, 16);
+	flowclen = flowclen16 * 16;
+
+	skb = get_skb(NULL, flowclen, GFP_KERNEL);
+	if (WARN_ON(!skb))
+		return -ENOMEM;
+
+	flowc = (struct fw_flowc_wr *)__skb_put(skb, flowclen);
+	memset(flowc, 0, flowclen);
+
+	flowc->op_to_nparams = cpu_to_be32(V_FW_WR_OP(FW_FLOWC_WR) |
+					   V_FW_FLOWC_WR_NPARAMS(nparams));
+	flowc->flowid_len16 = cpu_to_be32(V_FW_WR_LEN16(flowclen16) |
+					  V_FW_WR_FLOWID(qhp->qp_type == IB_QPT_GSI ?
+					  qhp->rhp->rdev.gsi_ftid : qhp->roce_attr.hwtid));
+
+	flowc->mnemval[0].mnemonic = FW_FLOWC_MNEM_PFNVFN;
+	flowc->mnemval[0].val = cpu_to_be32(
+		V_FW_PFVF_CMD_PFN(qhp->rhp->rdev.lldi.pf));
+	flowc->mnemval[1].mnemonic = FW_FLOWC_MNEM_CH;
+	flowc->mnemval[1].val = cpu_to_be32(tx_chan); /* Bhar: finalize tx_chan for GSI*/
+	flowc->mnemval[2].mnemonic = FW_FLOWC_MNEM_PORT;
+	flowc->mnemval[2].val = cpu_to_be32(tx_chan); /* Bhar: finalize tx_chan for GSI*/
+	flowc->mnemval[3].mnemonic = FW_FLOWC_MNEM_ULP_MODE;
+	flowc->mnemval[3].val = cpu_to_be32(ULP_MODE_RDMA_V2);
+	flowc->mnemval[4].mnemonic = FW_FLOWC_MNEM_MSS;
+	flowc->mnemval[4].val = cpu_to_be32(1024); //BHar: remove hardcoded value
+	flowc->mnemval[5].mnemonic = FW_FLOWC_MNEM_IQID;
+	flowc->mnemval[5].val = cpu_to_be32(rss_qid);//Bhar: change the hardcoded value to rss_qid
+# if 0 
+/* Bhar: disable vlan for bringup */
+	u16 vlan = qhp->roce_attr.roce_ah.vlan_id;
+	if (vlan == CPL_L2T_VLAN_NONE)
+		nparams = 9;
+	else
+		nparams = 10;
+	if (nparams == 10) {
+		u16 pri;
+		pri = (vlan & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
+		flowc->mnemval[9].mnemonic = FW_FLOWC_MNEM_SCHEDCLASS;
+		flowc->mnemval[9].val = cpu_to_be32(pri);
+	} 
+#endif
+	set_wr_txq(skb, CPL_PRIORITY_DATA, txq_idx); /* Bhar: check txq_idx later */
+	return chrd_ofld_send(&qhp->rhp->rdev, skb);
+}
+
+u64 roce_select_ntuple(struct net_device *dev,
+		       struct chrd_qp *qhp)
+{
+	struct adapter *adap = netdev2adap(dev);
+	struct tp_params *tp = &adap->params.tp;
+	u64 ntuple = 0;
+	
+	/* Initialize each of the fields which we care about which are present
+	 * in the Compressed Filter Tuple.
+	 */
+//     if (tp->vlan_shift >= 0 && l2t->vlan != CPL_L2T_VLAN_NONE)			/*Todo: Check the Commented Code*/
+//             ntuple |= (u64)(F_FT_VLAN_VLD | l2t->vlan) << tp->vlan_shift;
+
+//     if (tp->port_shift >= 0)
+//             ntuple |= (u64)l2t->lport << tp->port_shift;				/*Todo: Check the Commented Code*/
+
+	if (tp->protocol_shift >= 0)
+		ntuple |= (u64)IPPROTO_UDP << tp->protocol_shift;
+//
+//     if (tp->vnic_shift >= 0 && (tp->ingress_config & F_VNIC)) {
+//             struct port_info *pi = (struct port_info *)netdev_priv(dev);
+//
+//             ntuple |= (u64)(V_FT_VNID_ID_VF(pi->vin) |				/*Todo: Check the Commented Code*/
+//                             V_FT_VNID_ID_PF(adap->pf) |
+//                             V_FT_VNID_ID_VLD(pi->vivld)) << tp->vnic_shift;
+//        }
+
+	ntuple |= (u64)(1) << tp->roce_shift;
+
+	return ntuple;
+}
+
+static int roce_act_open_req(struct chrd_qp *qhp)
+{
+	struct chrd_ah *ahp = &qhp->roce_attr.roce_ah;
+	struct cpl_t7_act_open_req6 *t7req6 = NULL;
+	struct cpl_t7_act_open_req *t7req = NULL;
+	struct net_device *netdev = qhp->netdev;
+	struct cpl_act_open_req6 *req6 = NULL;
+	struct cpl_act_open_req *req = NULL;
+	struct chrd_dev *dev = qhp->rhp;
+//	unsigned int mtu_idx;
+	unsigned int chip_ver;
+	int sizev4, wrlen, sizev6;
+	struct port_info *pi;
+	struct sk_buff *skb;
+	u32 smac_idx;
+	u16 rss_qid;
+	u32 tx_chan;
+	u64 params;
+	u64 opt0;
+	u32 opt2;
+	int ret, step;
+
+	struct sockaddr_in *la = (struct sockaddr_in *)&ahp->sgid_addr.saddr_in;
+	struct sockaddr_in *ra = (struct sockaddr_in *)&ahp->dgid_addr.saddr_in;
+	struct sockaddr_in6 *la6 = (struct sockaddr_in6 *)&ahp->sgid_addr.saddr_in6;
+	struct sockaddr_in6 *ra6 = (struct sockaddr_in6 *)&ahp->dgid_addr.saddr_in6;
+
+	chip_ver = CHELSIO_CHIP_VERSION(qhp->rhp->rdev.lldi.adapter_type);
+	switch (chip_ver) {
+	case CHELSIO_T7:
+	default:
+		sizev4 = sizeof(struct cpl_t7_act_open_req);
+		sizev6 = sizeof(struct cpl_t7_act_open_req6);
+	}
+
+	wrlen = (ahp->net_type == RDMA_NETWORK_IPV4) ? roundup(sizev4, 16) : roundup(sizev6, 16);
+	qhp->roce_attr.atid = cxgb4_alloc_atid(dev->rdev.lldi.tids, qhp);
+	qhp->roce_attr.hwtid = -1;					/*Todo: hardcoded value*/
+
+	//Bhar: add atid to xarray for debug and rpl processing`
+//     err = xa_insert_irq(&ep->com.dev->atids, ep->atid, ep, GFP_KERNEL);		/*Todo: Check the Commented Code*/
+//     if (err)
+//             goto fail2a;
+
+	pr_debug("qhp 0x%llx roce_attr 0x%llx ahp 0x%llx atid %u\n",
+		 (unsigned long long)qhp, (unsigned long long)&qhp->roce_attr,
+		 (unsigned long long)ahp, qhp->roce_attr.atid);
+
+	pr_debug("ahp 0x%llx sport %u dport %u smac %pM dmac %pM "
+		 "dest_ip %pI4, src_ip %pI4\n", (unsigned long long)ahp,
+		 ahp->src_port, ahp->dst_port, ahp->smac, ahp->dmac,
+		 &ahp->dest_ip_addr[3], &ahp->local_ip_addr[3]);
+
+	skb = get_skb(NULL, wrlen, GFP_KERNEL);
+	if (!skb) {
+		pr_err("%s - failed to alloc skb\n", __func__);
+		return -ENOMEM;
+	}
+	set_wr_txq(skb, CPL_PRIORITY_SETUP,
+		   NCHAN + cxgb4_port_idx(netdev)); //Bhar: set ctrlq_idx appropriately
+
+	//Bhar: set bellow fields appropriately based on port
+	tx_chan = cxgb4_port_chan(netdev);
+	step = dev->rdev.lldi.nrxq / dev->rdev.lldi.nchan;
+	pi = (struct port_info *)netdev_priv(netdev);
+	rss_qid = dev->rdev.lldi.rxq_ids[pi->port_id * step];
+	smac_idx = pi->smt_idx;
+
+	opt0 = F_TCAM_BYPASS |
+	       F_NON_OFFLOAD |
+	       F_KEEP_ALIVE |
+	       F_DELACK |
+//	       V_MSS_IDX(mtu_idx) |	/*Todo: Check the Commented Code*/
+	       V_L2T_IDX(0) | //Bhar: check if l2t_idx is needed
+	       V_TX_CHAN(tx_chan) |
+	       V_SMAC_SEL(smac_idx) |
+	     //  V_DSCP(ep->tos >> 2) |		/*Todo: Check the Commented Code*/
+	       V_ULP_MODE(ULP_MODE_RDMA_V2);
+
+	opt2 = V_TX_QUEUE(dev->rdev.lldi.tx_modq[tx_chan]) |
+	       V_RX_CHANNEL(0) |
+	       V_CCTRL_ECN(0) |
+	       F_RSS_QUEUE_VALID | V_RSS_QUEUE(rss_qid);
+
+       params = roce_select_ntuple(netdev, qhp); // may be needed to set roce filter bit
+
+	if (ahp->net_type == RDMA_NETWORK_IPV6) {
+		ret = cxgb4_clip_get(qhp->rhp->rdev.lldi.ports[0],
+				(const u32 *)&la6->sin6_addr.s6_addr, 1);
+		if (ret)
+			return ret;
+	}
+
+//	t4_set_arp_err_handler(skb, ep, act_open_req_arp_failure);	/*Todo: Check the Commented Code*/
+
+	if (ahp->net_type == RDMA_NETWORK_IPV4) {
+		switch (chip_ver) {
+		case CHELSIO_T7:
+		default:
+			t7req = (struct cpl_t7_act_open_req *)__skb_put(skb, wrlen);
+			INIT_TP_WR(t7req, 0);
+			req = (struct cpl_act_open_req *)t7req;
+			break;
+		}
+
+		OPCODE_TID(req) = cpu_to_be32(
+                       MK_OPCODE_TID(CPL_ACT_OPEN_REQ,
+                       ((rss_qid<<14)|qhp->roce_attr.atid)));
+		req->local_port = cpu_to_be16(qhp->wq.sq.qid >> 8);
+		req->peer_port = cpu_to_be16((qhp->wq.sq.qid & 0xff) << 8);
+		//req->local_port = cpu_to_be16(ahp->src_port);			/*Todo: Check the Commented Code */
+		//req->peer_port = cpu_to_be16(ahp->dst_port);
+		req->local_ip = la->sin_addr.s_addr;
+		req->peer_ip = ra->sin_addr.s_addr;
+		req->opt0 = cpu_to_be64(opt0);
+		opt2 |= F_T5_OPT_2_VALID;
+		t7req->params = cpu_to_be64(V_T7_FILTER_TUPLE(params));
+		t7req->iss = cpu_to_be32(0);
+		t7req->opt2 = cpu_to_be32(opt2);
+		t7req->rsvd2 = 0;
+		t7req->opt3 = 0;
+		pr_debug("ahp 0x%llx sport %u dport %u smac %pM dmac %pM "
+			 "dest_ip %pI4, src_ip %pI4\n", (unsigned long long)ahp,
+			 req->local_port, req->peer_port, ahp->smac, ahp->dmac,
+			 &req->peer_ip, &req->local_ip);
+
+	} else {
+		switch (chip_ver) {
+		case CHELSIO_T7:
+		default:
+			t7req6 = (struct cpl_t7_act_open_req6 *) skb_put(skb,
+			                wrlen);
+			INIT_TP_WR(t7req6, 0);
+			req6 = (struct cpl_act_open_req6 *)t7req6;
+			break;
+		}
+
+		OPCODE_TID(req6) = cpu_to_be32(MK_OPCODE_TID(CPL_ACT_OPEN_REQ6,
+                                       ((rss_qid<<14)|qhp->roce_attr.atid)));
+		req->local_port = cpu_to_be16(qhp->wq.sq.qid >> 8);
+		req->peer_port = cpu_to_be16((qhp->wq.sq.qid & 0xff) << 8);
+		//req->local_port = cpu_to_be16(ahp->src_port);			/*Todo: Check the Commented Code */
+		//req->peer_port = cpu_to_be16(ahp->dst_port);
+		req6->local_ip_hi = *((__be64 *)(la6->sin6_addr.s6_addr));
+		req6->local_ip_lo = *((__be64 *)(la6->sin6_addr.s6_addr + 8));
+		req6->peer_ip_hi = *((__be64 *)(ra6->sin6_addr.s6_addr));
+		req6->peer_ip_lo = *((__be64 *)(ra6->sin6_addr.s6_addr + 8));
+		req6->opt0 = cpu_to_be64(opt0);
+
+		opt2 |= F_T5_OPT_2_VALID;
+		t7req6->params = cpu_to_be64(V_T7_FILTER_TUPLE(params));
+		t7req6->iss = cpu_to_be32(0);
+		t7req6->opt2 = cpu_to_be32(opt2);
+		t7req6->rsvd2 = 0;
+		t7req6->opt3 = 0;
+	}
+
+	chrd_ref_send_wait(&qhp->rhp->rdev, skb, qhp->wr_waitp, 0, qhp->wq.sq.qid, __func__);
+
+//	if (ret && ahp->net_type == RDMA_NETWORK_IPV6)
+//	cxgb4_clip_release(netdev, (const u32 *)&la6->sin6_addr.s6_addr, 1);	/*Todo: Check the Commented Code */
+	return ret;
+}
+
+static int rdma_roce_init(struct chrd_dev *rhp, struct chrd_qp *qhp)
+{
+	struct chrd_rc_hdr *hdrp;
+	struct fw_ri_immd *immdp;
+	struct fw_ri_wr *wqe;
+	struct chrd_ah *ahp;
+	struct sk_buff *skb;
+	int ret, wrlen;
+	u32 immdlen;
+	u32 tid;
+
+	if (qhp->qp_type == IB_QPT_GSI) {
+		tid = qhp->rhp->rdev.gsi_ftid;
+	} else if (qhp->qp_type == IB_QPT_RC) {
+		tid = qhp->roce_attr.hwtid;
+	} else {
+		pr_debug("Unwanted QP type!!!!!!\n");
+		BUG_ON(1);
+	}
+
+	if (qhp->qp_type == IB_QPT_GSI)
+		wrlen = sizeof *wqe;
+	else
+		wrlen = sizeof *wqe + roundup(sizeof(struct fw_ri_immd) + sizeof(struct chrd_rc_hdr), 16); //Bhar: 54 is assuming ipv4 without vlan, modify this after bringup
+
+	pr_debug("qhp 0x%llx qid 0x%x tid %d wrlen %d wqesz %lu rimmsz %lu\n",
+		 (unsigned long long)qhp, qhp->wq.sq.qid, tid, wrlen,
+		 sizeof(*wqe), sizeof(struct fw_ri_immd));
+
+	if (wrlen%16)
+		roundup(wrlen, 16);
+
+	skb = alloc_skb(wrlen, GFP_KERNEL | __GFP_NOFAIL);
+	if (!skb) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	ret = alloc_ird(rhp, qhp->attr.max_ird);
+	if (ret) {
+		qhp->attr.max_ird = 0;
+		kfree_skb(skb);
+		goto out;
+	}
+	set_wr_txq(skb, CPL_PRIORITY_DATA, 0);/* Bhar: check txq_idx later */
+
+	wqe = (struct fw_ri_wr *)__skb_put(skb, wrlen);
+	memset(wqe, 0, wrlen);
+	if ((qhp->qp_type == IB_QPT_GSI) || (qhp->qp_type == IB_QPT_UD))
+		wqe->op_compl = cpu_to_be32(V_FW_WR_OP(FW_RI_WR) |
+			F_FW_WR_COMPL |
+			V_FW_RI_WR_TRANSPORT_TYPE(FW_QP_TRANSPORT_TYPE_ROCEV2_UD));
+	else
+		wqe->op_compl = cpu_to_be32(V_FW_WR_OP(FW_RI_WR) |
+			F_FW_WR_COMPL |
+			V_FW_RI_WR_TRANSPORT_TYPE(FW_QP_TRANSPORT_TYPE_ROCEV2_RC));
+	wqe->flowid_len16 = cpu_to_be32(
+		V_FW_WR_FLOWID(tid) |
+		V_FW_WR_LEN16(DIV_ROUND_UP(wrlen, 16)));
+	wqe->cookie = (uintptr_t)qhp->wr_waitp;
+
+	wqe->u.rocev2_init.type = FW_RI_TYPE_INIT;
+
+	wqe->u.rocev2_init.qp_caps = FW_RI_QP_RDMA_READ_ENABLE |
+			    FW_RI_QP_RDMA_WRITE_ENABLE |
+			    FW_RI_QP_BIND_ENABLE;
+	if (!qhp->ibqp.uobject)
+		wqe->u.rocev2_init.qp_caps |= FW_RI_QP_FAST_REGISTER_ENABLE |
+				     FW_RI_QP_STAG0_ENABLE;
+	wqe->u.rocev2_init.nrqe = cpu_to_be16(t4_rqes_posted(&qhp->wq));
+	wqe->u.rocev2_init.pdid = cpu_to_be32(qhp->attr.pd);
+	wqe->u.rocev2_init.qpid = cpu_to_be32(qhp->wq.sq.qid);
+	wqe->u.rocev2_init.sq_eqid = cpu_to_be32(qhp->wq.sq.qid);
+	if (qhp->srq)
+		wqe->u.rocev2_init.rq_eqid = cpu_to_be32(FW_RI_INIT_RQEQID_SRQ |
+						  qhp->srq->idx);
+	else {
+		wqe->u.rocev2_init.rq_eqid = cpu_to_be32(qhp->wq.rq.qid);
+		wqe->u.rocev2_init.hwrqsize = cpu_to_be32(qhp->wq.rq.rqt_size);
+		wqe->u.rocev2_init.hwrqaddr = cpu_to_be32(qhp->wq.rq.rqt_hwaddr -
+						   rhp->rdev.lldi.vr->rq.start);
+	}
+	wqe->u.rocev2_init.scqid = cpu_to_be32(qhp->attr.scq);
+	wqe->u.rocev2_init.rcqid = cpu_to_be32(qhp->attr.rcq);
+	wqe->u.rocev2_init.ord_max = cpu_to_be32(qhp->attr.max_ord);
+	wqe->u.rocev2_init.ird_max = cpu_to_be32(qhp->attr.max_ird);
+	wqe->u.rocev2_init.psn_pkd = cpu_to_be32(qhp->roce_attr.gsi_attr.psn_nxt);
+	wqe->u.rocev2_init.epsn_pkd = cpu_to_be32(qhp->roce_attr.gsi_attr.epsn);
+//	wqe->u.rocev2_init.q_key = cpu_to_be32(qhp->roce_attr.q_key);	/*Todo: Check the Commented Code */
+	wqe->u.rocev2_init.q_key = cpu_to_be32(0x80010000);		/*Todo: hardcoded value */
+//	wqe->u.rocev2_init.p_key = cpu_to_be16(qhp->roce_attr.roce_ah.p_key);	*Todo: Check the Commented Code */
+	wqe->u.rocev2_init.p_key = cpu_to_be16(0xFFFF);			/*Todo: hardcoded value */
+	wqe->u.rocev2_init.r = 0;
+	pr_debug("ird %u ord %u psn_pkd %u epsn_pkd %u\n", qhp->attr.max_ord, qhp->attr.max_ird, qhp->roce_attr.gsi_attr.psn_nxt, qhp->roce_attr.gsi_attr.epsn);
+	if (unlikely(qhp->qp_type == IB_QPT_GSI)) {
+		wqe->u.rocev2_init.pkthdrsize = 0;
+	} else {
+		ahp = &qhp->roce_attr.roce_ah;
+		immdlen = sizeof(struct cpl_tx_tnl_lso) + sizeof(struct cpl_tx_pkt_core) + sizeof(struct chrd_rc_hdr);
+		roce_fill_tnl_lso(qhp, (struct cpl_tx_tnl_lso *)wqe->u.rocev2_init.tnl_lso, 0);
+		immdp = (struct fw_ri_immd *)(wqe->u.rocev2_init.tnl_lso + sizeof(struct cpl_tx_tnl_lso) + sizeof(struct cpl_tx_pkt_core));
+		//hdrp = (struct chrd_rc_hdr *)(immdp + 1);			/*Todo: Check the Commented Code */
+		hdrp = (struct chrd_rc_hdr *)(wqe->u.rocev2_init.tnl_lso + sizeof(struct cpl_tx_tnl_lso) + sizeof(struct cpl_tx_pkt_core) + sizeof(struct fw_ri_immd));
+		pr_debug("ahp 0x%llx tnl_lso 0x%llx immdp 0x%llx hdrp 0x%llx\n",
+			 (unsigned long long)ahp, (unsigned long long)wqe->u.rocev2_init.tnl_lso,
+			 (unsigned long long)immdp,
+			 (unsigned long long)hdrp);
+		immdp->op = FW_RI_DATA_IMMD;
+		immdp->r1 = 0;
+		immdp->r2 = 0;
+		immdp->immdlen = cpu_to_be32(sizeof(struct chrd_rc_hdr));//Bhar: check this while bringup
+
+		/* build headers here  */
+		/* ETH header */
+		memcpy(hdrp->eth.ethh.h_source, ahp->smac, ETH_ALEN);
+		memcpy(hdrp->eth.ethh.h_dest, ahp->dmac, ETH_ALEN);
+		//hdrp->eth.ethh.h_proto = htons(is_ipv4 ? ETH_P_IP : ETH_P_IPV6);		/*Todo: Check  the Commented Code*/
+		hdrp->eth.ethh.h_proto = cpu_to_be16(ETH_P_IP); //Bhar: modify this to above later
+
+		//bhar: check for ipv4 or ipv6 from gsi attr later
+		/* IP header */
+		hdrp->iph.ip4h.version = 4;
+		hdrp->iph.ip4h.frag_off = 0x40; // set fragment off
+		hdrp->iph.ip4h.protocol = IPPROTO_UDP;
+		hdrp->iph.ip4h.ihl = 0x5;		/*Todo: hardcoded value */
+		hdrp->iph.ip4h.ttl = 0x40;		/*Todo: hardcoded value */
+		memcpy(&hdrp->iph.ip4h.daddr, &ahp->dest_ip_addr[3], 4);
+		memcpy(&hdrp->iph.ip4h.saddr, &ahp->local_ip_addr[3], 4);
+		hdrp->iph.ip4h.check = (u16)(~ip_fast_csum((u8 *)&hdrp->iph.ip4h, hdrp->iph.ip4h.ihl));
+
+		/* UDP header */
+		hdrp->udph.source = cpu_to_be16(ahp->src_port);
+		hdrp->udph.dest = cpu_to_be16(ahp->dst_port);
+
+		/* BTH header */
+		hdrp->bth.flags = 0;
+		//hdrp->bth.pkey = cpu_to_be16(ahp->p_key);	/*Todo: Check the Commented Code */
+		hdrp->bth.pkey = cpu_to_be16(0xFFFF);		/*Todo: hardcoded value */
+		hdrp->bth.destination_qpn = cpu_to_be32(ahp->dest_qp);
+
+		wqe->u.rocev2_init.pkthdrsize = roundup(sizeof(struct chrd_rc_hdr), 16);//Bhar: check this while bringup
+		/* Init WR has 16B word boundary.may need to initialize last 
+			10B(64 - 54) with 0 */
+	}
+
+	if (unlikely((qhp->qp_type == IB_QPT_GSI) || (qhp->qp_type == IB_QPT_UD))) {
+		ret = chrd_ref_send_wait(&rhp->rdev, skb, qhp->wr_waitp,
+					 qhp->rhp->rdev.gsi_ftid,
+					 qhp->wq.sq.qid, __func__);
+	} else {
+		ret = chrd_ref_send_wait(&rhp->rdev, skb, qhp->wr_waitp,
+					 qhp->roce_attr.hwtid,
+					 qhp->wq.sq.qid, __func__);
+	}
+
+	if (!ret)
+		goto out;
+
+	free_ird(rhp, qhp->attr.max_ird);
+out:
+	pr_debug("ret %d\n", ret);
+	return ret;
+}
+
+int chrd_modify_roce_qp(struct chrd_qp *qhp, int attr_mask,
+			struct ib_qp_attr *attr, int internal)
+{
+	struct chrd_roce_qp_attributes new_roce_attr = qhp->roce_attr;
+	struct chrd_common_qp_attributes newattr = qhp->attr;
+	struct chrd_common_qp_attributes attrs;
+	const struct ib_gid_attr *sgid_attr;
+	struct ib_qp *ibqp = &qhp->ibqp;
+	struct chrd_dev *rhp = qhp->rhp;
+	enum chrd_qp_attr_mask mask = 0;
+	enum ib_qp_state cur_state;
+	enum ib_qp_state new_state;
+	struct chrd_ah *ahp;
+	int abort = 0;
+	int free = 0;
+	int ret = 0;
+	u16 vlan_id;
+
+	memset(&attrs, 0, sizeof attrs);
+	cur_state = attr_mask & IB_QP_CUR_STATE ? attr->cur_qp_state : qhp->attr.state;
+	new_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;
+
+	attrs.next_state = chrd_convert_v2_state(attr->qp_state);
+	attrs.enable_rdma_read = (attr->qp_access_flags &
+			       IB_ACCESS_REMOTE_READ) ?  1 : 0;
+	attrs.enable_rdma_write = (attr->qp_access_flags &
+				IB_ACCESS_REMOTE_WRITE) ? 1 : 0;
+	attrs.enable_bind = (attr->qp_access_flags & IB_ACCESS_MW_BIND) ? 1 : 0;
 
-	pr_debug("qhp %p sqid 0x%x rqid 0x%x ep %p state %d -> %d\n",
-	     qhp, qhp->wq.sq.qid, qhp->wq.rq.qid, qhp->ep, qhp->attr.state,
-	     (mask & CHRD_QP_ATTR_NEXT_STATE) ? attrs->next_state : -1);
+	mask |= (attr_mask & IB_QP_STATE) ? CHRD_QP_ATTR_NEXT_STATE : 0;
+	mask |= (attr_mask & IB_QP_ACCESS_FLAGS) ?
+			(CHRD_QP_ATTR_ENABLE_RDMA_READ |
+			 CHRD_QP_ATTR_ENABLE_RDMA_WRITE |
+			 CHRD_QP_ATTR_ENABLE_RDMA_BIND) : 0;
+
+	pr_debug("qhp 0x%llx roce_attr 0x%llx new_roce_attr 0x%llx ahp 0x%llx sqid 0x%x rqid 0x%x mask 0x%X "
+		  "state %d -> %d, attr_mask 0x%X %d > %d\n",
+		  (unsigned long long)qhp, (unsigned long long)&qhp->roce_attr, (unsigned long long)&new_roce_attr,
+		  (unsigned long long)&qhp->roce_attr.roce_ah,
+		  qhp->wq.sq.qid, qhp->wq.rq.qid, mask, qhp->attr.state,
+		  (mask & CHRD_QP_ATTR_NEXT_STATE) ? attrs.next_state : -1, attr_mask, cur_state, new_state);
+	if (!ib_modify_qp_is_ok(cur_state, new_state,
+				qhp->ibqp.qp_type, attr_mask)) {
+		pr_err("%s Invalid modify QP parameters\n", __func__);
+		ret = -EINVAL;
+		goto out;
+	}
 
 	mutex_lock(&qhp->mutex);
 
 	/* Process attr changes if in IDLE */
 	if (mask & CHRD_QP_ATTR_VALID_MODIFY) {
-		if (qhp->attr.state != CHRD_QP_STATE_IDLE) {
-			ret = -EIO;
-			goto out;
+		if (attr_mask & IB_QP_STATE && qhp->attr.state != CHRD_QP_V2_STATE_RESET) {
+			pr_debug("check here!\n");
+		//      ret = -EIO;								/*Todo: Check the Commented Code*/
+		//      goto out;
 		}
-		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_READ)
-			newattr.enable_rdma_read = attrs->enable_rdma_read;
-		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_WRITE)
-			newattr.enable_rdma_write = attrs->enable_rdma_write;
-		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_BIND)
-			newattr.enable_bind = attrs->enable_bind;
-		if (mask & CHRD_QP_ATTR_MAX_ORD) {
-			if (attrs->max_ord > chrd_max_read_depth) {
+		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_READ)
+			newattr.enable_rdma_read = attrs.enable_rdma_read;
+		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_WRITE)
+			newattr.enable_rdma_write = attrs.enable_rdma_write;
+		if (mask & CHRD_QP_ATTR_ENABLE_RDMA_BIND)
+			newattr.enable_bind = attrs.enable_bind;
+		if (mask & CHRD_QP_ATTR_MAX_ORD) {
+			if (attrs.max_ord > chrd_max_read_depth) {
 				ret = -EINVAL;
 				goto out;
 			}
-			newattr.max_ord = attrs->max_ord;
+			newattr.max_ord = attrs.max_ord;
 		}
 		if (mask & CHRD_QP_ATTR_MAX_IRD) {
-			if (attrs->max_ird > cur_max_read_depth(rhp)) {
+			if (attrs.max_ird > cur_max_read_depth(rhp)) {
 				ret = -EINVAL;
 				goto out;
 			}
-			newattr.max_ird = attrs->max_ird;
+			newattr.max_ird = attrs.max_ird;
 		}
 		qhp->attr = newattr;
 	}
 
+#if 0
+Bhar: This may not be needed for T7 roce, Revisit later.
+	attrs.sq_db_inc = attr->sq_psn;
+	attrs.rq_db_inc = attr->rq_psn;
+	mask |= (attr_mask & IB_QP_SQ_PSN) ? CHRD_QP_ATTR_SQ_DB : 0;
+	mask |= (attr_mask & IB_QP_RQ_PSN) ? CHRD_QP_ATTR_RQ_DB : 0;
+	if (!is_t4(to_chrd_qp(ibqp)->rhp->rdev.lldi.adapter_type) &&
+	    (mask & (CHRD_QP_ATTR_SQ_DB|CHRD_QP_ATTR_RQ_DB)))
+		return -EINVAL;
+
 	if (mask & CHRD_QP_ATTR_SQ_DB) {
 		ret = ring_kernel_sq_db(qhp, attrs->sq_db_inc);
 		goto out;
 	}
 	if (mask & CHRD_QP_ATTR_RQ_DB) {
 		ret = ring_kernel_rq_db(qhp, attrs->rq_db_inc);
 		goto out;
 	}
+#endif
+	if (attr_mask & ~IB_QP_ATTR_STANDARD_BITS) {
+		return -EOPNOTSUPP;
+		goto out;
+	}
+	if (attr_mask & IB_QP_PKEY_INDEX) {
+		new_roce_attr.roce_ah.p_key = attr->pkey_index; //Bhar: use chrd_query_pkey() if needed
+		pr_debug("pkey_index %u\n", attr->pkey_index);
+	}
+	if (attr_mask & IB_QP_DEST_QPN) {
+		new_roce_attr.roce_ah.dest_qp = attr->dest_qp_num;
+		pr_debug("attr->dest_qp_num %u\n", attr->dest_qp_num);
+	}
+	if (attr_mask & IB_QP_QKEY) {
+		new_roce_attr.q_key = attr->qkey;
+		pr_debug("attr->qkey %u\n", attr->qkey);
+	}
+	if (attr_mask & IB_QP_PATH_MTU) {
+		new_roce_attr.gsi_attr.snd_mss = ib_mtu_enum_to_int(attr->path_mtu); //Bhar: validate this later
+		pr_debug("snd_mss %u path_mtu %u\n", new_roce_attr.gsi_attr.snd_mss, attr->path_mtu);
+	}
+	if (attr_mask & IB_QP_SQ_PSN) {
+		new_roce_attr.gsi_attr.psn_nxt = attr->sq_psn & CHRD_ROCE_PSN_MASK;
+		new_roce_attr.gsi_attr.lsn =  0xffff;
+		new_roce_attr.gsi_attr.psn_una = attr->sq_psn & CHRD_ROCE_PSN_MASK;
+		new_roce_attr.gsi_attr.psn_max = attr->sq_psn & CHRD_ROCE_PSN_MASK;
+		pr_debug("attr->sq_psn %u\n", attr->sq_psn);
+	}
+	if (attr_mask & IB_QP_RQ_PSN) {
+        	new_roce_attr.gsi_attr.epsn = attr->rq_psn & CHRD_ROCE_PSN_MASK;
+		pr_debug("attr->rq_psn %u\n", attr->rq_psn);
+	}
+	if (attr_mask & IB_QP_RNR_RETRY) {
+        	new_roce_attr.gsi_attr.rnr_nak_thresh = attr->rnr_retry;
+		pr_debug("attr->attr->rnr_retry %u\n", attr->rnr_retry);
+	}
+	if (attr_mask & IB_QP_RETRY_CNT) {
+        	new_roce_attr.gsi_attr.rexmit_thresh = attr->retry_cnt;
+		pr_debug("attr->retry_cnt %u\n", attr->retry_cnt);
+	}
+	if (attr_mask & IB_QP_AV) {
+		ahp = &new_roce_attr.roce_ah;
+		//ahp_old = to_chrd_ah();	/*Todo: Check the Commented Code*/
+		pr_debug("Mask AV qhp 0x%llx roce_attr 0x%llx ahp 0x%llx\n",
+			 (unsigned long long)qhp, (unsigned long long)&qhp->roce_attr, (unsigned long long)ahp);
+		vlan_id = VLAN_N_VID;
+		ahp->attr = attr->ah_attr;
+		ahp->dst_port = CHRD_ROCE_PORT;
+
+		if (attr->ah_attr.ah_flags & IB_AH_GRH) {
+			new_roce_attr.gsi_attr.ttl = attr->ah_attr.grh.hop_limit;
+			new_roce_attr.gsi_attr.flow_label = attr->ah_attr.grh.flow_label;
+			new_roce_attr.gsi_attr.tos = attr->ah_attr.grh.traffic_class;
+			ahp->src_port = rdma_get_udp_sport(new_roce_attr.gsi_attr.flow_label,
+						   ibqp->qp_num, ahp->dest_qp);
+			pr_debug("NA for v2, GRH set, src_port %u dst_port %u\n",
+ 				 ahp->src_port, ahp->dst_port);
+		} else {
+			ahp->src_port = 0xd000;
+			pr_debug("GRH not set, src_port %u dst_port %u\n",
+                                ahp->src_port, ahp->dst_port);
+		}
+
+		sgid_attr = attr->ah_attr.grh.sgid_attr;
+		ahp->net_type = rdma_gid_attr_network_type(sgid_attr);
+		memcpy(ahp->dmac, attr->ah_attr.roce.dmac, ETH_ALEN);
+		ret = rdma_read_gid_l2_fields(sgid_attr, &vlan_id, ahp->smac);
+        	if (ret)
+                	return ret;
+		pr_debug("smac %pM dmac %pM\n", ahp->smac, ahp->dmac);
+
+	        if (vlan_id >= VLAN_N_VID)
+        	        vlan_id = 0;
+        	if (vlan_id < VLAN_N_VID) {
+                	new_roce_attr.roce_ah.insert_vlan_tag = true;
+			new_roce_attr.roce_ah.vlan_id = vlan_id; //Bhar: Recheck
+        	} else {
+                	new_roce_attr.roce_ah.insert_vlan_tag = false;
+        	}
+
+        	rdma_gid2ip((struct sockaddr *)&ahp->sgid_addr, &sgid_attr->gid);
+        	rdma_gid2ip((struct sockaddr *)&ahp->dgid_addr, &attr->ah_attr.grh.dgid);
+        	if (ahp->net_type == RDMA_NETWORK_IPV6) {
+                	__be32 *daddr =
+                        	ahp->dgid_addr.saddr_in6.sin6_addr.in6_u.u6_addr32;
+                	__be32 *saddr =
+                        	ahp->sgid_addr.saddr_in6.sin6_addr.in6_u.u6_addr32;
+
+                	chrd_copy_ip_ntohl(&ahp->dest_ip_addr[0], daddr);
+                	chrd_copy_ip_ntohl(&ahp->local_ip_addr[0], saddr);
+
+          		ahp->ipv4 = false;
+
+			/* Bhar: Fields should be in BE or network order */
+                	ahp->dst = find_route6(rhp, (__u8 *)saddr, (__u8 *)daddr,
+					       ahp->src_port,
+					       ahp->dst_port,
+					       0, 0); /* Bhar: fix sin6_scope_id*/
+        	} else if (ahp->net_type == RDMA_NETWORK_IPV4) {
+                	__be32 saddr = ahp->sgid_addr.saddr_in.sin_addr.s_addr;
+                	__be32 daddr = ahp->dgid_addr.saddr_in.sin_addr.s_addr;
+
+                	ahp->ipv4 = true;
+	                ahp->dest_ip_addr[0] = 0;
+        	        ahp->dest_ip_addr[1] = 0;
+                	ahp->dest_ip_addr[2] = 0;
+			//ahp->dest_ip_addr[3] = ntohl(daddr);			/*Todo: Check the Commented Code*/
+                	ahp->dest_ip_addr[3] = daddr;
+
+                	ahp->local_ip_addr[0] = 0;
+                	ahp->local_ip_addr[1] = 0;
+                	ahp->local_ip_addr[2] = 0;
+			//ahp->local_ip_addr[3] = ntohl(saddr);			/*Todo: Check the Commented Code*/
+                	ahp->local_ip_addr[3] = saddr;
+
+			/* Bhar: Fields should be in BE or network order */
+                	ahp->dst = find_route(rhp, saddr, daddr, ahp->src_port,
+					      ahp->dst_port, 0);
+        	}
+		pr_debug("ahp 0x%llx sport %u dport %u smac %pM dmac %pM"
+			 " dest_ip %pI4, src_ip %pI4\n", (unsigned long long)ahp,
+			 ahp->src_port, ahp->dst_port, ahp->smac, ahp->dmac,
+			 &ahp->dest_ip_addr[3], &ahp->local_ip_addr[3]);
+
+	}
+
+	if (attr_mask & IB_QP_MAX_QP_RD_ATOMIC) {
+       	if (attr->max_rd_atomic)
+                	new_roce_attr.ord_size = attr->max_rd_atomic;
+	}
+
+	if (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC) {
+        	if (attr->max_dest_rd_atomic)
+                	new_roce_attr.ird_size = attr->max_dest_rd_atomic;
+	}
+	qhp->roce_attr = new_roce_attr;
 
 	if (!(mask & CHRD_QP_ATTR_NEXT_STATE))
 		goto out;
-	if (qhp->attr.state == attrs->next_state)
+	if (qhp->attr.state == attrs.next_state)
+		goto out;
+
+//	if (!ib_modify_qp_is_ok(qhp->attr.state, attrs.next_state,	/*Todo: Check the Commented Code*/
+//				qhp->ibqp.qp_type, attr_mask)) {
+	if (!ib_modify_qp_is_ok(cur_state, new_state,
+				qhp->ibqp.qp_type, attr_mask)) {
+		pr_err("%s Invalid modify QP parameters\n", __func__);
+		ret = -EINVAL;
 		goto out;
+	}
 
 	switch (qhp->attr.state) {
-	case CHRD_QP_STATE_IDLE:
-		switch (attrs->next_state) {
-		case CHRD_QP_STATE_RTS:
-			if (!(mask & CHRD_QP_ATTR_LLP_STREAM_HANDLE)) {
-				ret = -EINVAL;
-				goto out;
-			}
-			if (!(mask & CHRD_QP_ATTR_MPA_ATTR)) {
-				ret = -EINVAL;
-				goto out;
-			}
-			qhp->attr.mpa_attr = attrs->mpa_attr;
-			qhp->attr.llp_stream_handle = attrs->llp_stream_handle;
-			qhp->ep = qhp->attr.llp_stream_handle;
-			set_state(qhp, CHRD_QP_STATE_RTS);
+	case CHRD_QP_V2_STATE_RESET:
+		switch (attrs.next_state) {
+		case CHRD_QP_V2_STATE_IDLE:
+			set_v2_state(qhp, CHRD_QP_V2_STATE_IDLE);
+			break;
+		default:
+			pr_debug("check here!\n");
+			ret = -EINVAL;
+			goto out;
+		}
+		break;
+	case CHRD_QP_V2_STATE_IDLE:
+		switch (attrs.next_state) {
+		case CHRD_QP_V2_STATE_IDLE:
+			set_v2_state(qhp, CHRD_QP_V2_STATE_IDLE);
+			break;
+		case CHRD_QP_V2_STATE_RTR:
+			set_v2_state(qhp, CHRD_QP_V2_STATE_RTR);
+			break;
+		case CHRD_QP_V2_STATE_RTS:
+			set_v2_state(qhp, CHRD_QP_V2_STATE_RTS);
+			break;
+		case CHRD_QP_V2_STATE_ERROR:
+			set_v2_state(qhp, CHRD_QP_V2_STATE_ERROR);
+			flush_qp(qhp);
+			break;
+		default:
+			ret = -EINVAL;
+			goto out;
+		}
+		break;
+	case CHRD_QP_V2_STATE_RTR:
+		switch (attrs.next_state) {
+		case CHRD_QP_V2_STATE_RTS:
+			set_v2_state(qhp, CHRD_QP_V2_STATE_RTS);
 
-			/*
-			 * Ref the endpoint here and deref when we
-			 * disassociate the endpoint from the QP.  This
-			 * happens in CLOSING->IDLE transition or *->ERROR
-			 * transition.
-			 */
-			chrd_get_ep(&qhp->ep->com);
-			ret = rdma_init(rhp, qhp);
+			if (qhp->qp_type == IB_QPT_RC) {
+				ret = roce_act_open_req(qhp);
+				if (ret)
+					goto err;
+			}
+			ret = send_roce_flowc(qhp);
+			if (ret)
+				goto err;
+			ret = rdma_roce_init(rhp, qhp);
 			if (ret)
 				goto err;
 			break;
-		case CHRD_QP_STATE_ERROR:
-			set_state(qhp, CHRD_QP_STATE_ERROR);
+		case CHRD_QP_V2_STATE_ERROR:
+			set_v2_state(qhp, CHRD_QP_V2_STATE_ERROR);
 			flush_qp(qhp);
 			break;
 		default:
@@ -2861,47 +4540,23 @@ int c4iw_modify_rc_qp(struct c4iw_qp *qhp, enum c4iw_qp_attr_mask mask,
 			goto out;
 		}
 		break;
-	case CHRD_QP_STATE_RTS:
-		switch (attrs->next_state) {
-		case CHRD_QP_STATE_CLOSING:
+	case CHRD_QP_V2_STATE_RTS:
+		switch (attrs.next_state) {
+		case CHRD_QP_V2_STATE_CLOSING:
 			t4_set_wq_in_error(&qhp->wq, 0);
-			set_state(qhp, CHRD_QP_STATE_CLOSING);
-			ep = qhp->ep;
-			if (!internal) {
-				abort = 0;
-				disconnect = 1;
-				chrd_get_ep(&qhp->ep->com);
-			}
-			ret = rdma_fini(rhp, qhp, ep);
+			set_v2_state(qhp, CHRD_QP_V2_STATE_CLOSING);
+//			ret = rdma_roce_fini(rhp, qhp);			/*Todo: Check the Commented Code*/
 			if (ret)
 				goto err;
 			break;
-		case CHRD_QP_STATE_TERMINATE:
+		case CHRD_QP_V2_STATE_TERMINATE:
 			t4_set_wq_in_error(&qhp->wq, 0);
-			set_state(qhp, CHRD_QP_STATE_TERMINATE);
-			qhp->attr.layer_etype = attrs->layer_etype;
-			qhp->attr.ecode = attrs->ecode;
-			ep = qhp->ep;
-			if (!internal) {
-				chrd_get_ep(&qhp->ep->com);
-				terminate = 1;
-				disconnect = 1;
-			} else {
-				terminate = qhp->attr.send_term;
-				ret = rdma_fini(rhp, qhp, ep);
-				if (ret)
-					goto err;
-			}
+			set_v2_state(qhp, CHRD_QP_V2_STATE_TERMINATE);
+//			qhp->attr.ecode = attrs->ecode;			/*Todo: Check the Commented Code*/
 			break;
-		case CHRD_QP_STATE_ERROR:
+		case CHRD_QP_V2_STATE_ERROR:
 			t4_set_wq_in_error(&qhp->wq, 0);
-			set_state(qhp, CHRD_QP_STATE_ERROR);
-			if (!internal) {
-				abort = 1;
-				disconnect = 1;
-				ep = qhp->ep;
-				chrd_get_ep(&qhp->ep->com);
-			}
+			set_v2_state(qhp, CHRD_QP_V2_STATE_ERROR);
 			goto err;
 			break;
 		default:
@@ -2909,34 +4564,31 @@ int c4iw_modify_rc_qp(struct c4iw_qp *qhp, enum c4iw_qp_attr_mask mask,
 			goto out;
 		}
 		break;
-	case CHRD_QP_STATE_CLOSING:
+	case CHRD_QP_V2_STATE_CLOSING:
 
 		/*
 		 * Allow kernel users to move to ERROR for qp draining.
 		 */
-		if (!internal && (qhp->ibqp.uobject || attrs->next_state !=
-				  CHRD_QP_STATE_ERROR)) {
+		if (!internal && (qhp->ibqp.uobject || attrs.next_state !=
+				  CHRD_QP_V2_STATE_ERROR)) {
 			ret = -EINVAL;
 			goto out;
 		}
-		switch (attrs->next_state) {
-		case CHRD_QP_STATE_IDLE:
+		switch (attrs.next_state) {
+		case CHRD_QP_V2_STATE_IDLE:
 			flush_qp(qhp);
-			set_state(qhp, CHRD_QP_STATE_IDLE);
-			qhp->attr.llp_stream_handle = NULL;
-			chrd_put_ep(&qhp->ep->com);
-			qhp->ep = NULL;
+			set_v2_state(qhp, CHRD_QP_V2_STATE_IDLE);
 			wake_up(&qhp->wait);
 			break;
-		case CHRD_QP_STATE_ERROR:
+		case CHRD_QP_V2_STATE_ERROR:
 			goto err;
 		default:
 			ret = -EINVAL;
 			goto err;
 		}
 		break;
-	case CHRD_QP_STATE_ERROR:
-		if (attrs->next_state != CHRD_QP_STATE_IDLE) {
+	case CHRD_QP_V2_STATE_ERROR:
+		if (attrs.next_state != CHRD_QP_V2_STATE_IDLE) {
 			ret = -EINVAL;
 			goto out;
 		}
@@ -2944,9 +4596,9 @@ int c4iw_modify_rc_qp(struct c4iw_qp *qhp, enum c4iw_qp_attr_mask mask,
 			ret = -EINVAL;
 			goto out;
 		}
-		set_state(qhp, CHRD_QP_STATE_IDLE);
+		set_v2_state(qhp, CHRD_QP_V2_STATE_IDLE);
 		break;
-	case CHRD_QP_STATE_TERMINATE:
+	case CHRD_QP_V2_STATE_TERMINATE:
 		if (!internal) {
 			ret = -EINVAL;
 			goto out;
@@ -2961,15 +4613,7 @@ int c4iw_modify_rc_qp(struct c4iw_qp *qhp, enum c4iw_qp_attr_mask mask,
 	}
 	goto out;
 err:
-	pr_debug("disassociating ep %p qpid 0x%x\n", qhp->ep,
-	     qhp->wq.sq.qid);
-
-	/* disassociate the LLP connection */
-	qhp->attr.llp_stream_handle = NULL;
-	if (!ep)
-		ep = qhp->ep;
-	qhp->ep = NULL;
-	set_state(qhp, C4IW_QP_STATE_ERROR);
+	set_v2_state(qhp, CHRD_QP_V2_STATE_ERROR);
 	free = 1;
 	abort = 1;
 	flush_qp(qhp);
@@ -2977,42 +4621,22 @@ err:
 out:
 	mutex_unlock(&qhp->mutex);
 
-	if (terminate)
-		post_terminate(qhp, NULL, internal ? GFP_ATOMIC : GFP_KERNEL);
-
-	/*
-	 * If disconnect is 1, then we need to initiate a disconnect
-	 * on the EP.  This can be a normal close (RTS->CLOSING) or
-	 * an abnormal close (RTS/CLOSING->ERROR).
-	 */
-	if (disconnect) {
-		c4iw_ep_disconnect(ep, abort, internal ? GFP_ATOMIC :
-							 GFP_KERNEL);
-		c4iw_put_ep(&ep->com);
-	}
-
-	/*
-	 * If free is 1, then we've disassociated the EP from the QP
-	 * and we need to dereference the EP.
-	 */
-	if (free)
-		c4iw_put_ep(&ep->com);
-	pr_debug("exit state %d\n", qhp->attr.state);
+	pr_debug("exit state %d ret %d\n", qhp->attr.state, ret);
 	return ret;
 }
 
 static void destroy_raw_qp(struct ib_qp *ib_qp)
 {
 	struct chrd_dev *rhp;
 	struct chrd_raw_qp *rqp;
 	struct chrd_common_qp_attributes attrs;
 
 	rqp = to_chrd_raw_qp(ib_qp);
-	rhp = rqp->dev;
+	rhp = rqp->rhp;
 
 	pr_debug("qpid %d\n", ib_qp->qp_num);
 	attrs.next_state = CHRD_QP_STATE_ERROR;
 	modify_raw_qp(rqp, CHRD_QP_ATTR_NEXT_STATE, &attrs);
 
 	if (!ib_qp->srq)
 		xa_erase_irq(&rhp->rawiqs, rqp->iq.cntxt_id);
@@ -3050,22 +4674,22 @@ static void destroy_raw_qp(struct ib_qp *ib_qp)
 	return;
 }
 
-static void destroy_rc_qp(struct ib_qp *ib_qp)
+/*static void destroy_rc_qp(struct ib_qp *ib_qp)
 {
 	struct chrd_dev *rhp;
 	struct chrd_qp *qhp;
 	struct chrd_ucontext *ucontext;
 	struct chrd_common_qp_attributes attrs;
 
 	qhp = to_chrd_qp(ib_qp);
 	rhp = qhp->rhp;
 	ucontext = qhp->ucontext;
 
 	attrs.next_state = CHRD_QP_STATE_ERROR;
 	if (qhp->attr.state == CHRD_QP_STATE_TERMINATE)
 		chrd_modify_iw_rc_qp(qhp, CHRD_QP_ATTR_NEXT_STATE, &attrs, 1);
 	else	
 		chrd_modify_iw_rc_qp(qhp, CHRD_QP_ATTR_NEXT_STATE, &attrs, 0);
 	wait_event(qhp->wait, !qhp->ep);
 
 
@@ -3077,27 +4701,31 @@ static void destroy_rc_qp(struct ib_qp *ib_qp)
 	xa_unlock_irq(&rhp->qps);
 	free_ird(rhp, qhp->attr.max_ird);
 
 	chrd_iw_qp_rem_ref(ib_qp);
 
 	wait_for_completion(&qhp->qp_rel_comp);
 
 	pr_debug("ib_qp %p qpid 0x%0x\n", ib_qp, qhp->wq.sq.qid);
 	pr_debug("qhp %p ucontext %p\n", qhp, ucontext);
 
 	free_rc_queues(&rhp->rdev, &qhp->wq, ucontext ?
 		       &ucontext->uctx : &rhp->rdev.uctx, !qhp->srq);
-
+	if (unlikely(qhp->qp_type == IB_QPT_GSI)) {
+		del_filter((struct chrd_raw_qp *)qhp, qhp->gsi_filt.filter_id);
+	}
 	chrd_put_wr_wait(qhp->wr_waitp);
 
 	return;
-}
+}*/
 
 int chrd_destroy_qp(struct ib_qp *ib_qp, struct ib_udata *udata)
 {
 	pr_debug("qpid %d\n", ib_qp->qp_num);
 	switch (ib_qp->qp_type) {
 	case IB_QPT_RC:
-		destroy_rc_qp(ib_qp);
+	case IB_QPT_GSI:
+	case IB_QPT_UD:
+		//destroy_rc_qp(ib_qp);			/*Todo: Check the Commented Code */
 		break;
 	case IB_QPT_RAW_ETH:
 		destroy_raw_qp(ib_qp);
@@ -3109,52 +4737,203 @@ int c4iw_destroy_qp(struct ib_qp *ib_qp, struct ib_udata *udata)
 	return 0;
 }
 
+static int chrd_validate_qp_attrs(struct chrd_dev *rhp,
+				  struct ib_qp_init_attr *attrs,
+				  struct ib_udata *udata)
+{
+	if (attrs->cap.max_inline_data > T4_MAX_SEND_INLINE)
+		return -EINVAL;
+
+	if (rdma_protocol_roce(&rhp->ibdev, 1)) {
+		if (attrs->qp_type != IB_QPT_RC &&
+			attrs->qp_type != IB_QPT_UD &&
+			attrs->qp_type != IB_QPT_GSI)
+			return -EOPNOTSUPP;
+	} else {
+			if (attrs->qp_type != IB_QPT_RC)
+			return -EOPNOTSUPP;
+	}
+
+	if (!attrs->srq) {
+		if (attrs->cap.max_recv_wr > rhp->rdev.hw_queue.t4_max_rq_size)
+			return -E2BIG;
+	}
+
+	if (attrs->cap.max_send_wr > rhp->rdev.hw_queue.t4_max_sq_size)
+		return -E2BIG;
+
+	return 0;
+}
+
+# if 0
+static int create_gsi_qp(struct chrd_pd *php, struct chrd_dev *rhp,
+				  struct ib_qp_init_attr *attrs,
+				  struct ib_udata *udata)
+{
+	struct ch_filter filt;
+	struct filter_ctx ctx;
+
+	qhp->rhp = rhp;
+	qhp->qp_type = IB_QPT_GSI;
+	qhp->qp_trans = IB_QPT_GSI;
+	qhp->ibqp.qp_num = 1;
+	qhp->attr.pd = php->pdid;
+	qhp->attr.sq_num_entries = attrs->cap.max_send_wr;
+	qhp->attr.sq_max_sges = attrs->cap.max_send_sge;
+	qhp->attr.rq_num_entries = attrs->cap.max_recv_wr;
+	qhp->attr.rq_max_sges = attrs->cap.max_recv_sge;
+	qhp->attr.state = CHRD_QP_V2_STATE_RESET;
+	qhp->attr.next_state = CHRD_QP_STATE_RESET;
+	qhp->sq_sig_all = attrs->sq_sig_type == IB_SIGNAL_ALL_WR;
+
+	spin_lock_init(&qhp->lock);
+	mutex_init(&qhp->mutex);
+	init_waitqueue_head(&qhp->wait);
+	init_completion(&qhp->qp_rel_comp);
+	refcount_set(&qhp->qp_refcnt, 1);
+
+	/* ToDo: Allocate SQ and RQ SW queues */
+
+	// Create ipv4 and ipv6 filters for RoCE port 4791
+	memset(&filt, 0, sizeof filt);
+	netdev = rhp->rdev.lldi.ports[0];
+	filt.filter_id = cxgb4_get_free_ftid(netdev, PF_INET, 0); /*set prio as 1 if you need hpf*/
+	filt.filter_ver = CH_FILTER_SPECIFICATION_ID;
+	// Todo: set vlan specific filt fields in case of vlan
+	filt.fs.val.lport = CHRD_ROCE_PORT;
+	filt.fs.mask.lport = -1;
+	filt.fs.val.proto = IPPROTO_UDP;
+	filt.fs.mask.proto = -1;
+	filt.fs.action = FILTER_PASS;
+	filt.fs.dirsteer = 1;
+	//filt.fs.iq = rhp->rdev.lldi.rxq_ids[0];		/*Todo: Check the Commented Code */
+	filt.fs.rpttid = 1;
+	filt.fs.hitcnts = 1;
+	filt.cmd = CHELSIO_SET_FILTER;
+
+	init_completion(&ctx.completion);
+	rtnl_lock();
+	ret = cxgb4_set_filter(netdev, filt.filter_id, &filt.fs, &ctx,
+			       GFP_KERNEL);
+	rtnl_unlock();
+	if (!ret) {
+		ret = wait_for_completion_timeout(&ctx.completion, 10*HZ);
+		if (!ret) {
+			printk("%s: filter creation timed out\n", __func__);
+			goto free_rq;
+		} else {
+			ret = ctx.result;
+		}
+	}
+
+	rhp->rdev.gsi_scq = get_chp(rhp, ((struct chrd_cq *)attrs->send_cq)->cq.cqid);
+	rhp->rdev.gsi_rcq = get_chp(rhp, ((struct chrd_cq *)attrs->recv_cq)->cq.cqid);
+	rhp->rdev.gsi_qp = qhp;
+	rhp->rdev.gsi_qp_inuse = 1;
+	rhp->rdev.gsi_scq->gsi_cq = 1;
+	rhp->rdev.gsi_rcq->gsi_cq = 1;
+
+	return &qhp->ibqp;
+free_rq:
+	kfree(wq->rq.sw_rq);
+free_sq:
+	kfree(wq->sq.sw_sq);
+free_qhp:
+	kfree(qhp);
+	return ERR_PTR(ret);
+}
+#endif
+
+static int create_gsi_filter(struct net_device *netdev, struct chrd_qp *qhp, u8 port_num)
+{
+	struct chrd_dev *rhp = qhp->rhp;
+	struct ch_filter filt = qhp->gsi_filt;
+	struct filter_ctx ctx;
+	int ret = 0;
+
+	memset(&filt, 0, sizeof filt);
+	pbr_debug("netdev 0x%llx\n", (unsigned long long)netdev);
+	filt.filter_id = cxgb4_get_free_ftid(netdev, PF_INET, 1); /*set prio as 1 if you need hpf*/
+	pbr_debug("NON HPF cxgb4_get_free_ftid: %d\n", filt.filter_id);
+	filt.filter_ver = CH_FILTER_SPECIFICATION_ID;
+	// Todo: set vlan specific filt fields in case of vlan
+	filt.fs.val.lport = 0x0;
+	filt.fs.mask.lport = -1;
+	filt.fs.val.fport = 0x0100;	/*Todo: hardcoded value */
+	filt.fs.mask.fport = -1;
+	filt.fs.val.iport = port_num;
+	filt.fs.mask.iport = -1;
+	//filt.fs.val.proto = IPPROTO_UDP;	/*Todo: Check the Commented Code */
+	//filt.fs.mask.proto = -1;
+	filt.fs.action = FILTER_PASS;
+	filt.fs.dirsteer = 1;
+	//filt.fs.iq = rhp->rdev.lldi.rxq_ids[0]; /* not needed */
+	//filt.fs.rpttid = 1;		/*Todo: Check the Commented Code */
+	filt.fs.prio = 1;
+	filt.fs.hitcnts = 1;
+	filt.fs.val.roce = 1;
+	filt.fs.mask.roce = -1;
+	filt.fs.val.rocev2_qpn = 1;		/* GSI QP number */
+	filt.cmd = CHELSIO_SET_FILTER;
+	
+	init_completion(&ctx.completion);
+	rtnl_lock();
+	ret = cxgb4_set_filter(netdev, filt.filter_id, &filt.fs, &ctx,
+			       GFP_KERNEL);
+	rtnl_unlock();
+	pbr_debug("cxgb4_set_filter() tid %d ret: %d\n", ctx.tid, ret);
+	if (!ret) {
+		ret = wait_for_completion_timeout(&ctx.completion, 10*HZ);
+		if (!ret) {
+			pr_err("%s: filter creation timed out\n", __func__);
+		}
+	}
+
+//	rhp->rdev.gsi_scq = get_chp(rhp, ((struct chrd_cq *)attrs->send_cq)->cq.cqid);	/*Todo: Check the Commented Code */
+//	rhp->rdev.gsi_rcq = get_chp(rhp, ((struct chrd_cq *)attrs->recv_cq)->cq.cqid);
+	rhp->rdev.gsi_ftid = filt.filter_id;
+	//rhp->rdev.gsi_ftid = filt.filter_id == 0 ? 192 : filt.filter_id;		/*Todo: Check the Commented Code */
+	//rhp->rdev.gsi_ftid = ctx.tid;
+	rhp->rdev.gsi_qp_inuse = 1;
+	rhp->rdev.gsi_qp = qhp;
+
+	return ret;
+}
+
 static int create_rc_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 			 struct ib_udata *udata)
 {
+	struct chrd_create_qp_resp uresp = {0};
+	struct chrd_ucontext *ucontext;
 	struct ib_pd *pd = qp->pd;
 	struct chrd_dev *rhp;
 	struct chrd_qp *qhp = to_chrd_qp(qp);
 	struct chrd_pd *php;
 	struct chrd_cq *schp;
 	struct chrd_cq *rchp;
-	struct chrd_create_qp_resp uresp = {0};
 	int sqsize, rqsize = 0;
-	struct chrd_ucontext *ucontext = rdma_udata_to_drv_context(
-			udata, struct chrd_ucontext, ibucontext);
-	int ret;
+	int ret = 0;
 	struct chrd_mm_entry *sq_key_mm, *rq_key_mm = NULL, *sq_db_key_mm;
 	struct chrd_mm_entry *rq_db_key_mm = NULL, *ma_sync_key_mm = NULL;
 	struct resource *res;
 
-	pr_debug("ib_pd %p\n", pd);
-
+	ucontext = rdma_udata_to_drv_context(udata, struct chrd_ucontext,
+					     ibucontext);
 	php = to_chrd_pd(pd);
 	rhp = php->rhp;
 
 	res = cxgb4_bar_resource(rhp->rdev.lldi.ports[0], 0);
 	if (!res)
 		return -EOPNOTSUPP;
 
 	schp = get_chp(rhp, ((struct chrd_cq *)attrs->send_cq)->cq.cqid);
 	rchp = get_chp(rhp, ((struct chrd_cq *)attrs->recv_cq)->cq.cqid);
 	if (!schp || !rchp)
 		return -EINVAL;
 
-	if (attrs->cap.max_inline_data > T4_MAX_SEND_INLINE)
-		return -EINVAL;
-
-	if (!attrs->srq) {
-		if (attrs->cap.max_recv_wr > rhp->rdev.hw_queue.t4_max_rq_size)
-			return -E2BIG;
-		rqsize = attrs->cap.max_recv_wr + 1;
-		if (rqsize < 8)
-			rqsize = 8;
-	}
-
-	if (attrs->cap.max_send_wr > rhp->rdev.hw_queue.t4_max_sq_size)
-		return -E2BIG;
-
+	ret = chrd_validate_qp_attrs(rhp, attrs, udata);
+	if (ret)
+		return ret;
 	/* 
 	 * Temporary workaround for iSER. iSER needs relatively large SQ for iw_cxgb4.
 	 * Therefore we factor max_send_wr with 3 based on unique SQ size of iSER
@@ -3167,8 +4946,13 @@ static int create_rc_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 
 	if (sqsize < 8)
 		sqsize = 8;
+	if (!attrs->srq) {
+		rqsize = attrs->cap.max_recv_wr + 1;
+		if (rqsize < 8)
+			rqsize = 8;
+	}
 
 	qhp->wr_waitp = chrd_alloc_wr_wait(GFP_KERNEL);
 	if (!qhp->wr_waitp)
 		return -ENOMEM;
 
@@ -3189,10 +4973,16 @@ static int create_rc_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 			qhp->wq.rq.memsize = roundup(qhp->wq.rq.memsize, PAGE_SIZE);
 	}
 
+	if (rdma_protocol_roce(&rhp->ibdev, 1)) {
+		qhp->qp_trans = CHRD_TRANSPORT_ROCEV2;
+	} else {
+		qhp->qp_trans = CHRD_TRANSPORT_IWARP;
+	}
+	qhp->qp_type = attrs->qp_type;
 
-	ret = alloc_rc_queues(&rhp->rdev, &qhp->wq, &schp->cq, &rchp->cq,
+	ret = alloc_rc_queues(rhp, qhp, &schp->cq, &rchp->cq,
 			      ucontext ? &ucontext->uctx : &rhp->rdev.uctx,
-			      !attrs->srq, qhp->wr_waitp);
+			      !attrs->srq);
 	if (ret)
 		goto err_free_wr_wait;
 
@@ -3211,8 +5001,13 @@ static int create_rc_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 		qhp->attr.rq_num_entries = attrs->cap.max_recv_wr;
 		qhp->attr.rq_max_sges = attrs->cap.max_recv_sge;
 	}
-	qhp->attr.state = CHRD_QP_STATE_IDLE;
-	qhp->attr.next_state = CHRD_QP_STATE_IDLE;
+	if (rdma_protocol_roce(&rhp->ibdev, 1)) {
+		qhp->attr.state = CHRD_QP_V2_STATE_RESET;
+		qhp->attr.next_state = CHRD_QP_V2_STATE_RESET;
+	} else {
+		qhp->attr.state = CHRD_QP_STATE_IDLE;
+		qhp->attr.next_state = CHRD_QP_STATE_IDLE;
+	}
 	qhp->attr.enable_rdma_read = 1;
 	qhp->attr.enable_rdma_write = 1;
 	qhp->attr.enable_bind = 1;
@@ -3342,17 +5137,23 @@ static int create_rc_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 		qhp->wq.qp_errp = &qhp->wq.sq.queue[qhp->wq.sq.size].status.qp_err;
 		qhp->wq.srqidxp = &qhp->wq.sq.queue[qhp->wq.sq.size].status.srqidx;
 	}
+
 	qhp->ibqp.qp_num = qhp->wq.sq.qid;
+	qhp->netdev = rhp->rdev.lldi.ports[attrs->port_num - 1];
+	if (unlikely(qhp->qp_type == IB_QPT_GSI)) {
+		ret = create_gsi_filter(qhp->netdev, qhp, attrs->port_num - 1);
+	}
+
 	if (attrs->srq)
 		qhp->srq = to_chrd_srq(attrs->srq);
 	INIT_LIST_HEAD(&qhp->fcl.db_fc_entry);
 	qhp->fcl.type = RC_QP;
 
-	pr_debug("sq id %u size %u memsize %lu num_entries %u "
-	     "rq id %u size %u memsize %lu num_entries %u\n",
+	pr_debug("sq id %u size %u memsize %lu num_entries %u "
+	     "rq id %u size %u memsize %lu num_entries %u sq_sig_all %u scqid %u\n",
 	     qhp->wq.sq.qid, qhp->wq.sq.size, (unsigned long)qhp->wq.sq.memsize,
 	     attrs->cap.max_send_wr, qhp->wq.rq.qid, qhp->wq.rq.size,
-	     (unsigned long)qhp->wq.rq.memsize, attrs->cap.max_recv_wr);
+	     (unsigned long)qhp->wq.rq.memsize, attrs->cap.max_recv_wr, qhp->sq_sig_all, schp->cq.cqid);
 
 	return 0;
 err_free_ma_sync_key:
@@ -3473,7 +5274,7 @@ static int create_raw_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 	rqp->netdev = rhp->rdev.lldi.ports[ureq.port];
 	rqp->vlan_pri = ureq.vlan_pri;
 	rqp->nfids = ureq.nfids;
-	rqp->dev = rhp;
+	rqp->rhp = rhp;
 	rqp->txq_idx = cxgb4_port_idx(rqp->netdev) * rhp->rdev.lldi.ntxq /
 						     rhp->rdev.lldi.nchan;
 
@@ -3735,16 +5536,26 @@ err1:
 	return ret;
 }
 
 int chrd_create_qp(struct ib_qp *qp, struct ib_qp_init_attr *attrs,
 		   struct ib_udata *udata)
 {
+	struct chrd_dev *rhp;
+	struct chrd_pd *php;
 	int ret;
 
+	php = to_chrd_pd(qp->pd);
+	rhp = php->rhp;
+	pr_debug("ib_pd %p QP type %d\n", qp->pd, attrs->qp_type);
+
 	switch (attrs->qp_type) {
 	case IB_QPT_RC:
+	case IB_QPT_GSI:
+	case IB_QPT_UD:
 		ret = create_rc_qp(qp, attrs, udata);
 		break;
 	case IB_QPT_RAW_ETH:
+		if (!(qp->pd->uobject))
+			return -EINVAL;
 		ret = create_raw_qp(qp, attrs, udata);
 		break;
 	default:
@@ -3794,3 +5605,3 @@ int c4iw_ib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 	 */
 	attrs.sq_db_inc = attr->sq_psn;
 	attrs.rq_db_inc = attr->rq_psn;
	mask |= (attr_mask & IB_QP_SQ_PSN) ? CHRD_QP_ATTR_SQ_DB : 0;
 	mask |= (attr_mask & IB_QP_RQ_PSN) ? CHRD_QP_ATTR_RQ_DB : 0;
 	if (!is_t4(to_chrd_qp(ibqp)->rhp->rdev.lldi.adapter_type) &&
 	    (mask & (CHRD_QP_ATTR_SQ_DB|CHRD_QP_ATTR_RQ_DB)))
 		return -EINVAL;
 
 	switch (ibqp->qp_type) {
 	case IB_QPT_RC:
 		ret = chrd_modify_iw_rc_qp(to_chrd_qp(ibqp), mask, &attrs, 0);
 		break;
 	case IB_QPT_RAW_ETH:
 		ret = modify_raw_qp(to_chrd_raw_qp(ibqp), mask, &attrs);
+		break;
+	default:
+		WARN_ONCE(1, "unknown qp type %u\n", ibqp->qp_type);
+	}
+	return ret;
+}
+
+int chrd_roce_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
+		      int attr_mask, struct ib_udata *udata)
+{
+	int ret = 0;
+
+	pbr_debug("ib_qp 0x%llx\n", (unsigned long long)ibqp);
+
+	if (!attr_mask)
+		return 0;
+
+	switch (ibqp->qp_type) {
+	case IB_QPT_GSI:
+	case IB_QPT_RC:
+	case IB_QPT_UD:
+		ret = chrd_modify_roce_qp(to_chrd_qp(ibqp), attr_mask, attr, 0);
+		break;
+	case IB_QPT_XRC_INI:
+	case IB_QPT_XRC_TGT:
+		WARN_ONCE(1, "XRC qp type %u\n", ibqp->qp_type);
 		break;
 	default:
 		WARN_ONCE(1, "unknown qp type %u\n", ibqp->qp_type);
 	}
+
 	return ret;
 }
 
@@ -3865,31 +5703,37 @@ out:
 	return ret;
 }
 
 int chrd_modify_srq(struct ib_srq *ib_srq, struct ib_srq_attr *attr,
 		    enum ib_srq_attr_mask srq_attr_mask,
 		    struct ib_udata *udata)
 {
 	struct chrd_srq *srq = to_chrd_srq(ib_srq);
 
 	if (srq->fcl.type == RAW_SRQ)
 		return modify_raw_srq(ib_srq, attr, srq_attr_mask, udata);
 	return modify_srq(ib_srq, attr, srq_attr_mask, udata);
 }
 
 struct ib_qp *chrd_iw_get_qp(struct ib_device *dev, int qpn)
 {
 	pr_debug("ib_dev %p qpn 0x%x\n", dev, qpn);
 	return (struct ib_qp *)get_qhp(to_chrd_dev(dev), qpn);
 }
 
 int chrd_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,
 		     int attr_mask, struct ib_qp_init_attr *init_attr)
 {
 	struct chrd_qp *qhp = to_chrd_qp(ibqp);
 
 	memset(attr, 0, sizeof *attr);
 	memset(init_attr, 0, sizeof *init_attr);
-	attr->qp_state = to_ib_qp_state(qhp->attr.state);
+	if (rdma_protocol_roce(ibqp->device, 1)) {
+		attr->qp_state = v2_to_ib_qp_state(qhp->attr.state);
+		pr_debug("ibqp 0x%llx attr->qp_state %d\n", (unsigned long long)ibqp, attr->qp_state);
+	} else {
+		attr->qp_state = to_ib_qp_state(qhp->attr.state);
+		pr_debug("ibqp 0x%llx attr->qp_state %d\n", (unsigned long long)ibqp, attr->qp_state);
+	}
 	init_attr->cap.max_send_wr = qhp->attr.sq_num_entries;
 	init_attr->cap.max_recv_wr = qhp->attr.rq_num_entries;
 	init_attr->cap.max_send_sge = qhp->attr.sq_max_sges;
diff --git a/dev/T4/linux/iw_cxgb4/resource.c b/dev/T4/linux/iw_cxgb4/resource.c
index 17415e529..3a174836a 100644
--- a/dev/T4/linux/iw_cxgb4/resource.c
+++ b/dev/T4/linux/iw_cxgb4/resource.c
@@ -323,30 +323,107 @@ int c4iw_pblpool_create(struct c4iw_rdev *rdev)
 			}
 			pbl_chunk >>= 1;
 		} else {
 			pr_debug("added PBL chunk (%x/%x)\n",
 				 pbl_start, pbl_chunk);
 			pbl_start += pbl_chunk;
 		}
 	}
 
 	return 0;
 }
 
 void chrd_pblpool_destroy(struct chrd_rdev *rdev)
 {
 	kref_put(&rdev->pbl_kref, destroy_pblpool);
 }
 
+#define MIN_RRQT_SHIFT 6	/*Todo: Asummed to be 64B for each entry*/
+
+u32 chrd_rrqtpool_alloc(struct chrd_rdev *rdev, int size)
+{
+        unsigned long addr = gen_pool_alloc(rdev->rrqt_pool, size);
+        pbr_debug("addr 0x%x size %d\n", (u32)addr, size << 6);
+        if (!addr)
+                pr_warn_ratelimited("Out of RRQT memory\n");
+
+        mutex_lock(&rdev->stats.lock);
+        if (addr) {
+                rdev->stats.rrqt.cur += roundup(size, 1 << MIN_RRQT_SHIFT);
+                if (rdev->stats.rrqt.cur > rdev->stats.rrqt.max)
+                        rdev->stats.rrqt.max = rdev->stats.rrqt.cur;
+                kref_get(&rdev->rrqt_kref);
+        } else
+                rdev->stats.rrqt.fail++;
+        mutex_unlock(&rdev->stats.lock);
+        return (u32)addr;
+}
+
+static void destroy_rrqtpool(struct kref *kref)
+{
+	struct chrd_rdev *rdev;
+
+	rdev = container_of(kref, struct chrd_rdev, rrqt_kref);
+	gen_pool_destroy(rdev->rrqt_pool);
+	complete(&rdev->rrqt_compl);
+}
+
+void chrd_rrqtpool_free(struct chrd_rdev *rdev, u32 addr, int size)
+{
+	pbr_debug("addr 0x%x size %d\n", addr, size);
+	mutex_lock(&rdev->stats.lock);
+	rdev->stats.rrqt.cur -= roundup(size, 1 << MIN_RRQT_SHIFT);
+	mutex_unlock(&rdev->stats.lock);
+	gen_pool_free(rdev->rrqt_pool, (unsigned long)addr, size);
+	kref_put(&rdev->rrqt_kref, destroy_rrqtpool);
+}
+
+int chrd_rrqtpool_create(struct chrd_rdev *rdev)
+{
+        unsigned rrqt_start, rrqt_chunk, rrqt_top;
+
+        rdev->rrqt_pool = gen_pool_create(MIN_RRQT_SHIFT, -1);
+        if (!rdev->rrqt_pool)
+                return -ENOMEM;
+
+        rrqt_start = rdev->lldi.vr->rq.start;
+        rrqt_chunk = rdev->lldi.vr->rq.size;
+        rrqt_top = rrqt_start + rrqt_chunk;
+
+        while (rrqt_start < rrqt_top) {
+                rrqt_chunk = min(rrqt_top - rrqt_start + 1, rrqt_chunk);
+                if (gen_pool_add(rdev->rrqt_pool, rrqt_start, rrqt_chunk, -1)) {
+                        pbr_debug("failed to add RRQT chunk (%x/%x)\n",
+                                 rrqt_start, rrqt_chunk);
+                        if (rrqt_chunk <= 1024 << MIN_RRQT_SHIFT) {
+                                pr_warn("Failed to add all RRQT chunks (%x/%x)\n",
+                                        rrqt_start, rrqt_top - rrqt_start);
+                                return 0;
+                        }
+                        rrqt_chunk >>= 1;
+                } else {
+                        pbr_debug("added RRQT chunk (%x/%x)\n",
+                                 rrqt_start, rrqt_chunk);
+                        rrqt_start += rrqt_chunk;
+                }
+        }
+        return 0;
+}
+
+void chrd_rrqtpool_destroy(struct chrd_rdev *rdev)
+{
+	kref_put(&rdev->rrqt_kref, destroy_rrqtpool);
+}
+
 /*
  * RQT Memory Manager.  Uses Linux generic allocator.
  */
 
 #define MIN_RQT_SHIFT 10	/* 1KB == min RQT size (16 entries) */
 
 u32 chrd_rqtpool_alloc(struct chrd_rdev *rdev, int size)
 {
 	unsigned long addr = gen_pool_alloc(rdev->rqt_pool, size << 6);
 	pr_debug("addr 0x%x size %d\n", (u32)addr, size << 6);
 	if (!addr)
 		pr_warn_ratelimited("%s: Out of RQT memory\n",
 				    rdev->lldi.name);
diff --git a/dev/T4/linux/iw_cxgb4/t4.h b/dev/T4/linux/iw_cxgb4/t4.h
index 34f5d8d55..fcbbc7501 100644
--- a/dev/T4/linux/iw_cxgb4/t4.h
+++ b/dev/T4/linux/iw_cxgb4/t4.h
@@ -87,6 +95,8 @@ struct t4_status_page {
 #define T4_RQ_NUM_SLOTS 2
 #define T4_RQ_NUM_BYTES (T4_EQ_ENTRY_SIZE * T4_RQ_NUM_SLOTS)
 #define T4_MAX_RECV_SGE 4
+#define T7_MAX_RD_SGE 4
+#define GSI_MAX_RECV_SGE 1
 #define ISER_SQ_SIZE 523
 
 #define T4_WRITE_CMPL_MAX_SGL 4
@@ -95,10 +105,17 @@ union t4_wr {
 	struct fw_ri_res_wr res;
 	struct fw_ri_wr ri;
 	struct fw_ri_rdma_write_wr write;
+	struct fw_ri_v2_rdma_write_wr v2_write;
 	struct fw_ri_send_wr send;
+	struct fw_ri_v2_send_wr v2_send;
+	struct fw_ri_v2_ud_send_wr v2_ud_send;
 	struct fw_ri_rdma_read_wr read;
+	struct fw_ri_v2_rdma_read_wr v2_read;
+	struct fw_ri_v2_atomic_wr v2_atomic;
 	struct fw_ri_bind_mw_wr bind;
+	struct fw_ri_v2_bind_mw_wr v2_bind;
 	struct fw_ri_fr_nsmr_wr fr;
+	struct fw_ri_v2_fr_nsmr_wr v2_fr;
 	struct fw_ri_fr_nsmr_tpte_wr fr_tpte;
 	struct fw_ri_inv_lstag_wr inv;
 	struct fw_ri_rdma_write_cmpl_wr write_cmpl;
@@ -201,8 +218,17 @@ struct t4_cqe {
 			__be32 msn;
 			__u64 imm_data;
 		} imm_data_rcqe;
+		struct {
+			__be32 stag;
+			__be32 msn;
+			__be32 v2_header;
+			__be32 abs_rqe_idx;
+		} v2_com;
+		//Todo : fix this later ASAP, by correctly pointing to immdata for rocev2
 	} u;
-	__be64 reserved[3];
+	__be64 v2_ext_hi;
+	__be64 v2_ext_lo;
+	__be64 reserved;
 	__be64 bits_type_ts;
 };
 
@@ -247,6 +273,12 @@ struct t4_cqe {
 #define CQE_STATUS(x)     (G_CQE_STATUS(be32_to_cpu((x)->header)))
 #define CQE_OPCODE(x)     (G_CQE_OPCODE(be32_to_cpu((x)->header)))
 
+#define S_CQE_V2_OPCODE      24
+#define M_CQE_V2_OPCODE      0x7F
+#define G_CQE_V2_OPCODE(x)   ((((x) >> S_CQE_V2_OPCODE)) & M_CQE_V2_OPCODE)
+#define V_CQE_V2_OPCODE(x)   ((x)<<S_CQE_V2_OPCODE)
+#define CQE_V2_OPCODE(x)     (G_CQE_V2_OPCODE(be32_to_cpu((x)->u.v2_com.v2_header)))
+
 #define CQE_SEND_OPCODE(x)( \
 	(G_CQE_OPCODE(be32_to_cpu((x)->header)) == FW_RI_SEND) || \
 	(G_CQE_OPCODE(be32_to_cpu((x)->header)) == FW_RI_SEND_WITH_SE) || \
@@ -343,6 +375,7 @@ struct t4_swrqe {
 	u64 wr_id;
 	ktime_t host_time;
 	u64 sge_ts;
+	struct ib_sge sg_list[GSI_MAX_RECV_SGE];
 	int valid;
 };
 
@@ -503,11 +536,16 @@ static inline u32 t4_rq_avail(struct t4_wq *wq)
 	return wq->rq.size - 1 - wq->rq.in_use;
 }
 
-static inline void t4_rq_produce(struct t4_wq *wq, u8 len16)
+static inline void t4_sw_rq_produce(struct t4_wq *wq)
 {
 	wq->rq.in_use++;
 	if (++wq->rq.pidx == wq->rq.size)
 		wq->rq.pidx = 0;
+}
+
+static inline void t4_rq_produce(struct t4_wq *wq, u8 len16)
+{
+	t4_sw_rq_produce(wq);
 	wq->rq.wq_pidx += DIV_ROUND_UP(len16*16, T4_EQ_ENTRY_SIZE);
 	if (wq->rq.wq_pidx >= wq->rq.size * T4_RQ_NUM_SLOTS)
 		wq->rq.wq_pidx %= wq->rq.size * T4_RQ_NUM_SLOTS;
@@ -825,16 +865,21 @@ static inline struct t4_cqe *t4_next_sw_cqe(struct t4_cq *cq)
 	return NULL;
 }
 
-static inline int t4_next_cqe(struct t4_cq *cq, struct t4_cqe **cqe)
+static inline int t4_next_cqe(struct t4_cq *cq, struct t4_cqe **cqe, u8 *hcqe)
 {
 	int ret = 0;
 
-	if (cq->error)
+	if (cq->error) {
 		ret = -ENODATA;
-	else if (cq->sw_in_use)
+	} else if (cq->sw_in_use) {
+		pbr_debug("sw_in_use %d\n", cq->sw_in_use);
+		//BUG_ON(1); // disable for iwarp
 		*cqe = &cq->sw_queue[cq->sw_cidx];
-	else
+	} else {
 		ret = t4_next_hw_cqe(cq, cqe);
+		*hcqe = 1;			/*Todo: hardcoded value */
+		pbr_debug("ret %d\n", ret);
+	}
 	return ret;
 }
 
