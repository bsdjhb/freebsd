We would like to have a general purpose framework for doing efficient
zero-copy I/O.  Something like IO-Lite might work.  A brief summary of
my understanding of how IO-Lite worked:

- I/O data are stored in immutable buffers.  These buffers can be shared
  among multiple consumers and are free'd when their associated reference
  count drops to zero.
- An I/O request can stitch together multiple subsets of data from
  different buffers (basically an iovec of data buffers).  IO-Lite
  called these "buffer aggregates".
- User applications have to use a different API than read and write.
  These different entry points allow buffers to be specified as I/O
  requests.  In particular, the application doesn't allocate user
  buffers for read(2) but gets told where read(2) data is.

Additional things that IO-Lite might have also done but that I think you
need regardless:

- An I/O buffer has to be wired while it has a kernel reference.

In general I think this all sounds hunky-dory, but one of the sticky
widgets for me is mapping these buffers into userspace.  The kernel can
get by with sfbufs for when it needs virtual addresses and not just DMA
(I think).

One possible (but naive) design is to use a new VM object for each I/O
buffer.  For files you would want some sort of reverse-shadow object that
gives you a snapshot of the file's data.  That is, you would move the
existing pages from the file into a swap-backed object that would be
"behind" the vnode object so that writes would COW and allocate a page
in the vnode object, but reads would fall through to the page in the
I/O buffer for unchanged data.  For non-files you would use swap-backed
VM objects for data.  Internally, read operations would have to map
buffers into the process' address space.  Ideally you would unmap the
objects when the process was finished with them so that the objects
(and associated pages) could be harvested once they were no longer used.
(This would mean needing some way to say "done with this return from
read".)

My worry here is that this approach could add a lot of overhead.
You don't want a new object for every page-sized read that you
do (though to preserve the "snapshot" semantics of read from
files you may actually need that).  From non-files it might be nice to
amortize the cost if possible.  It would also be nice to "cache"
mappings to avoid mapping operations on read if the same buffer is
reused.

IO-Lite's paper makes mention of a special pager.  My
initial reading of this is that this means they provided a single
address window in each process that all I/O buffers had to live in,
but I'm not sure if that's really accurate to what they did.
Hmm, 2.2 does imply that each I/O stream (socket, etc.) kept a
pool of recently-free'd pages that could be reused.  I think this
implies that most larger-than-a-page I/O's would not be virtually
contiguous.

--------------------------------------------------------------------

Files make this complicated, so I'm going to punt and think about
just using this for streaming files (sockets to start, though ttys
should be "adoptable" as well).

The IO-Lite model seems to not be creating an object for each buffer.
In fact, it may be that what you end up with is one single object that
can grow over time.  The equivalent of aggregates would be (offset,
length) tuples relative to the single object.  In userland a library
would keep a cache of mappings.  In-kernel consumers would need to
wire pages while using them, but could unwire them when done.
The kernel would need to keep an in-kernel version of the buffers it
has given to each process (possibly to each vmspace instead?) so
that it can drop the reference counts on buffers if a process exits
without having done so (crash, etc.).  We would need a way to determine
which pages in the object were free and could be reused once they were
no longer referenced by any buffers (perhaps a per-page refcount),
and one optimization is that when the kernel requests a buffer we
should prefer page offsets that have a resident page to those that
do not.  I'm not sure if there's anything we can do to be smarter and
try to use superpages here when possible.  Note that the userland
mappings would generally be read-only except when requesting a new
buffer to use for a write.

Eventually if you wanted this to be "transparent" for regular NICs
you would want receive mbufs to be allocated as this type of I/O
buffer from the start rather than being mbuf clusters I think.
Otherwise you will need to copy the data out of any "normal" mbuf
clusters into a new I/O buffer before sending it to userland.

I'm not sure if this lends itself to having the same kind of per-fd
affinity that IO-Lite mentions so that you try to reuse buffers that
already exist.  Perhaps you could do that by placing "free" buffers on
a per-fd list as well as a global list (global lock would not be ideal
though).  If an fd's "free" list is empty it would start picking
things from the global list.  Maybe it would be better instead to only
have free buffers on the per-fd list but to instead have a list of
file descriptors and look for a file with the most free buffers and
grab those from the per-fd list of the other file.  You would still
have a global lock for the list of file descriptors, but it would only
change when a file descriptor started using I/O buffers or was closed.
The "find a free buffer" case would only have to read-lock the list.

I'm not sure how to detect when the user library should release
address space.  It would have to keep any addresses in active
aggregates mapped, but the idea of the cache is to avoid remapping
a buffer if it gets reused quickly, so you don't want to unmap
just because a given page is no longer used.  OTOH, you don't want
to starve out malloc() or dlopen() (especially on 32-bit platforms).
Assuming the bits live in libc, you could hook malloc() at least so
that if malloc fails to get a new chunk of address space it could
try a drain of the I/O buffer object before retrying it's mmap().
However, that isn't friendly to out-of-tree mallocs (e.g. debugging
mallocs), and it's harder to do this for dlopen().

--------------------------------------------------------------------

I had another thought in the shower this morning.  Some of the
concerns I have could be addressed at the expense of making this a bit
less automagical, but instead requiring userland to be more explicit.

Suppose you provided an API to allow userland to create a "pool" of
zerocopy I/O buffers.  Initially the API might use a single fixed-size
pool (e.g. N buffers of size M), though I think it would not be hard
to grow (and possibly shrink) the number of buffers in the pool
(having multiple sizes would also be doable, but that becomes more
complicated).

Implementation-wise, each pool would be backed by a single VM object
that is mapped into the address space of the process when the pool is
created.  In general the idea is that the mapping operations are now
explicitly done by userland so their cost can be amortized and so that
address space can be reserved in a way that error handling is
feasible.

Each pool would then keep an internal list of "free" buffers.
(Initially all buffers would be "free").  There would still be a
different API for zero-copy (at least for receive).  However, the way
it would work is that an application would "bind" a zero-copy pool to
a file descriptor to use for receiving data.  (Once this is done, all
data would go this route and not via read/recv.)  Multiple descriptors
can share the same pool.  Drivers/protocols would have access to this
pool if they want to grab free buffers to arrange for DMAs to write
directly into buffers that are then handed up the stack.  The
replacement for read/recv would instead return either a buffer ID for
each buffer that is returned.  At that point userland "owns" the
buffer.  It can return it to the free pool or use it for some other
purpose.

For zero-copy send, the application would need to allocate a free
buffer (or use one it just got via a read) and pass that to write.  We
might be able to get away with letting regular write use these buffers
via the user's mapping and just recognizing it as an I/O buffer
internally and knowing we can wire it and use the pages directly.
This would work on the honor system that userland won't scribble on it
while it is pending.  Once the write completes the kernel would return
the buffer to the pool's free list.

Note that this is a bit different from IO-Lite, but perhaps not super
different.  One thing that this API doesn't quite do is the immutable
buffer thing.  We could bring that back.  One way that is a slightly
stronger honor system would be to map the object as PROT_READ (but
with maxprot of PROT_READ | PROT_WRITE) and use mprotect to alter
buffers requested for a zero-copy send to PROT_READ | PROT_WRITE and
then switch them back to PROT_READ when the buffer was sent.  This
would likely be easiest if using an alternate API entry point for
write.

Another implementation choice is that the VM object could be
OBJT_DEFAULT but that individual buffers would be wired while in use
by the kernel (e.g. when a driver requests a free buffer to use for
receive, or when a zero-copy write is requested).  The wiring would be
released when the buffer is no longer needed for DMA (so when a driver
fills it in the Chelsio DDP case, or when a transmit is complete).
This would limit the amount of wired memory in use by allowing any
not-currently-being-DMA'd-to memory to be paged out at the expense of
extra overhead.

Another alternate approach for this API would be to just treat the VM
object as a pool of memory instead of a list of fixed-size buffers.
You would still need to honor page boundaries, but instead of passing
buffer ID's back and forth you would pass (offset, length) tuples that
are relative to the backing object.  We would need something akin to
vmem perhaps to manage which parts of the object are "free" vs used.
This would probably be a bit closer to how IO-Lite worked.

One thing this does not have that IO-Lite had is shared ownership
which would allow you to keep a handle on a buffer sent for zero-copy
send and reuse it to send the same data in the future.  I'm not
certain that you couldn't retrofit this into the API by providing a
reference count on buffers.  Bumping the reference count on a buffer
above 1 would mark it as read-only.  Finishing a zero-copy send would
drop the reference count and only put it back in the free pool if the
count dropped to zero.  For that use case, userland would allocate a
free buffer, scribble data into it, bump its reference count to "seal"
it, and then use it for multiple sends.  When it was finished with the
buffer it would drop its local reference count.

You could permit IPC to use this via explicit sharing (perhaps via a
new API).  One way we could allow this would be to use a file
descriptor as the handle for an I/O buffer object (you could almost
use shm_open(SHM_ANON) to get a good chunk of this) and pass it across
fork() or a Unix domain socket.  You could also add a dedicated API
to pass an I/O buffer object to another process, but then you also
need a way to allow a process to receive it, etc.  A new fd type might
make several of these things a lot easier to deal with (it's an easy
way to pass in a reference for a socket option or ioctl to bind receive
operations to a pool for example).

Aside from Chelsio DDP, there might be some other options for using this
for zero-copy receive.  I think the biggest caveat is that most drivers
split a single 4k page into two mbuf clusters, so if you were to do some
page swapping magic where you stuffed a page into a free "slot" of the
I/O object as a kind of remapping operation (ala the old zero copy sockets
stuff) you would leak data from some other random packet to userland.  It
would only be safe to do that sort of remapping trick if you have buffers
whose backing pages don't contain any other data.

File data would seem like it would be somewhat ideal if you let the
I/O buffer object shadow the file's VM object?  I think that doesn't
really work though as what you really want is the ability to shadow
multiple objects and at different offsets, etc.  (At least it wouldn't
work out of the box.)

I think things like pipes might work fine, but those are already
optimized to do this today I think via remapping.  They would just
do the same here.

Of course, one downside to the remapping thing is it means you start
stuffing arbitrary pages into the I/O object.  It might be nice if the
I/O object used a nice set of pages (superpages, NUMA, etc.).  That
would certainly be true for the Chelsio DDP case.

You can always implement the fallback for the zero-copy receive API by
doing copies from regular buffers (such as mbufs) to the zero-copy
object.  It would not be worse than what is there now (same number of
copies).
