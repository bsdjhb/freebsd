We would like to have a general purpose framework for doing efficient
zero-copy I/O.  Something like IO-Lite might work.  A brief summary of
my understanding of how IO-Lite worked:

- I/O data are stored in immutable buffers.  These buffers can be shared
  among multiple consumers and are free'd when their associated reference
  count drops to zero.
- An I/O request can stitch together multiple subsets of data from
  different buffers (basically an iovec of data buffers).  IO-Lite
  called these "buffer aggregates".
- User applications have to use a different API than read and write.
  These different entry points allow buffers to be specified as I/O
  requests.  In particular, the application doesn't allocate user
  buffers for read(2) but gets told where read(2) data is.

Additional things that IO-Lite might have also done but that I think you
need regardless:

- An I/O buffer has to be wired while it has a kernel reference.

In general I think this all sounds hunky-dory, but one of the sticky
widgets for me is mapping these buffers into userspace.  The kernel can
get by with sfbufs for when it needs virtual addresses and not just DMA
(I think).

One possible (but naive) design is to use a new VM object for each I/O
buffer.  For files you would want some sort of reverse-shadow object that
gives you a snapshot of the file's data.  That is, you would move the
existing pages from the file into a swap-backed object that would be
"behind" the vnode object so that writes would COW and allocate a page
in the vnode object, but reads would fall through to the page in the
I/O buffer for unchanged data.  For non-files you would use swap-backed
VM objects for data.  Internally, read operations would have to map
buffers into the process' address space.  Ideally you would unmap the
objects when the process was finished with them so that the objects
(and associated pages) could be harvested once they were no longer used.
(This would mean needing some way to say "done with this return from
read".)

My worry here is that this approach could add a lot of overhead.
You don't want a new object for every page-sized read that you
do (though to preserve the "snapshot" semantics of read from
files you may actually need that).  From non-files it might be nice to
amortize the cost if possible.  It would also be nice to "cache"
mappings to avoid mapping operations on read if the same buffer is
reused.

IO-Lite's paper makes mention of a special pager.  My
initial reading of this is that this means they provided a single
address window in each process that all I/O buffers had to live in,
but I'm not sure if that's really accurate to what they did.
Hmm, 2.2 does imply that each I/O stream (socket, etc.) kept a
pool of recently-free'd pages that could be reused.  I think this
implies that most larger-than-a-page I/O's would not be virtually
contiguous.

--------------------------------------------------------------------

Files make this complicated, so I'm going to punt and think about
just using this for streaming files (sockets to start, though ttys
should be "adoptable" as well).

The IO-Lite model seems to not be creating an object for each buffer.
In fact, it may be that what you end up with is one single object that
can grow over time.  The equivalent of aggregates would be (offset,
length) tuples relative to the single object.  In userland a library
would keep a cache of mappings.  In-kernel consumers would need to
wire pages while using them, but could unwire them when done.
The kernel would need to keep an in-kernel version of the buffers it
has given to each process (possibly to each vmspace instead?) so
that it can drop the reference counts on buffers if a process exits
without having done so (crash, etc.).  We would need a way to determine
which pages in the object were free and could be reused once they were
no longer referenced by any buffers (perhaps a per-page refcount),
and one optimization is that when the kernel requests a buffer we
should prefer page offsets that have a resident page to those that
do not.  I'm not sure if there's anything we can do to be smarter and
try to use superpages here when possible.  Note that the userland
mappings would generally be read-only except when requesting a new
buffer to use for a write.

Eventually if you wanted this to be "transparent" for regular NICs
you would want receive mbufs to be allocated as this type of I/O
buffer from the start rather than being mbuf clusters I think.
Otherwise you will need to copy the data out of any "normal" mbuf
clusters into a new I/O buffer before sending it to userland.

I'm not sure if this lends itself to having the same kind of per-fd
affinity that IO-Lite mentions so that you try to reuse buffers that
already exist.  Perhaps you could do that by placing "free" buffers on
a per-fd list as well as a global list (global lock would not be ideal
though).  If an fd's "free" list is empty it would start picking
things from the global list.  Maybe it would be better instead to only
have free buffers on the per-fd list but to instead have a list of
file descriptors and look for a file with the most free buffers and
grab those from the per-fd list of the other file.  You would still
have a global lock for the list of file descriptors, but it would only
change when a file descriptor started using I/O buffers or was closed.
The "find a free buffer" case would only have to read-lock the list.

I'm not sure how to detect when the user library should release
address space.  It would have to keep any addresses in active
aggregates mapped, but the idea of the cache is to avoid remapping
a buffer if it gets reused quickly, so you don't want to unmap
just because a given page is no longer used.  OTOH, you don't want
to starve out malloc() or dlopen() (especially on 32-bit platforms).
Assuming the bits live in libc, you could hook malloc() at least so
that if malloc fails to get a new chunk of address space it could
try a drain of the I/O buffer object before retrying it's mmap().
However, that isn't friendly to out-of-tree mallocs (e.g. debugging
mallocs), and it's harder to do this for dlopen().
